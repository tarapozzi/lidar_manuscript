Result_G[i,4]=quantile(post, 0.75)
Result_G[i,5]=quantile(post,0.975)
Result_G[i,6]=r2
}
write.csv(Result_G, "G:/Everyone-Temp/TaraPozzi/Pre_Registration/Analysis//Result_G.csv")
View(Result_G)
set.seed(123)
N=200 # number of survey respondensim.data
K=19 # number of predictors
## 1) Set the intercept ##
intercept=0 ## mean value of lidar use when all predictors are equal to 0
## 2) Set the predictor variables ##
## set simulate experience with binomial for each type of experience
experience_1 <- rbinom(N, 1, .5) # damage to property in community # yes (1) no (0)
# not sure what the appropriate probability is here... I think it is probably different for each level of experience closeness
experience_2 <- rbinom(N, 1, .5) # deaths or injury to people in your community # yes (1) no (0)
experience_3 <- rbinom(N, 1, .5) # damage to home # yes (1) no (0)
experience_4 <- rbinom(N, 1, .5) # deaths or injuries to you or members of your immediate family # yes (1) no (0)
experience_5 <- rbinom(N, 1, .5) # disruption to your electric, water, phone, and other basic services # yes (1) no (0)
gov_trust <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
currentmap <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
science_trust <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
gov_involve <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
future_1 <- sample(1:5, N, replace=TRUE) # damge to property in the community within next 30 years 0% (1) 100%(5)
future_2 <- sample(1:5, N, replace=TRUE) # deaths or injury to people in community within next 30 years 0% (1) 100%(5)
future_3 <- sample(1:5, N, replace=TRUE) # damage to home in the next 30 years 0% (1) 100%(5)
future_4 <- sample(1:5, N, replace=TRUE) # deaths or injuries to your or members of your immediate family within next 30 years 0% (1) 100%(5)
future_5 <- sample(1:5, N, replace=TRUE) # disruption to your electric, water, phone, and other basic services within the next 30 years 0% (1) 100% (5)
incr_no_flood<- sample(1:3, N, replace=TRUE) # no (1) stay the same (2) yes (3)
incr_sev_flood<- sample(1:3, N, replace=TRUE) # no (1) stay the same (2) yes (3)
prepared<- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
age <- sample(1:5, N, replace=TRUE) # les than 20 years (1) 50+ years (5)
gender <- sample(1:3, N, replace=TRUE) # male (1) female(2) prefer to self-describe (3)
education <- sample(1:6, N, replace=TRUE) # some high school (1) advanced degree (6)
barrier_1 <- sample(1:5, N, replace=TRUE) # lack of funding strongly disagree (1) strongly agree (5)
barrier_2 <- sample(1:5, N, replace=TRUE) # lack of expertise strongly disagree (1) strongly agree (5)
barrier_3 <- sample(1:5, N, replace=TRUE) # sparse population strongly disagree (1) strongly agree (5)
barrier_4 <- sample(1:5, N, replace=TRUE) # low rate of economic development strongly disagree (1) strongly agree (5)
barrier_5 <- sample(1:5, N, replace=TRUE) # low flooding risk strongly disagree (1) strongly agree (5)
barrier_6 <- sample(1:5, N, replace=TRUE) # lack of political support strongly disagree (1) strongly agree (5)
barrier_7 <- rbinom (N, 1,.2) # this is whether respondent is in an area with publically available lidar or not...the probobaly will change once I calculate the % of publically avaiable data across FEMA Region X
soep <-  sample(0:10, N, replace=TRUE) # range of risk preference from 0 to 10, where 0 = I generally prefer to take risks to 10 = I generally prefer to avoid risks
alter_lidar_prop <- runif(N, 0, 1) # this simulates the a range of potential proportion of lidar users in alters
alter_exp_mean <- runif(N, 0, 10) # this will rep the mean expertise of alters
alter_comm_mean <- runif(N, 1, 6) # reps the mean communication of alters with respondent
# Make data frame with raw data
raw.data <- data.frame(experience_1, experience_2, experience_3, experience_4, experience_5, gov_trust, currentmap, science_trust, gov_involve, future_1,
future_2, future_3, future_4, future_5, incr_no_flood, incr_sev_flood, prepared, age, gender, education, barrier_1, barrier_2,
barrier_3, barrier_4, barrier_5, barrier_6, barrier_7, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)
raw.data <- tibble::rowid_to_column(raw.data, "ID")
## 3) Now that we have our raw data, let's do the indexing process ##
# Summation of direct experiences. First we need to quantify the "level of closeness" of the direct experience
raw.data$experience_2[raw.data$experience_2==1] <- 1
raw.data$experience_2[raw.data$experience_2==1] <- 2
raw.data$experience_3[raw.data$experience_3==1] <- 4
raw.data$experience_4[raw.data$experience_4==1] <- 5
raw.data$experience_5[raw.data$experience_5==1] <- 3
raw.data$direct_exp_sum <- apply(raw.data[,c(2:6)],1,sum) # closer the experience, higher the number
# Average risk perception
# QUESTION: do I need to rescale the two likert scales to the same scale?
raw.data$risk_percp_mean <- (future_1*1 + future_2*2 + future_3*4 + future_4*5 +future_5*3 + # future predictors are scaled based on the "level of closness" # closer the future event, higher the number
incr_no_flood + incr_sev_flood)/3
# Total strength of respondent's relationship with alters that use lidar. In this case there is only one alter.
raw.data$alter_strength_tot <- (alter_lidar_prop*alter_comm_mean)
# Total expertise of respondent's alters that use lidar
raw.data$alter_expertise_tot <- (alter_lidar_prop*alter_exp_mean)
# Look at indexed data frame
str(raw.data)
# In order for this next step to work, we need to turn these indexed predictors into "values"
direct_exp_sum <- as.numeric(raw.data$direct_exp_sum)
risk_percp_mean <- as.numeric(raw.data$risk_percp_mean)
alter_strength_tot <- as.numeric(raw.data$alter_strength_tot)
alter_expertise_tot <- as.numeric(raw.data$alter_expertise_tot)
## Now let's set the effect size ##
b_direct_exp_sum <- .1
b_gov_trust <- .05
#b_currentmap <- 0 # I think this could be left out of statisical analysis
b_science_trust <- .2
b_gov_involve <- .02
b_risk_percp_mean <- .1
#b_prepared<- .03 # I think this could be left out of statisical analysis
b_age <- -.1
b_gender <- 0
b_education <- .1
b_barrier_1 <- -.05
b_barrier_2 <- -.03
b_barrier_3 <- -.05
b_barrier_4 <- -.02
b_barrier_5 <- -.05
b_barrier_6 <- 0
b_barrier_7 <- 0
b_soep <-  .2
b_alter_lidar_prop <- .05
b_alter_strength_tot <- .1
b_alter_expertise_tot <- .1
## 4) Set the response variable ##
p <- intercept + direct_exp_sum*b_direct_exp_sum + gov_trust*b_gov_trust + science_trust*b_science_trust + gov_involve*b_gov_involve +
risk_percp_mean*b_risk_percp_mean + age*b_age + gender*b_gender + education*b_education + barrier_1*b_barrier_1 +
barrier_2*b_barrier_2 + barrier_3*b_barrier_3 + barrier_4*b_barrier_4 + barrier_5*b_barrier_5 + barrier_6*b_barrier_6 +
barrier_7*b_barrier_7 + soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_strength_tot*b_alter_strength_tot +
alter_expertise_tot*b_alter_expertise_tot
pr <- plogis(p) # convert from log odds to probability
lidaruse <- rbinom(N,1,pr)
## 5) Combine data into dataframe ##
sim.data <- data.frame(lidaruse, direct_exp_sum, gov_trust, science_trust, gov_involve,
risk_percp_mean, age, gender, education, barrier_1,
barrier_2, barrier_3, barrier_4, barrier_5, barrier_6,
barrier_7, soep, alter_lidar_prop, alter_strength_tot,
alter_expertise_tot)
write.csv(sim.data, "sim_data.csv")
#### Descriptive Stasim.data ####
# Check for correlations
cor(sim.data) # nothing > |.5| so we are good!
# plot the raw data & check for outliers
boxplot(sim.data[-24])
# scatter plots
sim.data %>%
gather(-lidaruse, key = "var", value = "value") %>%
ggplot(aes(x = value, y = lidaruse)) +
geom_point() +
geom_smooth(se = TRUE, method = 'lm') +
facet_wrap(~ var, scales = "free") +
theme_bw()
mod3.stan <- stan_glm(lidaruse ~ direct_exp_sum + gov_trust + alter_lidar_prop +alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial)
summary(mod3.stan)
plot(mod3.stan, "areas", prob=0.95)
median(bayes_R2(mod3.stan))
full.mod.stan.200 <- stan_glm(lidaruse ~ direct_exp_sum + gov_trust + science_trust + gov_involve +
risk_percp_mean + age + gender + education + barrier_1 +
barrier_2 + barrier_3 + barrier_4 + barrier_5 + barrier_6 +
barrier_7 + soep + alter_lidar_prop + alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial)
summary(full.mod.stan.200)
plot(full.mod.stan.200, "areas", prob=0.95)
median(bayes_R2(full.mod.stan.200))
b_direct_exp_sum <- .1
b_gov_trust <- .05
#b_currentmap <- 0 # I think this could be left out of statisical analysis
b_science_trust <- .2
b_gov_involve <- .02
b_risk_percp_mean <- .1
#b_prepared<- .03 # I think this could be left out of statisical analysis
b_age <- -.1
b_gender <- 0
b_education <- .1
b_barrier_1 <- -.05
b_barrier_2 <- -.03
b_barrier_3 <- -.05
b_barrier_4 <- -.02
b_barrier_5 <- -.2
b_barrier_6 <- 0
b_barrier_7 <- 0
b_soep <-  .2
b_alter_lidar_prop <- .05
b_alter_strength_tot <- .1
b_alter_expertise_tot <- .1
## 4) Set the response variable ##
p <- intercept + direct_exp_sum*b_direct_exp_sum + gov_trust*b_gov_trust + science_trust*b_science_trust + gov_involve*b_gov_involve +
risk_percp_mean*b_risk_percp_mean + age*b_age + gender*b_gender + education*b_education + barrier_1*b_barrier_1 +
barrier_2*b_barrier_2 + barrier_3*b_barrier_3 + barrier_4*b_barrier_4 + barrier_5*b_barrier_5 + barrier_6*b_barrier_6 +
barrier_7*b_barrier_7 + soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_strength_tot*b_alter_strength_tot +
alter_expertise_tot*b_alter_expertise_tot
pr <- plogis(p) # convert from log odds to probability
lidaruse <- rbinom(N,1,pr)
## 5) Combine data into dataframe ##
sim.data <- data.frame(lidaruse, direct_exp_sum, gov_trust, science_trust, gov_involve,
risk_percp_mean, age, gender, education, barrier_1,
barrier_2, barrier_3, barrier_4, barrier_5, barrier_6,
barrier_7, soep, alter_lidar_prop, alter_strength_tot,
alter_expertise_tot)
write.csv(sim.data, "sim_data.csv")
#### Descriptive Stasim.data ####
# Check for correlations
cor(sim.data) # nothing > |.5| so we are good!
# plot the raw data & check for outliers
boxplot(sim.data[-24])
# scatter plots
sim.data %>%
gather(-lidaruse, key = "var", value = "value") %>%
ggplot(aes(x = value, y = lidaruse)) +
geom_point() +
geom_smooth(se = TRUE, method = 'lm') +
facet_wrap(~ var, scales = "free") +
theme_bw()
full.mod.stan.200 <- stan_glm(lidaruse ~ direct_exp_sum + gov_trust + science_trust + gov_involve +
risk_percp_mean + age + gender + education + barrier_1 +
barrier_2 + barrier_3 + barrier_4 + barrier_5 + barrier_6 +
barrier_7 + soep + alter_lidar_prop + alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial)
summary(full.mod.stan.200)
plot(full.mod.stan.200, "areas", prob=0.95)
median(bayes_R2(full.mod.stan.200))
full.mod.brms <- brm(lidaruse ~ mo(direct_exp_sum) + mo(gov_trust) + mo(science_trust) + mo(gov_involve) +
risk_percp_mean + mo(age) + gender + mo(education) + mo(barrier_1) +
mo(barrier_2) + mo(barrier_3) + mo(barrier_4) + mo(barrier_5) + mo(barrier_6) +
mo(barrier_7) + mo(soep) + alter_lidar_prop + alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial)
summary(full.mod.brms)
plot(full.mod.brms, "areas", prob=0.95)
median(bayes_R2(full.mod.brms))
## RISK MODEL
egonet.mod.stan <- stan_glm(lidaruse ~ alter_lidar_prop + alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial)
summary(egonet.mod.stan)
plot(egonet.mod.stan, "areas", prob=0.95)
median(bayes_R2(egonet.mod.stan))
kfold(egonet.mod.stan, K=10)
Result_G <- data.frame(matrix(nrow=nrow(sim.data), ncol=6))
#set the column names
colname(Result_G) <- c("MedianG", "LowerBound1_G", "LowerBound2_G", "UpperBound1_G", "UpperBound2_G", "R2_Bayes")
#do the leave one out cross validation
# create for loop function
for(i in 1:length(sim.data$lidaruse)){ # for all the data poinsim.data
sub_dat <- sim.data[-i,] #remove each one at a time
m_sub <-  stan_glm(lidaruse ~ alter_lidar_prop + alter_strength_tot +
alter_expertise_tot, data=sim.data, family= binomial) # this is saying run the model without that point
post=posterior_predict(m_sub, sim.data[i,], draws=4000)
r2=r2_bayes(m_sub)# predict that point
Result_G[i,1]=quantile(post, 0.5) # fill a df with the credibility intervals
Result_G[i,2]=quantile(post, 0.25)
Result_G[i,3]=quantile(post, 0.025)
Result_G[i,4]=quantile(post, 0.75)
Result_G[i,5]=quantile(post,0.975)
Result_G[i,6]=r2
}
View(Result_G)
View(Result_G_AIC)
View(Result_G)
View(Result_G_AIC)
install.packages("citr")
library(citr)
install.packages("citr")
updateR()
devtools::install_github("crsh/citr")
library("rlang", lib.loc="~/R/R-4.0.3/library")
remove.packages("rlang", lib="~/R/R-4.0.3/library")
devtools::install_github("crsh/citr")
install.packages("citr")
install.packages("installr")
library(installr)
updateR()
install.packages(citr)
install.packages("citr")
install.packages("citr", dependencies=TRUE)
file.edit(file.path("~", ".Rprofile"))
install.packages("rlang")
install.packages("rlang")
install.packages("citr", dependencies=TRUE)
devtools::install_github("crsh/citr")
library(citr)
citr:::insert_citation()
setwd("~/Manuscript")
citr:::insert_citation()
citr:::insert_citation()
knitr::opts_chunk$set(echo = TRUE)
library(citr)
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
# Response rate: I need to bring in the total potential respondents #
# Oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
filter(grepl("2", screen))
knitr::opts_chunk$set(echo = TRUE)
library(citr)
library(dplyr)
# Response rate: I need to bring in the total potential respondents #
# Oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
filter(grepl("2", screen))
# load distribution history
or.dist <- read.csv("data/or_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 369-12: 357 potential responses
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
#Alaska
alaska.results <- read.csv("data/ak.csv")
ak.no <- alaska.results %>%
filter(grepl("2", screen))
#load distribution history
ak.dist <- read.csv("data/ak_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 55-1: 54 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
# 463 potential responses
## Total potential responses: 357 + 386 + 54 + 463= 1260
## RESPONSE RATE ##
#193/1260=15.3%
View(id.pot.responses)
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- count(nrow(wa.dist$Last.Name)) - 12
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- count(wa.dist$Last.Name) - 12
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- nrow(wa.dist$Last.Name) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- count(nrow(id.pot.responses$LastName))
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- nrow(wa.dist$Last.Name) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- nrow(id.pot.responses$LastName)
# 463 potential responses
total.response <-  384 + b
## RESPONSE RATE ##
#193/1260=15.3%
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- nrow(wa.dist$Last.Name) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- nrow(id.pot.responses$LastName)
# 463 potential responses
total.response <-  a + b
## RESPONSE RATE ##
#193/1260=15.3%
total.response
a
b
nrow(wa.dist$Last.Name)
count(wa.dist$Last.Name)
nrow(wa.dist$Email)
View(wa.dist)
length(wa.dist$Response.ID)
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- length(wa.dist$Response.ID) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- length(id.pot.responses$Response.ID)
# 463 potential responses
total.response <-  a + b
## RESPONSE RATE ##
#193/1260=15.3%
total.response
length(id.pot.responses$Response.ID)
length(id.pot.responses$X)
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- length(wa.dist$Email) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- length(id.pot.responses$Email)
# 463 potential responses
total.response <-  a + b
## RESPONSE RATE ##
#193/1260=15.3%
total.response
knitr::opts_chunk$set(echo = TRUE)
library(citr)
library(dplyr)
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
filter(grepl("2", screen))
#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
a <- length(wa.dist$Email) - 12
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-12: 384 potential responses
# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
b <- length(id.pot.responses$Email)
# 463 potential responses
total.sample.frame <-  a + b
## RESPONSE RATE ##
#193/1260=15.3%
library(tidyr)
library(ggplot2)
library(ggplotgui)
library(ggrepel)
library(plotly)
library(RColorBrewer)
library(here)
library(tinytex)
library(psych)
library(pastecs)
library(rstanarm)
library(loo)
library(tidybayes)
library(tidyverse)  # ggplot, dplyr, %>%, and friends
library(ggdag)  # Make DAGs with ggplot
library(dagitty)  # Do basic DAG math
library(broom)  # For converting model output to data frames
library(caret) # model comparison
library(plyr) # helps with model comparison computation
library(performance)
library(rworldmap)
library(hrbrthemes)
library(ggplotAssist)
library(clusterSim)
library(dplyr)
library(purrr)
library(forcats)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(rstan)
library(gganimate)
idaho <- read.csv("data/id.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries
filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
subset(.,experience_1!="") %>% # remove blank responses
dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid")) %>% # remove network data until pre-reg is complete
dplyr::rename(LastName = RecipientLastName,
FirstName = RecipientFirstName,
Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
#unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
add_column(.,location="Idaho") %>%
distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate
str(idaho)
oregon <- read.csv("data/or.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries
filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
subset(.,experience_1!="") %>% # remove blank responses
dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid")) %>% # remove network data until pre-reg is complete
dplyr::rename(LastName = RecipientLastName,
FirstName = RecipientFirstName,
Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
#unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
add_column(.,location="Oregon") %>%
distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate
str(oregon)
washington <- read.csv("data/wa.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries
filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
subset(.,experience_1!="") %>% # remove blank responses
dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid")) %>% # remove network data until pre-reg is complete
dplyr::rename(LastName = RecipientLastName,
FirstName = RecipientFirstName,
Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
#unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
add_column(.,location="Washington") %>%
distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate
str(washington)
alaska <- read.csv("data/ak.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries
filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
subset(.,experience_1!="") %>% # remove blank responses
dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid")) %>% # remove network data until pre-reg is complete
dplyr::rename(LastName = RecipientLastName,
FirstName = RecipientFirstName,
Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
#unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
add_column(.,location="Alaska") %>%
distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate
str(alaska)
# Combine all four states into one dataset #
survey.responses <- rbind(idaho, washington) %>%
subset(.,incr_no_flood!="") # remove additional blank responses
write.csv(survey.responses, "C:/Users/tarapozzi/Documents/Manuscript/survey.responses.csv")
View(survey.responses)
