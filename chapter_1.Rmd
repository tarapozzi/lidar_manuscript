---
title: "Chapter 1 Manuscript"
author:
- name: Tara Pozzi
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  word_document: default
thanks: '.....'
affiliation: Boise State University
runhead: XXXX
geometry: margin=1in
fontsize: 12pt
csl: crm.csl
bibliography: references.bib
indent: yes
colorlinks: yes
---

```{r setup, include=FALSE}
library(citr) # doesn't work in this version of r...
library(tidyr)
library(ggplot2)
library(ggplotgui) # doesn't work in this version of r 
library(ggrepel)
library(plotly)
library(RColorBrewer)
library(here)
library(tinytex)
library(psych)
library(pastecs)
library(rstanarm)
library(loo) 
library(tidybayes)
library(tidyverse)  # ggplot, dplyr, %>%, and friends
library(ggdag)  # Make DAGs with ggplot
library(dagitty)  # Do basic DAG math
library(broom)  # For converting model output to data frames
library(clusterSim)
library(caret) # model comparison
library(plyr) # helps with model comparison computation
library(performance)
library(rworldmap)
library(hrbrthemes)
library(ggplotAssist)
library(dplyr)
library(purrr)
library(forcats)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(rstan)
library(bookdown) # for cross-referencing
library(knitr) #for global options
library(formatR) 
```

```{r, global options, include=FALSE}
# Setup options for code decoration
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
highlight = TRUE)

# Setup options for code cache
opts_chunk$set(cache = 2, cache.path = "cache/")

# Setup options for plots
opts_chunk$set(fig.path = "Figures_MS/", dev = c("pdf", "png"),
dpi = 300)

## Locate figures as close as possible to requested position
## (=code)
opts_chunk$set(fig.pos = "H")

```

Abstract
========================
**"A concise and factual abstract is required. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself."**

# HIGHLIGHTS
*Come back and edit this when results and discussion are finished*
1. There is between state variation of lidar use between Idaho and Washington.
2. Data suggests a more variable environment lends to flood risk manager's making more risk-prone decisions.
3. Social learning plays a critical role in shortfall minimization and lidar adoption.

# 1 INTRODUCTION
Floods are one of the most frequent and destructive natural disasters in the United States [@pralleDrawingLinesFEMA2019;@RiskMappingAssessment]. Flood events disrupt and damage the technical, ecological, social, cultural, political, and economic landscapes causing incalculable expenses to our society oftentimes leaving vulnerable groups even more at risk in the future [@howellDamagesDoneLongitudinal2019]. Since the National Centers for Environmental Information (NCEI) began tracking natural disaster events in 1980, there has been an increase in flood events in the U.S., some of those with unprecedented amounts of rainfall. This is because as temperatures rise there is an increase in the amount of water vapor in the atmosphere, which increases the potential for extreme rainfall events. In addition to increased flood risk from climate change, there is an increasing rate of population growth and urbanization in coastal and inland floodplains [@pralleDrawingLinesFEMA2019; @schanzeFloodRiskManagement2006]. In 2015, 21.8 million (6.87%) of the U.S. population was identified as being exposed to a 100-year flood; meaning they lived in a location that would be inundated by a flood event that has a 1 in 100 chance of happening each year [@qiangDisparitiesPopulationExposed2019]. 100-year flood events are based on historical rainfall patterns, however this probability can change based on local land use, river impoundments, the amount of impervious surfaces, and long-term climate patterns [@TechniquesMethods2019]. Considering climate change, increaseed urbanization, and population growth in floodprone regions, experts in the field have established that absolute flood protection is not realistic anymore, rather flood risk management (FRM) is a more accurate way to refer to floodplain management [@schanzeFloodRiskManagement2006].

Risk has a variety of definitions based on the disciplinary domain in which the concept is being examined. In an everyday sense, risk can be considered the chance of a negative outcome occurring [@mishraDecisionMakingRiskIntegrating2014]. We see risk as inherently transdisciplinary and needs to encapsulate the full context of the topic it is being applied to. Therefore we define risk, in a flood context, as the quantifiable consequence of a flood event given the known, local social, environmental, and political factors. Communities understand their flood risk typically by using Federal Emergency Management Agency (FEMA) floodplain maps that estimate the extent of flood hazards through hydrologic and hydraulic models. These analyses require topography, rainfall and run-off frequency distributions, and flood control structures (e.g. diversion dams, levees, bridges). In addition, these floodplain maps are essential for communicating flood risk to vulnerable populations, helping communities mitigate and adapt to floods, and the functioning of insurance programs, such as the the FEMA's National Flood Insurance Program [@pralleDrawingLinesFEMA2019]. However, recent reports estimate that approximately 25% of the flood damage claims occur outside of FEMA mapped floodplains each year because these maps can be outdated and inaccurate [@ludyFloodRiskPerception2012].

In order to accurately predict flood risk, many communities are adopting new technology and management practices. One of the advances in FRM is the availability of higher-resolution terrain mapping and feature data. Previous research confirms that high-resolution topographic data is critical for an accurate floodplain map [@aliAssessingImpactDifferent2015; @cookEffectTopographicData2009].  In the past, flood risk managers typically used 10-meter or 30-meter resolution Digital Elevation Models (DEM) from sources such as the National Elevation Dataset (NED). One technology that has grown in popularity is Light Detection and Ranging (lidar), which is a laser-based remote sensing technology that uses the reflection of light to measure elevation and features on the ground such as vegetation and structures. Raw lidar data points form a three-dimenionsial (3D) point cloud. These 3D point clouds can then be used in a wide-array of hazard applications such as wildfire fuel load calculations or with identifying wildlife habitat viewsheds. In addition, raw lidar data can be used to create 1-meter or finer resolution DEMs [@muhadiUseLiDARDerivedFlood2020]. *should I insert example how this is used in FRM?*

Several government agencies initiated lidar acquisition projects in an effort to increase the availability of publically-accessible lidar. Foremost, the United States Geological Survey (USGS) established the 3D Elevation Program (3DEP) in 2010 as the first nationally-coordinated lidar acquisition program with a goal of flying the complete U.S. by 2023 with lidar data.  This would be the first ever national baseline of consistent, high-resolution topographic elevation data, including bare earth and 3D point clouds. In addition, FEMA established the Risk Mapping and Planning (RiskMAP) program as part of the Biggert-Waters Flood Insurance Reform Act of 2012. This act charged FEMA with reforming the flood insurance process, while also improving the accuracy and reliability of it's floodplain maps (USGS, 2017). As a result, both 3DEP and RiskMAP programs are used to fund lidar acquisition projects across the U.S. In addition, several other U.S. agencies including the National Oceanic and Atmospheric Administration (NOAA), the U.S. Department of Agriculture (USDA), the U.S. Army Corps of Engineers (USACE), and U.S. Forest Service (USFS) also participate in lidar acquisition. Figure XX displays the footprint of available topographic and bathymetric lidar across the contiguous, lower 48 states. From this image, there is a clear decrease in the availability of publically-accessible lidar in the western U.S. including Washington, Idaho, Montana, Oregon, Nevada, Utah, California, Arizona, and New Mexico. As lidar becomes more available and increasingly popular, it is important to understand the factors that influence a flood risk manager's decision to adopt this new technology into their practice of long-term risk mitigation.

!["The U.S. Interagency Elevation Inventory displays all publically-available lidar data and lidar-derived products for the continguous, lower 48 states. (source:https://coast.noaa.gov/inventory/)"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/lidar_map.PNG)

This study examines the landscape of factors that catalyze decision-makers to use lidar for flood risk management. Critical to understanding risk is the perception of that risk [@mishraDecisionMakingRiskIntegrating2014;@slovicPerceptionRisk1987]. This is because risk perception is a fundamental way to characterize how individual’s intuitive risk judgment and allows for the identification, characterization, and quantification of risk. However, previous research on risk perception has been opposing. Past findings suggests individuals who perceive that natural hazards pose a greater risk also behave more cautiously [@vinhhungFloodRiskManagement2007]. Interestingly, studies also found the opposite in that individuals who perceive a greater risk engage in fewer mitigating behaviors [@bubeckReviewRiskPerceptions2012; @wachingerRiskPerceptionParadoxImplications2013]. This paradoxical behavior suggests that risk perception is more nuanced and moderated by individual and socio-cultural factors [@baerenklauUnderstandingTechnologyAdoption2005; @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012; @kahanCultureIdentityProtectiveCognition2007; @vanvalkengoedMetaanalysesFactorsMotivating2019]. 

While the field of hazards and disaster research is inherently multidiscplinary, there has been a recent concerted effort for convergence research to integrate knowledge across disciplines and organizational boundaries to reduce disaster losses and promote collective well-being (Peek at al., 2020). In addition, previous flood risk research identified two main theoretical limitations of our existing intellectual understanding of the relationship between risk and adaptive behavior. Firstly, there is limited predictive power of the theories applied so far (e.g. protection motivation theory) [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action (e.g. social beliefs) [@kuhlickeBehavioralTurnFlood2020].
In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating behavior; there needs to be a critical look at the underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. We employ cultural evolutionary theory to study lidar adoption because it offers a convergent research lens to compare individual versus collective action and has the potential for prediction.  

In the next section we review cultural evolutionary theory in detail and explain how this theory is beneficial for understanding the underlying mechanisms that shape flood risk management behavior. Next, we apply this theoretical framework to our case study of lidar adoption for flood risk management.  This is followed by the methods section that explains our survey instrument development process and statistical approach for analyzing the survey data. The results from our analysis and a discussion about significant trends will follow. Finally, we discuss the implications of these results and need for further research. 

# 2 THEORY
**"A Theory section should extend, not repeat, the background to the article already dealt with in the Introduction and lay the foundation for further work. In contrast, a Calculation section represents a practical development from a theoretical basis."**
Kellens et al. reviewed 57 empirically based peer-reviewed articles on flood risk perception and communication to assess overall trends in flood risk research (2013). The authors found that majority of studies were exploratory and did not apply a theoretical framework to examine risk perception [@kellensPerceptionCommunicationFlood2013a]. Of the studies that employed a theoretical framework, protection motivation theory (PMT) was the most common. PMT The results of this review suggest future research that emphasizes a theoretical framework that caputures physical exposure and hazard experience to assess risk perception. PMT explains individual decisions about preparing for risk as a function of threat appraisal (e.g. likelihood of exposure to a flood, severity of expsoure, and fear) and coping appraisal (e.g. self-efficiacy, outcome efficacy, and outcome costs). If there is low threat appraisal, then adaptive behavior is unlikely to occur. However, if there is a high threat appraisl and high coping appraisal then it is likely an adaptive response will occur. Kuhlicke et al., summarizes additional individual behavior theories that have been used in flood risk management prior, including person-relative-to-event theory, theory of planned behavior, and protection action decision model [@kuhlickeBehavioralTurnFlood2020]. Additionally, Kuhlicke et al. highlights the use of the social identity model of collective action and social identity model of pro-environmental action as theories that explore the role of collective behavior on group adaptive behavior-related strategies [@kuhlickeBehavioralTurnFlood2020].There are several limitations with the application of these theories so far. Firstly, there is limited predictive power of the theories applied so far [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action in flood risk management research and therefore it is suggested that future research apply collective factors more rigourously [@kuhlickeBehavioralTurnFlood2020]. 
In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating beahvior; this work calls for a more critical perspective on underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. 

Humans are heavily reliant on social learning to acquire and manage their behavioral repertoire (Henrich and McElreath, 2008).  

## *2.1 Decision-making under flood risk*
Almost all human decisions are made while considering risk, which is different than uncertainty. With uncertainty, the outcome probability is unknown due to lack of knowledge, however this can be overcome with acquiring information about the environment [@henrichArePeasantsRisk2002]. With this information, an individual is now able to discern and quantify varying levels of risk. This is an important distinction to make because uncertainty can alter how an individual makes decisions under risk and therefore, can potentially be a confounding factor in data analyses [@winterhalderRisksenstiveAdaptiveTactics1999]. Risk has a variety of definitions based on the disciplinary domain in which the concept is being examined. In an everyday sense, risk can be considered the chance of a negative outcome occurring [@mishraDecisionMakingRiskIntegrating2014]. While from an ecologist’s perspective risk is an unpredictable variation in the outcome of a behavior, with consequences for an organism's fitness or utility [@winterhalderRisksenstiveAdaptiveTactics1999]. However, risk is inherently interdisciplinary and therefore needs to span both social and environmental contexts. For the purposes of this study, risk refers to the quantifiable consequence of a decision given the known, context-specific social and environmental factors.

Previous work has disporportionately focused on the effect of risk perception on risk adapating behavior. Risk perception is a basic way to characterize a person’s intuitive risk judgement and allows for the identification, characterization, and quantification of risk. For that reason, risk perception has been well studied in the past especially in terms of its role in decision-making under risk due to hazardous activities and technologies [@hungCriticalFactorsWAP2003; @slovicPerceptionRisk1987]. Interestingly, there has been research that identified an unexpected relationship between risk perception and lack of risk mitigating behavior named the “Risk Perception Paradox” [@bubeckReviewRiskPerceptions2012; @wachingerRiskPerceptionParadoxImplications2013]. This study examines why this phenomena may occur and proposes alternative predictors that may affect an individual’s risk mitigation behavior such as social and environmental factors that moderate risk perception’s effect size on decision-making and behavior [@vanvalkengoedMetaanalysesFactorsMotivating2019; @wachingerRiskPerceptionParadoxImplications2013]. 

## *2.2 Theoretical background*

Several theories have developed to understand decision-making under risk such as expected utility theory, prospect theory, and hueristic approaches. However, the only theory to account for the influence of evolutionary processes on decision-making under risk is risk-sensitivity theory. While this theory is well-established and can describe patterns of decision-making in variety of contexts, it has been underapplied [@mishraDecisionMakingRiskIntegrating2014]. Research thus far has found that risk-sensitivity theory accounts for a significant amount of variance in decision-making under risk [@mishraDecisionMakingRiskIntegrating2014]. This study applies risk-sensitivity theory to decision-making under flood risk to understand the variance in lidar use between Washington and Idaho. 

Risk-sensitivity theory is a normative theory traditionally used by behavioral ecologists to explain food acquisition decisions and overall foraging behavior. This theory has been widely-applied in studies of nonhuman animals [@mishraDecisionMakingRiskIntegrating2014; @winterhalderRisksenstiveAdaptiveTactics1999]. Researchers have found that the energy state of an individual significantly influences their risk preference. Therefore, risk-sensitivity theory predicts that the decision-maker will shift from risk-averse to risk-prone behavior in situations of need where there is a discrepancy between an individual’s present state and desired state [@mishraDecisionMakingRiskIntegrating2014]. Furthermore, previous research has examined risk-sensitivity theory with respect to the human decision-making model. For example, it has been shown that animals do not follow the predicted risk-sensitivity theory pattern when there is reward variance [@mishraDecisionMakingRiskIntegrating2014]. This could be due to heuristics that an individual uses to perceive their environment.

In addition, it is important to consider the impact of heuristics and biases on decision-making processes for they are a part of the human cognition system and affect how an individual interprets useful knowledge, practices, beliefs, and behaviors [@henrichEvolutionCulturalEvolution2003]. Within this, there are content and context biases. Content biases are direct, exploitive cues of information resulting in imitation of, typically, fitness-enhancing behavior. Context biases, on the other hand, exploit potential alternative behaviors or strategies.  Context biases are of particular interest because they have the ability to lead to evolution and natural selection of information within a population [@henrichEvolutionCulturalEvolution2003]. In addition, heuristics allows humans to make quick decisions and can be understood as the result of adaptive evolutionary processes to solve recurrent problems [@mishraDecisionMakingRiskIntegrating2014]. Sometimes these quick decisions are made with incomplete information, which is often true when making decisions under risk, making these decisions bounded by limited information [@mishraDecisionMakingRiskIntegrating2014]. Therefore, it is important to consider descriptive rationale when understanding the consequential decisions of risk. 

### **2.2.1 Applying risk-sensitivity theory**
Risk-sensitivity theory can be applied if the value of interest (e.g. utility, fitness) is nonlinear and one or more of the behavioral alternatives is characterized by unpredictable outcomes [@winterhalderRisksenstiveAdaptiveTactics1999].

!["This graph displays a sigmoid value function to represent the basic logic of risk sensitivity. The concave portion represents decreasing marginal returns. A constant outcome, denoted by k, represents the point where constant equals the probabilities of variable outcome (k+c, k-c). The convex portion represents increasing marginal returns  (Winterhalder et al., 1999)." ](C:/Users/tarapozzi/Documents/lidar_manuscript/images/value_function.png) 

As seen in Figure 1, when the resource of interest is scarce, value rises at an accelerating rate, convex, until it hits an inflection point where the resource is abundant there is a decelerating rate of value, concave. There are essentially two options for the individual to choose: a fixed reward, k, or an unpredictable reward with equal probability of being either k-c or k+c. An individual who chooses the fixed reward is typically risk averse, whereas the individual who is in poor condition will likely choose the variable reward making them risk prone. This is because the individual is making decisions based on decreasing marginal utility. If they are risk averse, they will remain on the concave shape of the utility function because it is more conservative. If they are risk-neutral, they would have a straight line utility curve at value k. If they are risk-prone, they would remain on the convex portion of the utility curve where they would prefer options with more variation [@henrichArePeasantsRisk2002]. In addition, the risk decision depends on time frame, urgency, and consequence of the decision as discussed previously. Research has found that animals are commonly risk averse when quantity is variable, however they are risk prone when the time to reward is variable [@winterhalderRisksenstiveAdaptiveTactics1999]. 

Considering these theoretical predictions, we expect to see flood risk managers who are located in variable floodings environments to be more risk prone and therefore choose riskier risk-mitigation behavior. Furthermore, there are additional social and environmental factors that moderate the effect size of risk sensitivity on management decisions. 

### **2.2.2. Risk mitigation behavior**
Research has found that individuals, human and nonhuman, make decisions to minimize their chances of falling below subsistence minimum known as shortfall minimization [@henrichArePeasantsRisk2002]. Most of the previous work in this field has been with empirical observations of nonhuman animals in an experimental or laboratory setting. The findings from these studies have shown consistent risk-sensitive behavior. Risk prone resource selection, variable reward versus constant reward with same overall reward value, is more common under negative energy balance for solitary species [@winterhalderRisksenstiveAdaptiveTactics1999]. However for larger animals, where there are alternative behavioral tactics to minimize shortfalls, risk-prone behavior is more rare because our their ability for cooperation. Two examples of this risk-minimizing behavior is resource pooling and social learning. 

Resource pooling opens up opportunities for trying new technology and techniques that one individual may not have been able to afford or know how to do on their own. For example, farming cooperatives in France have become popular because it allows individuals to pool wealth, tools, and knowledge. Consequently, these farmers see greater returns on investment due to economies of scale from pooled resources [@agarwalGroupFarmingFrance2019]. In addition, resource pooling can instill social capital in a community over a united purpose. This is because social capital can create trust, norms of reciprocity, and social networks that can be instrumental in successful group collaboration and cooperation [@wagnerDoesCommunityBasedCollaborative2008]. 

Additionally, social learning is an important component of risk minimization. Social learning is an individual’s ability to transmit information to another person. Culture forms as a result of this process, creating a shared set of beliefs and norms among a group of individuals. Culture evolves through the process of natural selection, creating between-group variation of adaptive behavior and cooperation [@richersonCulturalGroupSelection2016]. Consequently, some groups evolve to have more successful risk mitigation behavior than others. Institutions are an example of a group that has formed due to collective decision-making. Often times, there is competition between institutions in a similar market. The institution that wins typically has been able to suit the needs of their environment better than their competition [@richersonCulturalGroupSelection2016]. This is true for risk mitigation behavior as well and therefore social learning makes working as a group advantageous for minimizing risk. Considering these tactics, we expect to find that flood risk manager's decisions to be affected by these risk miminization tactics. 
 
## *2.3 Case study*

CASE STUDY justification: 
This is true for the western United States where we can expect to see an increase in precipitation and higher temperatures earlier in the year [@clarkChangesPatternsStreamflow2010; @ralphVisionFutureObservations2014; @idahoofficeofemergencymanagementStateIdahoHazard2018; @washingtonemergencymanagementdivisionWashingtonStateEnhanced2020].

Technology adoption in flood risk management provides an interesting case study to examine patterns of decision-making under flood risk due to the variation in use and reward of the interested technology. This study examines the adoption of lidar in communities throughout Washington and Idaho. Lidar provides highly detailed and accurate topographic data that flood risk managers use to model floodplains and assess their flood risk. The technology is expensive (~ $125/km2), requires a lot of storage, and can be slow. In addition, it takes additional training to learn how to process the raw lidar data and integrate the data points into programs such as ArcGIS to create useful products for flood risk management. 

 ![The image on the left is a traditional 30-meter digital elevation model and the image on the right is a 1-meter digital elevation model derived from lidar data.](C:/Users/tarapozzi/Documents/lidar_manuscript/images/dem_comp.png) 

The United State Geological Survey 3D Elevation Program (3DEP) was created to coordinate nationwide acquisition of lidar by 2023. While this technology has been backed by federal agencies, it has had variable adoption rates across the U.S.
One reason for this could be due to local governments having variable risk sensitivity to flooding and consequently varying action in adopting lidar. The two states of comparison in this example are Washington and Idaho. Flooding in Washington typically occurs on a seasonal basis due to rainfall from atmospheric rivers, rainfall on snow, flash foods from storms, and winter storms causing storm surges and high tide [@washingtonemergencymanagementdivisionWashingtonStateEnhanced2020]. Idaho is prone to flooding from rivers, flash floods, ice/debris jams, sheet or areal flooding, and mudflows [@idahoofficeofemergencymanagementStateIdahoHazard2018]. Overall, the two states see different amounts of damage and risk as shown in \autoref{tab:tab_statecomp} **should I turn this into a map?** . 

```{r comparison table, echo=FALSE, fig.cap="\\label{tab_statecomp}"}
id.floodrisk <- read.csv("data/idaho_floodrisk_firststreet.csv")
id.fema.2020.total <- round(sum(id.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
id.fema.2020.pct <- round(mean(id.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
id.fs.2020.total <- round(sum(id.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
id.fs.2020.pct <- round(mean(id.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

or.floodrisk <- read.csv("data/oregon_floodrisk_firststreet.csv")
or.fema.2020.total <- round(sum(or.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
or.fema.2020.pct <- round(mean(or.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
or.fs.2020.total <- round(sum(or.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
or.fs.2020.pct <- round(mean(or.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

wa.floodrisk <- read.csv("data/washington_floodrisk_firststreet.csv")
wa.fema.2020.total <- round(sum(wa.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
wa.fema.2020.pct <- round(mean(wa.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
wa.fs.2020.total <- round(sum(wa.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
wa.fs.2020.pct <- round(mean(wa.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

state.comp <- matrix(c(id.fema.2020.total, or.fema.2020.total, wa.fema.2020.total, id.fema.2020.pct, or.fema.2020.pct, wa.fema.2020.pct, id.fs.2020.total, or.fs.2020.total, wa.fs.2020.total, id.fs.2020.pct, or.fs.2020.pct, wa.fs.2020.pct), ncol=3, byrow=TRUE)
colnames(state.comp) <- c("Idaho", "Oregon", "Washington")
rownames(state.comp) <- c("Total FEMA Properties at Risk (2020) ", "Percent FEMA Properties at Risk (2020)", "Total FS Properties at Risk (2020) ", "Percent FS Properties at Risk (2020)")
knitr::kable(state.comp, caption="Summary information about environmental and social differences between Idaho and Washington.")
```


### **2.3.1 Examining predictors of lidar adoption in further detail**
Risk‐sensitivity theory suggests that decision-makers should prefer high‐risk options in situations of high need, when lower risk options are unlikely to meet those needs [@mishraFramingEffectsRisksensitive2012]. In this study, the value represents the adoption of lidar and the outcome is flood risk mitigation. Given this, as well as previously described information about the effects of variable reward and delay, I expect to see an adoption pattern similar to that suggested by the value function in Figure 1. Flood risk managers in negative energy space due to high flood risk, the convex portion of the value function, should act more risk prone in order to try and mitigate negative impacts of floods. Whereas, flood risk managers who are in the positive energy space, the concave portion of the value function, should act more risk averse and may remain status quo. I hypothesize that flood risk managers in Washington are more likely to risk adopting a new technology because of their higher need to address floods. 

!["Flow chart of factors influencing risk-sensitivity towards lidar adoption" ](C:/Users/tarapozzi/Documents/lidar_manuscript/images/theory_factors.jpg) 

Figure 3 displays the breakdown of factors that may influence a flood risk managers risk sensitivity and consequential decision-making process in adopting lidar. The first component is the variable time and reward of this decision. There is an inherent risk with adopting a new technology because it requires an investment of time and money in order to be adopted. One of way measuring the effect of this variation on adoption is by measuring the outcome efficacy of products produced from using this technology. In this case study, we measured a flood risk manager's trust in the accuracy of scientific products for FRM, as well as their trust in the government to produce useful FRM products. 

Risk-taking attitude, also known as risk preferences, has been studied previously as an important predictor of risk mitigation behavior. However, it is distinct from risk sensitivity because... (need to flush out this idea more). Several studies have looked at what factors influence an individual’s risk-taking attitude. For example, economists have thought that wealth and demographic variables, such as age and sex, would be highly correlated with risk prone behavior, however the results have not supported this [@henrichArePeasantsRisk2002]. Rather, it has been found that risk preferences can be driven by emotional reactions to a risky situation, individual versus group consideration, and the instability of the environment in which an individual is making a decision [@eckelRiskLovingStorm2009; @liuTimeChangeWhat2012; @nolinReproductiveResponsesEconomic2016; @shuppRiskPreferenceDifferentials2008]. Therefore, risk-taking attitude, similar to risk sensitivity, is the product of social and environmental cues and has important implications for risk mitigating behavior.

Shortfall minimization is a critical component of decision-making under risk. As discussed previously, risk can be minimized by resource pooling and social learning. Since lidar is an expensive investment, funding opportunities and collaborations can help minimize and share the risk of investing in lidar. For example, the Washington Geological Survey was granted funding from 2015-2021 for the collection and distribution of lidar data and lidar-derived products. Therefore, individuals who adopt this technology in Washington have the potential to minimize their risk of investment by pooling resources. Due to this resource pooling, we expect Washington to see an increase in lidar adoption. In addition, social learning can influence the norms and beliefs that a flood risk manager holds about appropriate adaptive behavior. Research has shown that collective memory provides context and is an important indicator of a community’s flood risk culture [@viglioneInsightsSociohydrologyModelling2014; @wachingerRiskPerceptionParadoxImplications2013]. Collective memory represents a community’s ability to keep awareness of previous events that shape their decisions for the future. Often times, flood risk managers develop their collective memory based on information they learn from their peers and experience within their community. This awareness influences an individual to take action based on a collective idea of risk, rather than from individual belief. 
- add section about awareness

There are additional social and environmental factors that could moderate decision-making under flood risk. These include community preparedness, experiences, and actual flood risk. Community preparedness is the product of the environment the flood risk manager is making decisions in. Community preparedness provides an idea of how a flood risk manager view's their community ability to handle future floods. Furthermore, their past experiences with flooding (e.g. flooding outside designated flood zone or flooding damage) may influence how their decision-making under risk process. Lastly, this study also considered the objective flood risk of Washington and Idaho as summarized in Table 1. 

This next section discusses how we measured each of these contributing factors and our methods for survey analysis.

# 3 METHODS (~ 1,000 words)
**"Provide sufficient details to allow the work to be reproduced by an independent researcher. Methods that are already published should be summarized, and indicated by a reference. If quoting directly from a previously published method, use quotation marks and also cite the source. Any modifications to existing methods should also be described."**

add part about interviews
```{r, survey distribution, include=FALSE}
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
  filter(grepl("2", screen))

#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

a <- length(wa.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-13: 383 potential responses

# oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
  filter(grepl("2", screen))

#load distribution history
or.dist <- read.csv("data/or_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

b <- length(or.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 357-13: 344 potential responses


# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
c <- length(id.pot.responses$Email)
# 463 potential responses

total.sample.frame <-  a + b + c

```

```{r, survey data upload, include=FALSE}

idaho <- read.csv("data/id.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Idaho") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(idaho)

oregon <- read.csv("data/or.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses  
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Oregon") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(oregon)

washington <- read.csv("data/wa.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Washington") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(washington)

alaska <- read.csv("data/ak.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Alaska") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(alaska)

# Combine all four states into one dataset # 
survey.responses <- rbind(idaho, oregon, washington) %>% 
  subset(.,incr_no_flood!="") # remove additional blank responses
  

#write.csv(survey.responses, "C:/Users/tarapozzi/Documents/Manuscript/survey.responses.csv")

```

### *Survey design*
Talk about the specific survey questions that I end up analyzing here...

This survey covered several topics to measure predictors that could affect lidar adoption. This includes the following: awareness, preparedness, risk-taking attitude, collective memory, trust, and outcome efficacy. See Appendix A for a copy of each survey. 

### *Survey procedures and participants*
The survey's target respondent was the floodplain manager or administrator from participating and non-participating National Flood Insurance Program (NFIP) communities in Idaho and Washington in the Western United States. This also included individuals that may use lidar for flood risk management applications in conjunction such as Geographic Information System (GIS). Lidar, in this context, means either raw lidar data (e.g. LAS or point clouds) or lidar-derived data (e.g. digital elevation model (DEM)).The majority of sample respondents were municipal, state, and federal employees, as well as some private industry employees. In order to achieve our target population, the sample frame included several sources of contacts including NFIP coordinators, Association of State Floodplain Managers (ASFPM) recognized Certified Floodplain Mangers (CFM), county-level GIS administrators, the five largest cities and tribal GIS administrators if present, county and tribal emergency managers, the Federal Geospatial Data Coordination Contacts by State, and additional, relevant contacts for the 2019 Northwest Regional Floodplain Managers Association (NORFMA) Conference contact list. 

In addition, this survey data was collected through a structured, online survey distributed through Qualtrics to email addresses in our sample frame. The survey was sent to Idaho and Washington between May and July 2020. The survey took on average xx minutes. There were four to six email correspondence messages with potential survey participants over the course of four weeks. There was a total of `r a` potential survey respondents in Idaho and `r b` potential survey respondents in Washington. 

``` {r, response rate, include=FALSE}

id.rr <- round((96/a)*100, digits=2) # a is the total number of potential responses

or.rr <- round((58/b)*100, digits=2)

wa.rr <- round((54/c)*100, digits=2)

```

Additionally, a non-response analysis (how do I do this?) was run due to the low survey response rate of `r id.rr`% in Idaho, 'r or.rr'% in Oregon, and `r wa.rr`% in Washington. However, there were no significant distortions of representativeness found for age, gender, geographical area, or level of education (Need to check this). It is important to note, our data collection was completed during COVID-19 pandemic which required extra time and energy from emergency managers, our target population for this survey. For this reason, we expected a lower survey response than typical (unnecessary to say?).

## **Survey analysis**
A Hierarchical Bayesian approach with a Generalized Logistic Regression meets the model criteria for understanding risk because it is able to characterize non-linear, unpredictable outcomes. Furthermore, a logistic regression was used because the response variable, lidar use, is binary. Therefore, this analysis did not need account for ordered categorical response. This analysis uses a Markov Chain Monte Carlo algorithm to predict posterior distributions of each parameter's effect on lidar use. 

**insert bayes theorem**

This model has two-levels of analysis. The first level of actors is represented by individual respondents and the second level is the cluster represented by each state. Furthermore, this model follows a binomial distribution curve, where the distribution of lidar use, y_{ij}, can be modeled as follows (https://idiom.ucsd.edu/~rlevy/pmsl_textbook/chapters/pmsl_8.pdf): 

\[b_i \approx N(0,\sigma_b)\]

\[\eta_i = \mu_\alpha + \beta x_{ij}+...+\beta_kx_{ij} + b_i\]

\[\pi_i = \frac{e^{\eta}_i}{1+e^{\eta}_i}\]

\[y_{ij} \approx Binom(1, \pi_i)\] (1)

where $x_{ij}$, predictors, are the ith rows of the known design matrices x, and $\beta$ is a vector of regression parameters. The Bayesian approach allows for adjustment of uncertainty associated with each parameter on the final outcome (lidar use). In order to do this, each parameter has to be assigned a prior belief of that parameter value. The values for these parameters are fit by sampling from these distributions to maximize the likelihood under this model[@kwonClimateInformedFlood2008]. The regression parameters, $\beta$, are normally distributed, \[\beta_k\approx N(\eta_{\beta k}, \sigma_k)\]. Additionally, the parameters of this distribution, $\eta_{\beta k}$ and $\sigma_k$, also have prior distributions assigned to them that are constrained by 0 and a positive value (should I be more specific?).

The primary model included all predictors of interest with a varying intercept due to location. Subsequent models were run, isolating each predictor and lidar use with vary intercept and slope. In addition, we ran a model comparison and assessed the overall performance through Leave-One-Out Cross-Validation (LOOCV). This process provides an absolute metric for the model's predictive ability. Lastly, because this model had categorical predictors, we plotted the predicted probability against the observed proportion for some binning of the data (Levy, 2012) [this text has a great example of how to plot this]

# 4 RESULTS + DISCUSSION  (~2,000)
**"This should explore the significance of the results of the work, not repeat them. A combined Results and Discussion section is often appropriate. Avoid extensive citations and discussion of published literature."**

```{r, recode, include=FALSE}
survey.responses$lidaruse <- revalue(survey.responses$lidaruse, c("1"="1", "2"="0", "3"="0"))
idaho$lidaruse <- revalue(idaho$lidaruse, c("1"="1", "2"="0", "3"="0"))
oregon$lidaruse <- revalue(oregon$lidaruse, c("1"="1", "2"="0", "3"="0"))
washington$lidaruse <- revalue(washington$lidaruse, c("1"="1", "2"="0", "3"="0"))

# yes=1, no=0

# Direct Experiences
survey.responses$experience_1 <- revalue(survey.responses$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
survey.responses$experience_2 <- revalue(survey.responses$experience_2, c("1"="1", "2"="0"))
survey.responses$experience_3 <- revalue(survey.responses$experience_3, c("1"="1", "2"="0"))
survey.responses$experience_4 <- revalue(survey.responses$experience_4, c("1"="1", "2"="0"))
survey.responses$experience_5 <- revalue(survey.responses$experience_5, c("1"="1", "2"="0"))

idaho$experience_1 <- revalue(idaho$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
idaho$experience_2 <- revalue(idaho$experience_2, c("1"="1", "2"="0"))
idaho$experience_3 <- revalue(idaho$experience_3, c("1"="1", "2"="0"))
idaho$experience_4 <- revalue(idaho$experience_4, c("1"="1", "2"="0"))
idaho$experience_5 <- revalue(idaho$experience_5, c("1"="1", "2"="0"))

washington$experience_1 <- revalue(washington$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
washington$experience_2 <- revalue(washington$experience_2, c("1"="1", "2"="0"))
washington$experience_3 <- revalue(washington$experience_3, c("1"="1", "2"="0"))
washington$experience_4 <- revalue(washington$experience_4, c("1"="1", "2"="0"))
washington$experience_5 <- revalue(washington$experience_5, c("1"="1", "2"="0"))

oregon$experience_1 <- revalue(oregon$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
oregon$experience_2 <- revalue(oregon$experience_2, c("1"="1", "2"="0"))
oregon$experience_3 <- revalue(oregon$experience_3, c("1"="1", "2"="0"))
oregon$experience_4 <- revalue(oregon$experience_4, c("1"="1", "2"="0"))
oregon$experience_5 <- revalue(oregon$experience_5, c("1"="1", "2"="0"))


# closer the experience, higher the number for yes

# Trust-- reverse code
survey.responses$gov_trust <- revalue(survey.responses$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$gov_trust <- revalue(idaho$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$gov_trust <- revalue(oregon$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$gov_trust <- revalue(washington$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

survey.responses$science_trust <- revalue(survey.responses$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$science_trust <- revalue(idaho$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$science_trust <- revalue(oregon$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$science_trust <- revalue(washington$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

# Gov Involvement
survey.responses$gov_involve <- revalue(survey.responses$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
idaho$gov_involve <- revalue(idaho$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
oregon$gov_involve <- revalue(oregon$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
washington$gov_involve <- revalue(washington$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))

# 5=completely involved
# 4=mostly involved
# 3=moderately involved
# 2=somewhat involved
# 1=not at all involved

# Risk Perception
# future flood risk
survey.responses$future_1<- revalue(survey.responses$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_2<- revalue(survey.responses$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_3<- revalue(survey.responses$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_4<- revalue(survey.responses$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_5<- revalue(survey.responses$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

idaho$future_1<- revalue(idaho$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_2<- revalue(idaho$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_3<- revalue(idaho$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_4<- revalue(idaho$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_5<- revalue(idaho$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

oregon$future_1<- revalue(oregon$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_2<- revalue(oregon$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_3<- revalue(oregon$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_4<- revalue(oregon$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_5<- revalue(oregon$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

washington$future_1<- revalue(washington$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_2<- revalue(washington$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_3<- revalue(washington$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_4<- revalue(washington$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_5<- revalue(washington$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
# 100% chance of happening
# 75% chance of happening
# 50% chance of happening
# 25% chance of happening
# 0% chance of happening
# 1-5 to represent closeness

#increase number of floods
survey.responses$incr_no_flood <- revalue(survey.responses$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_no_flood <- revalue(idaho$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_no_flood <- revalue(oregon$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_no_flood <- revalue(washington$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3# decrease=1# stay the same=2

#increase severity of floods
survey.responses$incr_sev_flood <- revalue(survey.responses$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_sev_flood <- revalue(idaho$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_sev_flood <- revalue(oregon$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_sev_flood <- revalue(washington$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3
# decrease=1
# stay the same=2

# Demographics
# age is ordered correctly: 1= less than 20, 2=20-29 years, 3=30-39 years, 4=40-49 years, 5=50+ years
# education is ordered correctly: 1= some high school, 2= high school diploma, 3=college edu, no grad, 4= associates, 5=bachelors, 6= advanced
# gender is ordered fine: 1= male, 2=female

# Structural Barriers
# too expensive
survey.responses$barrier_1<- revalue(survey.responses$barrier_1, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

#lack of expertise
survey.responses$barrier_2<- revalue(survey.responses$barrier_2, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# sparse population
survey.responses$barrier_3<- revalue(survey.responses$barrier_3, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low rate of economic development
survey.responses$barrier_4<- revalue(survey.responses$barrier_4, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low flooding risk
survey.responses$barrier_5<- revalue(survey.responses$barrier_5, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# lack of political support
survey.responses$barrier_6<- revalue(survey.responses$barrier_6, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# add a column for barrier 7 which is if lidar is present or not

# SOEP
survey.responses$soep_1<- revalue(survey.responses$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
idaho$soep_1<- revalue(idaho$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
oregon$soep_1<- revalue(oregon$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
washington$soep_1<- revalue(washington$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))

# 10: risk loving
# 0: risk averse

#Gender all good, except "3" for other
survey.responses$gender <- revalue(survey.responses$gender, c("1"="1", "2"="2", "3"="NA"))
idaho$gender <- revalue(idaho$gender, c("1"="1", "2"="2", "3"="NA"))
oregon$gender <- revalue(oregon$gender, c("1"="1", "2"="2", "3"="NA"))
washington$gender <- revalue(washington$gender, c("1"="1", "2"="2", "3"="NA"))

#age doesn't need recoding

# Edcuation
survey.responses$education <- revalue(survey.responses$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
idaho$education <- revalue(idaho$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
oregon$education <- revalue(oregon$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
washington$education <- revalue(washington$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
# 1: some high school
# 6: advanced degree

# prepared 
survey.responses$prepared <- revalue(survey.responses$prepared, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all prepared
#5: completely prepared

#usefulness
survey.responses$usefulid <- revalue(survey.responses$usefulid, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all useful
#5: very useful

#flood zone
survey.responses$flood_zone <- revalue(survey.responses$flood_zone, c("1"="1", "2"="0"))
#1: flooding outside of flood zone
#2: no flooding outside


# NETWORK PROXIES
# amount of commmunication 
survey.responses$comm_1 <- revalue(survey.responses$comm_1, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_2 <- revalue(survey.responses$comm_2, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_3 <- revalue(survey.responses$comm_3, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_4 <- revalue(survey.responses$comm_4, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_5 <- revalue(survey.responses$comm_5, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_6 <- revalue(survey.responses$comm_6, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_7 <- revalue(survey.responses$comm_7, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_8 <- revalue(survey.responses$comm_8, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
#1: a few times a year
#2: once a month
#3: 2-3 times a month
#4: once a week
#5: several times a week
#6: several times a day

# alter lidar use
survey.responses$lidar_1 <- revalue(survey.responses$lidar_1, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_2 <- revalue(survey.responses$lidar_2, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_3 <- revalue(survey.responses$lidar_3, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_4 <- revalue(survey.responses$lidar_4, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_5 <- revalue(survey.responses$lidar_5, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_6 <- revalue(survey.responses$lidar_6, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_7 <- revalue(survey.responses$lidar_7, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_8 <- revalue(survey.responses$lidar_8, c("1"="1", "2"="2", "3"="3"))

#1: yes
#2: no
#3: i don't know
 
#expertise
# no need to revalue
#0: no expertise at all
#10: lots of expertise 
```
```{r, numeric conversion, include=FALSE}
survey.responses <- survey.responses %>%
  mutate_at(vars('years','lidaruse', 'interestlid','LocationLatitude', 'LocationLongitude', 'years', 'experience_1', 'experience_2', 'experience_3', 
                 'experience_4', 'experience_5', 'future_1', 'future_2', 'future_3', 'future_4', 'future_5', 
                 'currentmap', 'flood_zone', 'prepared', 'incr_no_flood', 'incr_sev_flood', 'barrier_1', 'barrier_2', 
                 'barrier_3', 'barrier_4', 'barrier_5', 'barrier_6', 'soep_1', 'age', 'gender', 
                 'gov_trust', 'gov_involve', 'education', 'science_trust', 'toolslid', 'toolslid1', 'usefulid', 'no_alters',
                 'alter_names_1_TEXT', 'alter_names_2_TEXT', 'alter_names_3_TEXT', 'alter_names_4_TEXT', 'alter_names_5_TEXT', 'alter_names_6_TEXT',
                 'alter_names_7_TEXT', 'alter_names_9_TEXT', 'comm_1', 'lidar_1', 'expertise_1_1', 'comm_2', 'lidar_2', 'expertise_2_1', 'comm_3', 'lidar_3', 'expertise_3_1',
                'comm_4', 'lidar_4', 'expertise_4_1','comm_5', 'lidar_5', 'expertise_5_1', 'comm_6', 'lidar_6', 'expertise_6_1', 'comm_7', 'lidar_7','expertise_7_1',
                'comm_8', 'lidar_8', 'expertise_8_1'), as.numeric) 
```

```{r, network proxies, include=FALSE}
### Network Proxies

# proportion of alters that use lidar
network.variables <- c("Email", "no_alters","comm_1","lidar_1","expertise_1_1","comm_2","lidar_2","expertise_2_1","comm_3","lidar_3","expertise_3_1",
                "comm_4","lidar_4","expertise_4_1","comm_5","lidar_5","expertise_5_1","comm_6","lidar_6","expertise_6_1","comm_7","lidar_7","expertise_7_1",
                "comm_8","lidar_8","expertise_8_1")

network.subset <- survey.responses[network.variables] %>%
                  drop_na(no_alters) 

lidar.variables <- c("Email", "lidar_1","lidar_2","lidar_3",
                     "lidar_4","lidar_5","lidar_6","lidar_7",
                     "lidar_8")

lidar.subset <- survey.responses[lidar.variables] 

lidar.subset$no_lidar_alters <-  rowSums(lidar.subset == 1, na.rm=TRUE) # this adds allthe lidar users in the network (rep by 1)

# merge number of alters that use lidar with original dataset

survey.responses <- join(survey.responses, lidar.subset[, c("Email","no_lidar_alters")], by= "Email", type="left")


# now calculate the proportion of lidar users for each respondent 

survey.responses$prop_lidar_network <- survey.responses$no_lidar_alters/survey.responses$no_alters # calculates the proportion of lidar users in the network


# let's look at average amount of communication respondent has with network 

comm.variables <- c("Email", "comm_1","comm_2","comm_3",
                    "comm_4","comm_5","comm_6","comm_7",
                    "comm_8")

comm.subset <- survey.responses[comm.variables] 

comm.subset$comm_total <-  rowSums(comm.subset[,2:9], na.rm=TRUE)
comm.subset$comm_ave <- round(rowMeans(comm.subset[,2:9], na.rm=TRUE), digits=0)


survey.responses <- join(survey.responses, comm.subset[, c("Email", "comm_total", "comm_ave")], by= "Email", type="left")


# now let's calculate total expertise in a responsdent's network
expertise.variables <- c("Email", "expertise_1_1","expertise_2_1","expertise_3_1",
                         "expertise_7_1","expertise_6_1","expertise_5_1","expertise_4_1",
                         "expertise_8_1")

expertise.subset <- survey.responses[expertise.variables]


expertise.subset$expert_total <-  rowSums(expertise.subset[,2:9], na.rm=TRUE) 
expertise.subset$expert_ave <- round(rowMeans(expertise.subset[,2:9], na.rm=TRUE), digits=0)

survey.responses <- join(survey.responses, expertise.subset[, c("Email", "expert_total", "expert_ave")], by= "Email", type="left")

```

```{r, model specific data, include=FALSE}
# Dataset for the model
model.variables <- c("lidaruse", "experience_1", "future_1", "incr_sev_flood",  "soep_1", "science_trust","gov_trust", "location", "prepared", "flood_zone","prop_lidar_network", "comm_total", "expert_total")

model.data <- survey.responses[model.variables]

str(model.data)

```

```{r, na omit, include=FALSE}
model.data.na.omit <- na.omit(model.data)
n = nrow(model.data.na.omit)
n # 145
```

```{r, lidar use stats, include=FALSE}
count(idaho$lidaruse==1)
id.lidaruse <- round((48/96)*100, digits=1)

count(oregon$lidaruse==1)
or.lidaruse <- round((36/58)*100, digits=1)

count(washington$lidaruse==1)
wa.lidaruse <- round((35/54)*100, digits=1)

```

## *4.1 Descriptive Statistics*

Table xx below compares the demographic and experience representation of survey responses from flood risk managers by region. 

``` {r, survey demographics, include=FALSE}
id.sample.size <- count(idaho$location=="Idaho")
or.sample.size <- count(oregon$location=="Oregon")
wa.sample.size <- count(washington$location=="Washington")

count(idaho$age)
count(oregon$age)
count(washington$age)
# age breakdown
id.age.2 <- round((2/96)*100, digits=0) # the denominator is the sample size
id.age.3 <- round((17/96)*100, digits=0) # the numerator is from 
id.age.4 <- round((27/96)*100, digits=0)
id.age.5 <- round((48/96)*100, digits=0)

or.age.2 <- round((2/58)*100, digits=0)
or.age.3 <- round((9/58)*100, digits=0)
or.age.4 <- round((17/58)*100, digits=0)
or.age.5 <- round((25/58)*100, digits=0) # some people didn't respond

wa.age.2 <- round((3/54)*100, digits=0)
wa.age.3 <- round((10/54)*100, digits=0)
wa.age.4 <- round((17/54)*100, digits=0)
wa.age.5 <- round((23/54)*100, digits=0)

# gender breakdown
id.gender.male <- round((59/96)*100, digits=0)
id.gender.female <- round((37/96)*100, digits=0)

or.gender.male <- round((35/58)*100, digits=0)
or.gender.female <- round((20/58)*100, digits=0)

wa.gender.male <- round((30/54)*100, digits=0)
wa.gender.female <- round((24/54)*100, digits=0)

#education breakdown
id.edu.ba.ma <- round((37+29)/96*100, digits=0) # represents the number of respondents with either a bachelors or advanced degree
or.edu.ba.ma <- round((21+26)/58*100, digits=0) 
wa.edu.ba.ma <- round((17+24)/54*100, digits=0)

#years in the industry
idaho$years <- as.numeric(idaho$years)
id.years <- round(summarise(idaho, avg=mean(years)), digits=1)

oregon$years <- as.numeric(oregon$years)
or.years <- round(summarise(oregon, avg=mean(years, na.rm=TRUE)), digits=1) ### THIS ISNT WORKING

washington$years <- as.numeric(washington$years)
wa.years <- round(summarise(washington, avg=mean(years)), digits=1)
```

```{r, survey demographics table, echo=FALSE, fig.cap="\\label{demographics}"}
survey.dem <- matrix(c(96, 58, 54, "39%", "34%", "44%", "69%","81%", "76%",id.years, or.years, wa.years), ncol=3, byrow=TRUE)
colnames(survey.dem) <- c("Idaho", "Oregon", "Washington")
rownames(survey.dem) <- c("Sample Size", "Female", "University Education", "Average Flood Risk Experience (years)")
knitr::kable(survey.dem, caption="Comparative descriptive statistics for survey demographics across Idaho, Oregon, and Washington.")
```
The results show no significant distortions of representativeness found for age, gender, geographical
area, or level of education (Need to check this). [insert section about how FRM descriptive stats compare to FRM in other states?]

Table xx gives an overview of selected answers to questions relating to the respondent's individual experiences and knowledge regarding floods in their primary community of work, collective influence, and their adoption of lidar.

``` {r, survey variable decriptions, echo=FALSE, fig.cap="\\label{survey variables}"}

# Knowledge of flooding outside designated FEMA flood zone in the community
id.floodzone <- round((table(idaho$flood_zone == 1)/length(idaho$flood_zone))*100, digits=1) # reports results as true/false # is there a function to just report true?
id.floodzone <- 49.0
or.floodzone <- round((table(oregon$flood_zone == 1)/length(oregon$flood_zone))*100, digits=1)
or.floodzone <- 34.5
wa.floodzone <- round((table(washington$flood_zone == 1)/length(washington$flood_zone))*100, digits=1) 
wa.floodzone <- 59.3
# 1=yes

#Direct experience with flood damage in community
id.experience <- round((table(idaho$experience_1 == 1)/length(idaho$experience_1))*100, digits=1)
id.experience <- 79.2
or.experience <- round((table(oregon$experience_1 == 1)/length(oregon$experience_1))*100, digits=1)
or.experience <- 72.4
wa.experience <- round((table(washington$experience_1 == 1)/length(washington$experience_1))*100, digits=1) 
wa.experience <- 85.2
# 1=yes

#Over 50% chance of direct experience with future flood damage in the community
id.future <- round((table(idaho$future_1 > .25 )/length(idaho$future_1))*100, digits=1)
id.future <- 28.1
or.future <- round((table(oregon$future_1 > .25)/length(oregon$future_1))*100, digits=1)
or.future <- 39.7
wa.future <- round((table(washington$future_1 > .25)/length(washington$future_1))*100, digits=1) 
wa.future <- 48.1
# includes those who selected 50, 75, and 100% chance of event happening

#Perceived Increase in Flood Severity
id.incr.sev.flood <- (table(idaho$incr_sev_flood == 3)/length(idaho$incr_sev_flood))*100
id.incr.sev.flood <- 38.5
or.incr.sev.flood <- (table(oregon$incr_sev_flood == 3)/length(oregon$incr_sev_flood))*100
or.incr.sev.flood <- 41.4
wa.incr.sev.flood <- (table(washington$incr_sev_flood == 3)/length(washington$incr_sev_flood))*100
wa.incr.sev.flood <- 57.4
# 3=increasing

#Feel community is at least mostly prepared for a flood event
id.prepared <- (table(idaho$prepared == 3)/length(idaho$prepared))*100
id.prepared <- 44.8
or.prepared <- (table(oregon$prepared == 3)/length(oregon$prepared))*100
or.prepared <- 36.2
wa.prepared <- (table(washington$prepared == 3)/length(washington$prepared))*100
wa.prepared <- 40.7

#Risk-taking attitude
idaho$soep_1 <- as.numeric(idaho$soep_1)
id.soep <- round(summarise(idaho, avg=mean(soep_1, na.rm=TRUE)), digits=1)
id.soep <- 2.8
oregon$soep_1 <- as.numeric(oregon$soep_1)
or.soep <- round(summarise(oregon, avg=mean(soep_1, na.rm=TRUE)), digits=1)
or.soep <- 3.3
washington$soep_1 <- as.numeric(washington$soep_1)
wa.soep <- round(summarise(washington, avg=mean(soep_1, na.rm=TRUE)), digits=1)
wa.soep <- 3.7
#0-risk averse to 10-risk loving

#Trust in usefulness of government FRM products
id.gov.trust <- (table(idaho$gov_trust > 3)/length(idaho$gov_trust))*100
id.gov.trust <- 76.0
or.gov.trust <- (table(oregon$gov_trust > 3)/length(oregon$gov_trust))*100
or.gov.trust <- 70.7
wa.gov.trust <- (table(washington$gov_trust > 3)/length(washington$gov_trust))*100
wa.gov.trust <- 75.9
#4-somewhat trust, 5-completely trust

#Trust in accuracy of FRM scientific products
id.science.trust <- (table(idaho$science_trust > 3)/length(idaho$science_trust))*100
id.science.trust <- 82.3
or.science.trust <- (table(oregon$science_trust > 3)/length(oregon$science_trust))*100
or.science.trust <- 81.0
wa.science.trust <- (table(washington$science_trust > 3)/length(washington$science_trust))*100
wa.science.trust <- 90.7
#4-somewhat trust, 5-completely trust

#Proportion of lidar users in FRM network
idaho <- join(idaho, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
id.prop.lidar <- (round(summarise(idaho, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
id.prop.lidar <- 35.0
oregon <- join(oregon, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
or.prop.lidar <- (round(summarise(oregon, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
or.prop.lidar <- 40.0
washington <- join(washington, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
wa.prop.lidar <- (round(summarise(washington, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
wa.prop.lidar <- 42.0
#Total communication in FRM network
id.comm.network <- (round(summarise(idaho, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
id.comm.network <- 3
or.comm.network <- (round(summarise(oregon, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
or.comm.network <- 3
wa.comm.network <- (round(summarise(washington, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
wa.comm.network <- 3

#Perceived expertise in FRM network
id.expert.network <- (round(summarise(idaho, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
id.expert.network <- 6.6
or.expert.network <- (round(summarise(oregon, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
or.expert.network <- 7.3
wa.expert.network <- (round(summarise(washington, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
wa.expert.network <- 6.7

#Use of lidar for FRM
id.lidaruse <- (table(idaho$lidaruse == 1)/length(idaho$lidaruse))*100
id.lidaruse <- 50.0
or.lidaruse <- (table(oregon$lidaruse == 1)/length(oregon$lidaruse))*100
or.lidaruse <- 62.1
wa.lidaruse <- (table(washington$lidaruse == 1)/length(washington$lidaruse))*100
wa.lidaruse <- 64.8

survey.variable.stats <- matrix(c(id.floodzone, or.floodzone, wa.floodzone, id.experience, or.experience, wa.experience, id.future, or.future, wa.future, id.incr.sev.flood, or.incr.sev.flood, wa.incr.sev.flood, id.prepared, or.prepared, wa.prepared, id.soep, or.soep, wa.soep, id.gov.trust, or.gov.trust, wa.gov.trust, id.science.trust, or.science.trust, wa.science.trust, id.prop.lidar, or.prop.lidar, wa.prop.lidar, id.comm.network, or.comm.network, wa.comm.network, id.expert.network, or.expert.network, wa.expert.network, id.lidaruse, or.lidaruse, wa.lidaruse), ncol=3, byrow=TRUE)
colnames(survey.variable.stats) <- c("Idaho", "Oregon", "Washington")
rownames(survey.variable.stats) <- c("Knowledge of flooding outside designated FEMA floodzone in the community (%)", "Direct experience with flood damage in community (%)", "Direct experience with future flood damage in the community (%)","Perceived Increase in Flood Severity (%)", "Feel community is at least somewhat prepared for a flood event (%)", "Average risk-taking attitude /n (0 to 10 with 10 being risk-loving)", "Trust in usefulness of government FRM products (%)", "Trust in accuracy of FRM scientific products (%)", "Proportion of lidar users in FRM network (%)", "Average amount of communication with FRM network* ", "Perceived expertise in FRM network (0 to 10 with 10 being of highest expertise)", "Use lidar for FRM (%)" )
knitr::kable(survey.variable.stats, caption="Descriptive statistics of survey question responses by State.")
```
*should I break this table up? It could be broken up as follows: table 1- experience/future beliefs about flooding, table 2- trust, table 3- risk aversion, network OR table 1- individual variables (experience, future beliefs, trust, risk attitude) and table 2- collective variables (network)*

These results are representative for survey respondents who adopted lidar. For those that didn't use lidar, they were asked what barriers they face to adoption. Table XX reflects the results of barriers that flood risk managers face when adopting lidar into their practice. 

*add table summarizing barriers*

## *4.2 Estimation Results* 

```{r, full model, include=FALSE}

full.mod <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + prepared + soep_1 + future_1 + gov_trust + science_trust + flood_zone + prop_lidar_network + comm_total + expert_total + (1|location), data=model.data.na.omit, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 
summary(full.mod)
plot(full.mod)
plot(full.mod, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod))
pp_check(full.mod) # this checks the posterior predictive ability. looks good

# informed priors model
full.mod.priors <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + prepared + soep_1 + future_1 + gov_trust + science_trust + flood_zone + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = normal(location = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), # leaving the intercept at 0 had better results than using the intercept value from the previous model
                           scale = c(3.7, 6.4, 3.3, 1.1, 8.3, 2.8, 3.3, 5.0, 7.0, 0.3, 0.1), # this is from prior_summary of full.mod without priors
                           autoscale = FALSE), # this specifies prior between 0 and 1 because it is a binonmial response variable
                           prior_intercept = normal (location = 0, 
                           scale = 2.5, 
                           autoscale = FALSE), # intercept can only be between 0 and 1 
                           data=model.data.na.omit, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 
summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod.priors))
#pp_check(full.mod.priors)

# null model

null.mod <- stan_glm(lidaruse ~ 1, data=model.data.na.omit, family=binomial(link=logit))

## Model comparison

loo_compare(loo(full.mod), loo(full.mod.priors))
# Based on the LOOIC comparison, full mod without set priors has the greatest comparative out-of-sample predictive preformance
```

$\label{survey variables}$ displays variation among Idaho, Oregon, and Washington when it comes to lidar adoption.  We explored why this variation may exist through an exploratory analysis of individual and collective factors that influence a flood risk managers decision-making. Through the use of the programming language R and program Rstudio,  this analysis used Bayesian theory to analyze the effect of various predictors of lidar adoption. The analysis used four Monte Carol Markov Chains (MCMC) with 1,000 iterations for warmup and an additional 1,000 iterations for the model. Additionally, convergence was checked by visually inspecting the MCMC trace plots of the model parameters. We assessed effective sample size and checked model convergence, indicated by R-hat statistics close to 1 and stable, well-mixed chains  [@gelmanBayesianWorkflow2020]. 

$\label{tab:summary_stats}$ displays the results from a varying intercept model that considers the effect of all these factors on lidar use, while using partial pooling for location. Partial pooling is the benefit of using a hierarchical model and allows the model to appropriately generalize across similar but not identical settings [@gelmanBayesianWorkflow2020]. This works well in our study because we are looking at the regional variation of lidar adoption. In addition, this methodology allows for propagation of uncertainty throughout the model, which is important because...The results of this analysis suggest that proportion of lidar users and total amount of expertise in a respondent's network had the most significant effect on lidar adoption. In addition, awareness and knowledge of flood risk and risk-taking attitude had moderate impact on lidar use. Direct experience, preparedness, and trust had a minor impact on lidar use. 

```{r, model summary table, echo=FALSE, fig.cap="\\label{summary_stats}"}
#these are based on summary(full.mod) which returned a slightly better result that fullmod.priors
summary.stats <- matrix(c(-4.2, 2.1, -7.7, -0.9, 
                          0.5, 0.4, -0.2, 1.2, 
                          0.2, 0.7, -1.0, 1.3, 
                          0.5, 0.4, -0.1, 1.1, 
                          0.1, 0.1, -0.1, 0.3, 
                          0.9, 0.9, -0.6, 2.4, 
                          -0.4, 0.4, -1.0, 0.4, 
                          0.0, 0.4, -0.7, 0.7, 
                          1.0, 0.5, 0.1, 1.8, 
                          5.5, 0.9, 4.0, 7.1, 
                          0.0, 0.0, 0.0, 0.1, 
                          0.0, 0.0, 0.0, 0.0), ncol=4, byrow=TRUE) #this is the plogis values for mean and SD ### UPDATE THESE NUMBERS
colnames(summary.stats) <- c( "Mean", "S.D.", "5%", "95%")
rownames(summary.stats) <- c("Intercept", "Increase in Flood Severity", "Direct Experience with Flood Damage in Community", "Level of Community Preparedness", "Risk Attitude", "Future Flood Risk", "Trust in Usefulness of Government FRM Products", "Trust in Accuracy of FRM Scientific Products", "Flooding outside of Floodzone", "Proportion of Lidar Users in Respondent's FRM Network", "Level Communication with Respondent and their FRM Network", "Total Expertise in Respondent's FRM Network")
knitr::kable(summary.stats, caption="Estimation results from the model")
```

In addition, we ran a model with informative priors based on the prior summary of the model with weakly informative priors. However, this model did not do a better predictive probability so we didn't not explore it further (see model chunk code above). *am I inputting my priors wrong? I feel like it should have made the model's predictive ability better?*

## *Bayesian posteriors*

This section focuses on the explanatory power of each predictor and summarizes the effect size of predictor. The full model had a Bayesian R-squared value of 'r full.mod.r2`. The following section breaks down the influence of each predictor model fit. We calculated this running by individual models with predictor of interest and lidar use, with varying location. $\label{model_fit}$ summarizes these findings. *add sd & ci to table?*

``` {r, model fit table, echo=FALSE, fig.cap="\\label{model_fit}"}
model.fit <- matrix(c("", "", "", .51,
                      0.2, 0.3, 0.6, 0.05,
                      0.1, 0.1, 0.1, 0.01, 
                      0.0, 0.1, 0.1, 0.02,
                      0.0, 0.0, 0.2, 0.03,
                      0.5, 0.4, 0.6, 0.03,
                      0.0, -0.1, 0.0, 0.01,
                      -0.1, 0.0, 0.1, 0.02,
                      0.5, 0.8, 0.5, 0.05,
                      4.9, 5.2, 3.5, 0.35,
                      0.0, 0.0, 0.1, 0.05,
                      0.0, 0.0, 0.1, 0.09), ncol=4, byrow=TRUE) #this is the plogis values for mean and SD ### UPDATE THESE NUMBERS
colnames(model.fit) <- c( "Idaho", "Oregon.", "Washington", "Bayes R2")
rownames(model.fit) <- c("Full Model", "Increase in Flood Severity", "Direct Experience with Flood Damage in Community", "Level of Community Preparedness", "Risk Attitude", "Future Flood Risk", "Trust in Usefulness of Government FRM Products", "Trust in Accuracy of FRM Scientific Products", "Flooding outside of Floodzone", "Proportion of Lidar Users in Respondent's FRM Network", "Level Communication with Respondent and their FRM Network", "Total Expertise in Respondent's FRM Network")
knitr::kable(model.fit, caption="Slope estimation for influfence of each predictor on lidar use by State")

```
*add plot with posterior distributions?*

### *Significant Predictors of Lidar Adoption*

#### **Proportion of Lidar Use in Network**


```{r, proportion of lidar users model, include=FALSE}
prop.lidar.mod <- stan_glmer(lidaruse ~ (1+prop_lidar_network|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(prop.lidar.mod)
median(bayes_R2(prop.lidar.mod))
plot(prop.lidar.mod, "areas", prob = 0.95, prob_outer = 1)

b.id.prop.lidar <- 4.9
b.or.prop.lidar <- 5.2
b.wa.prop.lidar <- 3.5

```

Our results suggest that the respondent's social network has the largest effect on lidar use. More specifically, the proportion of respondent's alters that use lidar is significantly correlated with lidar use. The survey measured this by asking the respondent to report yes, no, or I don't know if their FRM network alter used lidar. Furthermore, Washington had a stronger correlation. Oregon had the largest correlation of 'r b.or.prop.lidar', where as Washington had the smallest of 'r b.id.prop.lidar'. These results are in line with what we expected to see because of the strong influence social learning has on technology adoption. 
*should I add more network information here? like number of alters?*

```{r, proportion of lidar users, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:prop.lidar.plot"}
prop.lidar.plot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Proportion of Lidar Users in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

prop.lidar.boxplot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Proportion of Lidar Users in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(prop.lidar.plot, prop.lidar.boxplot)

```

#### **Total Expertise of Alters in Network**
```{r, total expertise of FRM network model, include=FALSE}
expert.mod <- stan_glmer(lidaruse ~ (1+expert_total|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(expert.mod)
median(bayes_R2(expert.mod))
plot(expert.mod, "areas", prob = 0.95, prob_outer = 1)

b.id.expert <- 0.0
b.or.expert <- 0.0
b.wa.expert <- 0.1

```
The results on this are a bit confusing. While the slopes are 0, the r2 suggests that it contributes a significant amount to the variation? 

```{r, expertise, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:expertise.plot"}
expertise.plot <- ggplot(model.data.na.omit, aes(x = expert_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Perceived Expertise in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

expertise.boxplot <- ggplot(model.data.na.omit, aes(x = expert_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Total Perceived Expertise in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(expertise.plot, expertise.boxplot)

```

### *Moderate Predictors of Lidar Adoption*

#### **Total Communication with Alters in Network**
```{r, total communication with FRM network model, include=FALSE}
comm.mod <- stan_glmer(lidaruse ~ (1+comm_total|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(comm.mod)
median(bayes_R2(comm.mod))
plot(comm.mod, "areas", prob = 0.95, prob_outer = 1)

b.id.comm <- 0.0
b.or.comm <- 0.0
b.wa.comm <- 0.1

```

Similar results to expertise, just lower r2 value. 

```{r, comm in network, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:comm.plot"}
comm.plot <- ggplot(model.data.na.omit, aes(x = comm_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Communication with Alters in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

comm.boxplot <- ggplot(model.data.na.omit, aes(x = comm_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Total Communication with Alters in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(comm.plot, comm.boxplot)

```

#### **Awareness of Climate Change**

```{r, awareness model, include=FALSE}
awareness.mod <- stan_glmer(lidaruse ~ (1+incr_sev_flood|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(awareness.mod)
plot(awareness.mod, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(awareness.mod))

b.id.awareness <- 0.2
b.or.awareness <- 0.3
b.wa.awareness <- 0.6

```

Awareness was measured by asking the survey respondent about the perception of the average severity of flood damage in their community decreasing, staying the same, or increasing. Overall, there was a positive correlation between awareness of future risk and lidar use. Washington had largest slope of  This could be due to Washington having seen significantly more damage and portion of population at risk to floods shown in $\label{tab:state_comp}$. 

```{r, awareness, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:awarnessplot"}
awareness.plot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

awareness.boxplot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(awareness.plot, awareness.boxplot)

```

#### **Future Flood Damage**
```{r, future damage model, include=FALSE}
future1.mod <- stan_glmer(lidaruse ~ (1+future_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(future1.mod)
median(bayes_R2(future1.mod))

b.id.future <- 0.5
b.or.future <- 0.4
b.wa.future <- 0.6

```


Building off the first predictor, awareness, it is important for a community keep awarness high for it often has influence on how individuals may respond to risk. This analysis also investigated this awareness by examining the perceived future risk of damage from flooding. The survey measured this by asking the respondent how likely they think damage to property in their community will happen in the next 30 years and could choose 0%, 25%, 50%, 75%, or 100% of happening. Washington had the strongest correlation of future flood risk and lidar use with a slope of 'r b.wa.future' and Oregon with the smallest of 'r b.or.future'.

```{r, future damage, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:memplot"}
future1.plot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

future1.boxplot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

ggpubr::ggarrange(future1.plot, future1.boxplot)

```

#### **Flooding Outside of the Flood Zone**
This study also examined the awareness of the flood risk manager on historical flooding outside the designated FEMA flood zone in their community. This question was helpful to gauge the accuracy of floodplain maps. The survey measured this by asking the respondent if they were aware of floods happening outside of the designated flood zone on their community flood maps with a binary response of yes or no. Oregon had the strongest correlation of reporting flooding outside the flood zone and adopting lidar with a slope of 'r b.or.flood zone'.

```{r, flood zone model, include=FALSE}

floodzone.mod <- stan_glmer(lidaruse ~ (1+flood_zone|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(floodzone.mod)
median(bayes_R2(floodzone.mod))


b.id.floodzone <- 0.5
b.or.floodzone <- 0.8
b.wa.floodzone <- 0.5

```

```{r, floodzone damage, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:floodzone.plot"}
floodzone.plot <- ggplot(model.data.na.omit, aes(x = flood_zone, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

floodzone.boxplot <- ggplot(model.data.na.omit, aes(x = flood_zone, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(floodzone.plot, floodzone.boxplot)

```


#### **Risk Attitude**

```{r, risk attitude model, include=FALSE}
soep1.mod <- stan_glmer(lidaruse ~ (1+soep_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(soep1.mod)

b.id.soep <- 0.0
b.or.soep <- 0.0
b.wa.soep <- 0.1

```


As discussed earlier, there is a risk inherent with adopting a new technology such as lidar due to variable time delay and reward. This was measured by asking respondents to report their general risk tolerance between 0, risk averse, to 10, risk-loving. Washington was the only state that displayed a correlation between risk-taking attitude and lidar with a slope of 'r b.wa.soep'. In addition, Washington also reported the least risk aversion, on average, and therefore may explain flood risk managers willingness to take on the risk of adopting a new technology.

```{r, risk attitude, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:soepplot"}
soep1.plot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

soep1.boxplot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(soep1.plot, soep1.boxplot)
```

### *Minor predictors of lidar use*

```{r, experience model, include=FALSE}
exp.mod <- stan_glmer(lidaruse ~ (1+experience_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(exp.mod)

b.id.exp <- 0.1
b.or.exp <- 0.1
b.wa.exp <- 0.1

```

#### **Direct Experience**

Direct experience has been studied extensively in the past as a significant predictor of individual risk perception and behavior. Our survey asked respondents to report if they had experienced damage in their community, with yes or no responses. Our results found a minor effect of experience on lidar use and all three states had the same slope of 0.1. Previous research has found variable effects of experience on behavior and suggest that measuring the intensity of the event experience could provide a more informative measure. 


```{r, experience, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:expplot"}
exp.plot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

exp.boxplot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(exp.plot, exp.boxplot)
```

#### **Trust in Scientific FRM Products**

```{r, science trust model, include=FALSE}
science.mod <- stan_glmer(lidaruse ~ (1+science_trust|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(science.mod)

b.id.science <- -0.1
b.or.science <- 0.0
b.wa.science <- 0.1 

```

The survey measured this by asking respondents to report how much they trusted the usefulness of FEMA's flood risk management scientific products on a likert scale from not at all to completely. Our results found that Idaho had a negative slope of 'r b.id.science' on lidar use, where as Washington had a positive slope of 'r b.wa.science'. This is an interesting contradiction between the two states...

```{r, science trust, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:outeffplot"}
science.plot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
 scale_x_discrete(limits=c(1, 5), labels=c("Strongly distrust", "Strongly trust")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

science.boxplot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Strongly distrust", "Neither trust nor distrust", "Strongly trust")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(science.plot, science.boxplot)
```

#### **Prepared**

```{r, prepared model, include=FALSE}
prep.mod <- stan_glmer(lidaruse ~ (1+prepared|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(prep.mod)

b.id.prep <- 0.0
b.or.prep <- 0.1
b.wa.prep <- 0.1

```

The survey measured perceived community preparedness by asking respondents to report how prepared they felt their community was for a flood event with likert response options from not at all prepared to completely prepared. Both Oregon and Washington had a positive slope of 'r b.or.prep' between preparedness and lidar use, where Idaho had no relationship. 

```{r, prepared, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:prepplot"}
prep.plot <- ggplot(model.data.na.omit, aes(x = prepared, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Level of preparedness', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

prep.boxplot <- ggplot(model.data.na.omit, aes(x = prepared, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Level of preparedness', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(prep.plot, prep.boxplot)
```

#### **Trust in Government FRM Products**

```{r, gov trust model, include=FALSE}
gov.mod <- stan_glmer(lidaruse ~ (1+gov_trust|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(gov.mod)

b.id.gov <- 0.0
b.or.gov <- -0.1
b.wa.gov <- 0.0

```

Often times trust in the government has been measured in relation to ability to adapt. Although, most research has been conducted in terms of the trust the public has for government officials to protect them from floods. This survey asked flood risk managers how much they trust the usefulness of the products the federal government develops for FRM. Idaho and Washington both did not report an effect of this on lidar use, however Oregon had a negative slope of 'r b.or.gov'. Overall, there was a not a significant relationship between trust in government products and lidar use. This could be because flood risk managers in Oregon feel that these products are sufficient and they do not need additional data from lidar.  

```{r, gov trust, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:govplot"}
gov.plot <- ggplot(model.data.na.omit, aes(x = gov_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Trust in government FRM products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

gov.boxplot <- ggplot(model.data.na.omit, aes(x = gov_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Trust in government FRM products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(gov.plot, gov.boxplot)
```


## *Additional analyses*
**haven't touched this part of the analysis yet. I think feedback on the first part would be super helpful to direct this next section**

The following are parts of the analysis that I am not sure if I should include or not: 

```{r, exploratory factor analysis, include=FALSE}
#Do i need to do this?? https://www.r-bloggers.com/2018/05/exploratory-factor-analysis-in-r/

#Loading the dataset
#bfi_data=model.data
#Remove rows with missing values and keep only complete cases
#bfi_data=bfi_data[complete.cases(bfi_data),]
#Create the correlation matrix from bfi_data
#bfi_cor <- cor(bfi_data)
#Factor analysis of the data
#factors_data <- fa(r = bfi_cor, nfactors = 3)
#Getting the factor loadings and model analysis
#factors_data
```

```{r, aic, include=FALSE}
## Not sure how helpful this is...
# Full model using all predictors AIC or BIC # However, this is not a good approach for data with lots of na's
# https://bookdown.org/egarpor/PM-UC3M/app-nas.html

#model.data.na.omit <- na.omit(model.data)
#n = nrow(model.data.na.omit)
#n # 128

# k = log(n): penalty for BIC rather than AIC
#AIC.lm <- lm(lidaruse ~ ., data=model.data.na.omit,  family= binomial(link = "logit")) #I've been getting a warning about divergent transitions after warmup. In order to make this better, I am going to make the step size smaller, I also have eliminatead varying effects for the purposes of AIC # Okay looks like I can only run a LM, GLM doesn't provide step information for AIC

#AIC
#mod.AIC = step(AIC.lm, k=2) 
# the best model is with flood_zone, future_1, and incr_sev_flood
```

```{r, k fold, include=FALSE}

#kfold1<- kfold(mod1, K=10) 
#kfold1

#kfold2 <- kfold(mod2, K=10) 
#kfold2

#loo_compare(kfold1, kfold2) #mod2 is slightly better
```

```{r, LOOCV, include=FALSE}

#Result_G <- data.frame(matrix(nrow=nrow(model.data.na.omit), ncol=6))

#set the column names
#colnames(Result_G) <- c("MedianG", "LowerBound1_G", "LowerBound2_G", "UpperBound1_G", "UpperBound2_G", "R2_Bayes")

#do the leave one out cross validation
# create for loop function
#for(i in 1:length(model.data.na.omit$lidaruse)){ # for all the data points
 # sub_dat <- model.data.na.omit[-i,] #remove each one at a time
 # m_sub <-  stan_glmer(lidaruse ~ incr_sev_flood + soep_1 + future_1 + flood_zone +
                   #science_trust + (1|location), 
                  # prior = normal(location = c(0, 0, 0, 0, 0), 
                  # scale = c( 3.703526, 1.078133, 8.236779, 4.987931, 3.271231), 
                  # autoscale = FALSE), # this specifies prior between 0 and 1 because it is a binonmial response variable
                           #prior_intercept = normal (location = 0, 
                          # scale = 2.5, 
                          # autoscale = FALSE), # intercept can only be between 0 and 1 
                   #data=model.data.na.omit, family=binomial(link=logit), control = list(adapt_delta = 0.99))
  #post=posterior_predict(m_sub, model.data.na.omit[i,], draws=4000)
  #r2=r2_bayes(m_sub)# predict that point
 #Result_G[i,1]=quantile(post, 0.5) # fill a df with the credibility intervals
  #Result_G[i,2]=quantile(post, 0.25) 
  #Result_G[i,3]=quantile(post, 0.025)
  #Result_G[i,4]=quantile(post, 0.75)
  #Result_G[i,5]=quantile(post,0.975)
  #Result_G[i,6]=r2
#}

#write.csv(Result_G, "G:/Everyone-Temp/TaraPozzi/Survey/Analysis/data/Result_G.csv")
```

```{r, model plot, echo=FALSE, include=FALSE}
#pplot<-plot(mod2.priors, "areas", prob = 0.95, prob_outer = 1)
#pplot+ geom_vline(xintercept = 0)


# let's simulate data to see what how well our model is predicting
#incr_sev_flood.sim <- rep(c(1:3), each=129)

# set other predictor variables to their lowest affect at their minimum

#simdata <-add_fitted_draws(newdata=data.frame(incr_sev_flood= incr_sev_flood.sim,
                                              #soep_1=5,
                                             # future_1=1,
                                             # flood_zone=2,
                                             # science_trust=1),
                          # mod2.priors) 

## Plot the results 
#sev.plot <- ggplot(simdata, aes(as.factor(incr_sev_flood), .value)) + 
  #geom_boxplot() + 
  #labs(x = "Flood severity experienced \nby survey respondent", y = "Effect on probability of LiDAR adoption \n while holding other IVs at lowest values") +
 # theme_bw() 
  
```





# 5 CONCLUSION (~500)
**"The main conclusions of the study may be presented in a short Conclusions section, which may stand alone or form a subsection of a Discussion or Results and Discussion section."**

While this theoretical model has never been applied to understanding risk in hazard management, I think that it could provide improved insight into significant predictors of long-term risk mitigation. This paper examines alternative predictors to risk perception in an effort to address why risk perception may not align with long-term risk mitigation behavior, also called the "Risk Perception Paradox." From a behavioral ecology perspective, these predictors could be cultural and contextual factors that moderate risk perception's effect size on decision-making and behavior. This paper investigates how these predictors may affect lidar adoption in flood risk management in Idaho and Washington. This makes an interesting and effective case study because the benefits of this technology have a variable reward and time delay, two key factors that can feel risky in adopting this technology. Furthermore, the findings from this research could have important implications for the risk field of research and advance our understanding of the driving factors of an individual's long-term risk mitigation behavior. 

# 5 Literature Cited