---
title: "Chapter 1 Manuscript"
author:
- name: Tara Pozzi
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  word_document: default
  pdf_document: default
thanks: '.....'
affiliation: Boise State University
runhead: XXXX
geometry: margin=1in
fontsize: 12pt
csl: crm.csl
bibliography: references.bib
indent: yes
colorlinks: yes
---

```{r setup, include=FALSE}
library(citr) # doesn't work in this version of r...
library(tidyr)
library(ggplot2)
library(ggplotgui) # doesn't work in this version of r 
library(ggrepel)
library(plotly)
library(RColorBrewer)
library(here)
library(tinytex)
library(psych)
library(pastecs)
library(rstanarm)
library(loo) 
library(tidybayes)
library(tidyverse)  # ggplot, dplyr, %>%, and friends
library(ggdag)  # Make DAGs with ggplot
library(dagitty)  # Do basic DAG math
library(broom)  # For converting model output to data frames
library(clusterSim)
library(caret) # model comparison
library(plyr) # helps with model comparison computation
library(performance)
library(rworldmap)
library(hrbrthemes)
library(ggplotAssist)
library(dplyr)
library(purrr)
library(forcats)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(rstan)
library(bookdown) # for cross-referencing
library(knitr) #for global options
library(formatR) 
```

```{r, global options, include=FALSE}
# Setup options for code decoration
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
highlight = TRUE)

# Setup options for code cache
opts_chunk$set(cache = 2, cache.path = "cache/")

# Setup options for plots
opts_chunk$set(fig.path = "Figures_MS/", dev = c("pdf", "png"),
dpi = 300)

## Locate figures as close as possible to requested position
## (=code)
opts_chunk$set(fig.pos = "H")

```

Abstract
========================
**"A concise and factual abstract is required. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself."**

# HIGHLIGHTS
*Come back and edit this when results and discussion are finished*

1. There is between state variation of lidar use between Idaho and Washington.

2. Data suggests a more variable environment lends to flood risk manager's making more risk-prone decisions.

3. Social learning plays a critical role in shortfall minimization and lidar adoption.

# 1 Introduction
Floods are one of the most frequent and destructive natural disasters in the United States [@pralleDrawingLinesFEMA2019;@RiskMappingAssessment]. Flood events disrupt and damage the technical, ecological, social, cultural, political, and economic landscapes causing incalculable expenses to our society oftentimes leaving vulnerable groups even more at risk in the future [@howellDamagesDoneLongitudinal2019]. Since the National Centers for Environmental Information (NCEI) began tracking natural disaster events in 1980, there has been an increase in flood events in the U.S., some of those with unprecedented amounts of rainfall. This is because as temperatures rise there is an increase in the amount of water vapor in the atmosphere, which increases the potential for extreme rainfall events. In addition to increased flood risk from climate change, there is an increasing rate of population growth and urbanization in coastal and inland floodplains [@pralleDrawingLinesFEMA2019; @schanzeFloodRiskManagement2006]. In 2015, 21.8 million (6.87%) of the U.S. population was identified as being exposed to a 100-year flood; meaning they lived in a location that would be inundated by a flood event that has a 1 in 100 chance of happening each year [@qiangDisparitiesPopulationExposed2019]. 100-year flood events are based on historical rainfall patterns, however this probability can change based on local land use, river impoundments, the amount of impervious surfaces, and long-term climate patterns [@TechniquesMethods2019]. Considering climate change, increaseed urbanization, and population growth in floodprone regions, experts in the field have established that absolute flood protection is not realistic anymore, rather flood risk management (FRM) is a more accurate way to refer to floodplain management [@schanzeFloodRiskManagement2006].

Risk has a variety of definitions based on the disciplinary domain in which the concept is being examined. In an everyday sense, risk can be considered the chance of a negative outcome occurring [@mishraDecisionMakingRiskIntegrating2014]. We see risk as inherently transdisciplinary and needs to encapsulate the full context of the topic it is being applied to. Therefore we define risk, in a flood context, as the quantifiable consequence of a flood event given the known, local social, environmental, and political factors. Communities understand their flood risk typically by using Federal Emergency Management Agency (FEMA) floodplain maps that estimate the extent of flood hazards through hydrologic and hydraulic models. These analyses require topography, rainfall and run-off frequency distributions, and flood control structures (e.g. diversion dams, levees, bridges). In addition, these floodplain maps are essential for communicating flood risk to vulnerable populations, helping communities mitigate and adapt to floods, and the functioning of insurance programs, such as the the FEMA's National Flood Insurance Program [@pralleDrawingLinesFEMA2019]. However, recent reports estimate that approximately 25% of the flood damage claims occur outside of FEMA mapped floodplains each year because these maps can be outdated and inaccurate [@ludyFloodRiskPerception2012].

In order to accurately predict flood risk, many communities are adopting new technology and management practices. One of the advances in FRM is the availability of higher-resolution terrain mapping and feature data. Previous research confirms that high-resolution topographic data is critical for an accurate floodplain map [@aliAssessingImpactDifferent2015; @cookEffectTopographicData2009].  In the past, flood risk managers typically used 10-meter or 30-meter resolution Digital Elevation Models (DEM) from sources such as the National Elevation Dataset (NED). One technology that has grown in popularity is Light Detection and Ranging (lidar), which is a laser-based remote sensing technology that uses the reflection of light to measure elevation and features on the ground such as vegetation and structures. Raw lidar data points form a three-dimenionsial (3D) point cloud. These 3D point clouds can then be used in a wide-array of hazard applications such as wildfire fuel load calculations or with identifying wildlife habitat viewsheds. In addition, raw lidar data can be used to create 1-meter or finer resolution DEMs [@muhadiUseLiDARDerivedFlood2020]. *should I insert example how this is used in FRM?*

Several government agencies initiated lidar acquisition projects in an effort to increase the availability of publically-accessible lidar. Foremost, the United States Geological Survey (USGS) established the 3D Elevation Program (3DEP) in 2010 as the first nationally-coordinated lidar acquisition program with a goal of flying the complete U.S. by 2023 with lidar data.  This would be the first ever national baseline of consistent, high-resolution topographic elevation data, including bare earth and 3D point clouds. In addition, FEMA established the Risk Mapping and Planning (RiskMAP) program as part of the Biggert-Waters Flood Insurance Reform Act of 2012. This act charged FEMA with reforming the flood insurance process, while also improving the accuracy and reliability of it's floodplain maps (USGS, 2017). As a result, both 3DEP and RiskMAP programs are used to fund lidar acquisition projects across the U.S. In addition, several other U.S. agencies including the National Oceanic and Atmospheric Administration (NOAA), the U.S. Department of Agriculture (USDA), the U.S. Army Corps of Engineers (USACE), and U.S. Forest Service (USFS) also participate in lidar acquisition. Figure XX displays the footprint of available topographic and bathymetric lidar across the contiguous, lower 48 states. From this image, there is a clear decrease in the availability of publically-accessible lidar in the western U.S. including Washington, Idaho, Montana, Oregon, Nevada, Utah, California, Arizona, and New Mexico. As lidar becomes more available and increasingly popular, it is important to understand the factors that influence a flood risk manager's decision to adopt this new technology into their practice of long-term risk mitigation.

!["The U.S. Interagency Elevation Inventory displays all publically-available lidar data and lidar-derived products for the continguous, lower 48 states. (source:https://coast.noaa.gov/inventory/)"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/lidar_map.PNG)

This study examines the landscape of factors that catalyze decision-makers to use lidar for flood risk management. Past research emphasizes risk perception as a key driver of risk mitigation behavior [@birkholzRethinkingRelationshipFlood2014;@mishraDecisionMakingRiskIntegrating2014;@slovicPerceptionRisk1987]. Risk perception is a fundamental way to characterize how individualâ€™s intuitive risk judgment and allows for the identification, characterization, and quantification of risk. However, previous research on risk perception has been opposing. Past findings suggests individuals who perceive that natural hazards pose a greater risk also behave more cautiously [@vinhhungFloodRiskManagement2007]. Interestingly, studies also found the opposite in that individuals who perceive a greater risk engage in fewer mitigating behaviors [@bubeckReviewRiskPerceptions2012; @wachingerRiskPerceptionParadoxImplications2013]. This paradoxical behavior suggests that risk perception is more nuanced and moderated by individual and socio-cultural factors [@baerenklauUnderstandingTechnologyAdoption2005; @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012; @kahanCultureIdentityProtectiveCognition2007; @vanvalkengoedMetaanalysesFactorsMotivating2019].

While the field of hazards and disaster research is inherently multidiscplinary, there has been a recent, concerted effort for convergence research to integrate knowledge across disciplines and organizational boundaries to reduce disaster losses and promote collective well-being (Peek at al., 2020). In addition, previous flood risk research identified two main theoretical limitations of our existing intellectual understanding of risk-mitigating behavior. Firstly, there is limited predictive power of the theories applied so far (e.g. protection motivation theory) [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action (e.g. social beliefs) [@kuhlickeBehavioralTurnFlood2020].
In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating behavior; there needs to be a critical look at the underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. We employ cultural evolutionary theory to study lidar adoption because it offers a convergent research lens to compare individual and collective action and has the potential for prediction.  

In the next section we review cultural evolutionary theory in detail and explain how this theory is beneficial for understanding the underlying mechanisms that shape flood risk management behavior. Next, we apply this theoretical framework to our case study of lidar adoption for flood risk management.  This is followed by the methods section that explains our survey instrument development process and statistical approach for analyzing the survey data. The results from our analysis and a discussion about significant trends will follow. Finally, we discuss the implications of these results and need for further research. 

# 2 Individual and collective predictors of risk-mitigation behavior

It is important to consider the combined effects of both individual and collective predictors in predicting risk-mitigation behavior so that we can understand the relative contribution of each predictor [@vanvalkengoedMetaanalysesFactorsMotivating2019]. This study incorporates a relevant collection of individual and collective predictors that could potentially influence a flood risk manager to adopt lidar for flood risk mitigation. The following section examines the previous work completed in individual predictors of risk mitigaiton behavior and then explores how cultural evolutionary theory can help illuminate collective predictors of influence. 

## *2.1 Re-examining risk perception*

Previous FRM research focused on flood risk perception as a critical factor of developing effective FRM strategies [@birkholzRethinkingRelationshipFlood2014]. However, recent research re-examined risk perception's role as a significant factor of FRM. For example, a study by Bubeck et al. (2012) found risk perception as a weak predictor for precautionary behavior and suggests shifting focus towards flood-coping appraisal for explaining FRM behavior. In addition, Kellens et al. (2013) reviewed 57 empirically based peer-reviewed articles on flood risk perception and communication to assess overall trends in flood risk research. The authors found that majority of studies were exploratory and did not apply a theoretical framework to examine risk perception [@kellensPerceptionCommunicationFlood2013a]. Of the studies that employed a theoretical framework, protection motivation theory (PMT) was the most common. PMT explains individual decisions about preparing for risk as a function of threat appraisal (e.g. likelihood of exposure to a flood, severity of exposure, and fear) and coping appraisal (e.g. self-efficacy, outcome efficacy, and outcome costs). The results of this review suggest future research that emphasizes a theoretical framework that captures physical exposure and hazard experience to assess risk perception. Similarily, Kuhlicke et al. (2020), summarizes additional individual behavior theories that have been used in FRM research prior, including person-relative-to-event theory, theory of planned behavior, and protection action decision model. Kuhlicke et al. highlights the use of the social identity model of collective action and social identity model of pro-environmental action as theories that explore the role of collective behavior on group adaptive behavior-related strategies [@kuhlickeBehavioralTurnFlood2020]. 

There are several limitations with the application of these theories. Firstly, there is limited predictive power of the theories applied so far [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action in flood risk management research and therefore it is suggested that future research apply collective factors more rigorously [@kuhlickeBehavioralTurnFlood2020]. In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating behavior; this work calls for a more critical perspective on underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. Given these findings about previous shortcomings of FRM research, we employ cultural evolutionary theory to understand how collective action moderates risk perception and risk mitigation behavior.   

## *2.2 Culture and Risk*
Cultural evolutionary theory is the transmission and selection of ideas, behaviors, and artifacts over time. Reminiscent of genetic evolution, human culture is adaptive and aims to increase fitness or utility [@creanzaCulturalEvolutionaryTheory2017; @henrichEvolutionCulturalEvolution2003]. Unlike genetic transmission, it is important to note cultural transmission can occur over a short time scale, within a generation through social learning [@richersonCulturalGroupSelection2016]. Social learning is an individualâ€™s ability to transmit information to another person. Culture forms as a result of this process, creating a shared set of beliefs and norms among a group of individuals. Culture then evolves through the process of Natural Selection, creating between-group variation of adaptive behavior and cooperation [@richersonCulturalGroupSelection2016]. Consequently, some groups evolve to have more successful risk mitigation behavior than others. Humans are heavily reliant on social learning to acquire and manage their behavioral repertoire (Henrich and McElreath, 2008). CT and social learning are increasingly popular in explaining natural resource management because of the potential social learning has for increasing adaptive capacity [@reedWhatSocialLearning2010]. There are several definitions that could be used for social learning, however this study uses the concept of social learning as "change in understanding that goes beyond the individual to become situated within wider social units or communities of practice through social interactions between actors within social networks" [@reedWhatSocialLearning2010].

In a similar vein, the cultural theory of risk (CTR) specifies how social learning influences the way people understand specific risks [@douglasRiskCultureEssay1983]. Previous FRM research has suggested the use of CTR to contextualize the relationship of risk perception as a function of cultural adherence and social learning [@birkholzRethinkingRelationshipFlood2014]. CTR has been employed in a couple empirical FRM studies so far and provides an intriguing underpinning of risk perception [@shenFloodRiskPerception2009], however we chose not to utilize this theory in our study because of its pointed focus on perception. CT provides a broader perspective of the impact social learning has beyond risk perception. More specifically, CT can be used to explain network proxies of peer influence...(?)

## *Predictor selection*
In order to select relevant individual and collective predictors of flood risk mitigating behavior *a priori*, we conducted a literature review of previous work that looked at the effect of constructs on flood risk mitigating behavior. The literature is pre-dominantly focused on the public, rather than flood risk managers themselves. There has been some previous work focused on emergency manager decision-making, however this area is understudied [@robertsDecisionBiasesHeuristics2019; @brodyExaminingClimateChange2010]. 

!["Table XX. Individual and collective constructs, with the motivation, effect, and supporting literature for each construct, that potentially influence risk mitigating behavior for flood risk management"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_lit_review.jpg)
*section about each predictor?*
Could say something like of these thirteen constructs, we selected eight to examine in further detail for FRM. These are the reasons why...instead of leaving the motivation in the chart?

## *2.3 Case study description*
Technology adoption in flood risk management provides an interesting case study to examine the effect of individual and collective predictors on risk mitigating behavior. This study examines the adoption of lidar in communities throughout Idaho, Oregon, and Washington which are expected to see an increase in precipitation and higher temperatures earlier in the year [@clarkChangesPatternsStreamflow2010; @ralphVisionFutureObservations2014; @idahoofficeofemergencymanagementStateIdahoHazard2018; @washingtonemergencymanagementdivisionWashingtonStateEnhanced2020; @slaterRecentTrendsFlood2016]. In addition, the Pacific Northwest is experiencing increasing urbanization rates and population growth, therefore changing flood risk in communities. 

While Idaho, Oregon, and Washington all reside in the same geographic region, each state has unique sets of challenges when it comes to FRM due to differential spatial landscape, population growth and urbanization, and resource availability (e.g. funding for FRM, educational opportunities for flood risk managers). 

### **Idaho**
In 2019, Idaho was home to 1.79 million people across 82,643 square miles; 21.7 people per square mile [@CensusBureauQuickFactsb]. It is a land-locked state and can be broken down into three main areas: the panhandle in the north is filled with coniferous forests and lakes, the central section is filled with vast mountain ranges and alpine lakes, and the southern section, known as the Snake River Plain, is filled with sagebrush steppe and high desert environment. There is influence from the Pacific Ocean in the north and west side of Idaho, resulting in cloudy, humid, and wet winters, whereas the east is the opposite with wet summers and dry winters. Average annual rainfall ranges from 10" in the arid southwest regions to 50" at higher elevations in certain river basins [@d.o.oIdahoUSAClimate]. In addition, Idaho sees abundant amounts of snowfall in the mountains. 

While most of the population is concentrated in the southern part of the state, there is flooding across the entire state that impacts people and structures. Idaho is prone to riverine flooding, ice/debris jam flooding, levee/dam/canal breaks, stormwater, sheet or areal flooding, and mudflows [@idahoofficeofemergencymanagementStateIdahoHazard2018]. In 2021, Idaho had 145 NFIP participating communities across 44 counties [@CommunityStatusBook]. 

Lidar acquisition is coordinated by Boise State University's Idaho Lidar Consortium in conjunction with Idaho State University's GIS Research and Training Center, which stores lidar data for public use. There is no state-approved funding for lidar acquisition and therefore, communities rely on using local funding in addition to applying for funding from 3DEP and/or FEMA. By the end of 2020, Idaho had 25% of the state covered with publically-available lidar. 

### **Oregon**
In 2019, Oregon had over 4.2 million residents across 95,988 square miles; 43.8 people per square mile [@CensusBureauQuickFactsa]. Oregon can be broken down into six main areas: the Coast Range, the Willamette Lowland, the Cascade Mountains, the Klamath Mountains, the Columbia Plateau, and the Basin and Range Region. There is a maritime influence across the entire state due to the Pacific Ocean. The Coast range is predominantly evergreen forests with many small coastal lakes. The mountain regions are typically several thousand feet about sea-level and have a range of dense forests and lakes. Eastern Oregon contains high desert environment with few steep mountains. 

Oregon's population is concentrated in the coastal region of the state. Oregon has an extensive history of multiple types of flooding including riverine flooding, flash floods, ice/debris jam flooding, coastal flooding, shallow area flooding, urban flooding, and playa flooding [@laytonStateInteragencyHazard2015]. In 2021, Oregon had 228 NFIP participating communities across 36 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the State of Oregon Department of Geology and Minderal Industries' Oregon Lidar Consortium. Bu the end of 2020, Oregon had 98% of Oregon's populated areas were covered with publically-available lidar [@DOGAMILidarOregon]... [change state for lidar coverage in state? would be a lot less, maybe email dogami-info@oregon.gov] 

### **Washington**
In 2019, Washington had over 7.6 million residents across 66,455 square miles; 114 people per square mile [@CensusBureauQuickFacts]. Washington can be broken down into six main areas: the Olympic Mountains, Coast Range, Puget Sound Lowlands, Cascade Mountains, Columbia Plateau, and Rocky Mountains. Most of the areas in the western and northern parts of Washington are predominately evergreen forests, where the eastern and southern parts of Washington are semiarid where grasses, sagebrush, and scattered shrubs can be found. Annual precipitation on the Pacific side of the Olympic Peninsula exceeds 150 inches, but places on the northwest of the peninsula receive less than 20 inches a year and on the eastern side receive less than 8 inches [@WashingtonStateCapital].

More than three-fourths of the population lives in Puget Sound Lowlands [@WashingtonStateCapital] Flooding in Washington typically occurs on a seasonal basis due to rainfall from atmospheric rivers, rainfall on snow, flash foods from storms, and winter storms causing storm surges and high tide [@washingtonemergencymanagementdivisionWashingtonStateEnhanced2020]. It is estimated that in 2021, Washington had 277 NFIP participating communities across 39 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the Washington State Department of Natural Resources and receives funding from the Washington State Legislature to acquire and upkeep lidar data for the state.  Over 50% of the state has been flown with lidar data [@GerWaLidar]. *get an updated number from Abby*

### **Physical flood risk**
Since flooding is becoming an increasingly damaging and costly issue, there has been a rise in interest from non-governmental groups to predict flood risk at the property level for households and property owners to be aware of their true flood risk. First Street Foundation, a non-profit organization of modelers, researchers, and data scientists to create the first publicly-available flood risk model for the lower 48 states. According to First Street, there is nearly 70% properties with substantial flood risk than previously predicted by FEMA floodplain maps [@firststreet.orgMissionFirstStreet]. This study is focused on Idaho, Oregon, and Washington so in an effort to understand the nature of physical flood risk in each of these states, we have compared the FEMA projections to the First Street projections as seen \autoref{tab:tab_statecomp}. **should I turn this into a map?** . It is important to note that FEMA report's Idaho with the least amount of risk compared to Oregon and Washington, however First Street reports it as having the most. This difference could be because there are still many locations in Idaho that are not mapped by FEMA abd therefore building in floodplain areas could be more likely. 

```{r comparison table, echo=FALSE, fig.cap="\\label{tab_statecomp}"}
id.floodrisk <- read.csv("data/idaho_floodrisk_firststreet.csv")
id.fema.2020.total <- round(sum(id.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
id.fema.2020.pct <- round(mean(id.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
id.fs.2020.total <- round(sum(id.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
id.fs.2020.pct <- round(mean(id.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

or.floodrisk <- read.csv("data/oregon_floodrisk_firststreet.csv")
or.fema.2020.total <- round(sum(or.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
or.fema.2020.pct <- round(mean(or.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
or.fs.2020.total <- round(sum(or.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
or.fs.2020.pct <- round(mean(or.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

wa.floodrisk <- read.csv("data/washington_floodrisk_firststreet.csv")
wa.fema.2020.total <- round(sum(wa.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
wa.fema.2020.pct <- round(mean(wa.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
wa.fs.2020.total <- round(sum(wa.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
wa.fs.2020.pct <- round(mean(wa.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

state.comp <- matrix(c(id.fema.2020.total, or.fema.2020.total, wa.fema.2020.total, id.fema.2020.pct, or.fema.2020.pct, wa.fema.2020.pct, id.fs.2020.total, or.fs.2020.total, wa.fs.2020.total, id.fs.2020.pct, or.fs.2020.pct, wa.fs.2020.pct), ncol=3, byrow=TRUE)
colnames(state.comp) <- c("Idaho", "Oregon", "Washington")
rownames(state.comp) <- c("Total FEMA Properties at Risk (2020) ", "Percent FEMA Properties at Risk (2020)", "Total FS Properties at Risk (2020) ", "Percent FS Properties at Risk (2020)")
knitr::kable(state.comp, caption="Summary information about environmental and social differences between Idaho and Washington.")
```

## *2.4 Relevant predictors of lidar adoption*
**this section might be better under the methods section?**
Given the previous literature, as well as the nature of our case study we narrowed down our study to focus on eigth constructs, with a total of twelve predictors.   

!["Table XX. Predictors of lidar adoption"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_lidar.jpg)

# 3 METHODS
```{r, survey data upload, include=FALSE}

idaho <- read.csv("data/id.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Idaho") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(idaho)

oregon <- read.csv("data/or.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses  
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Oregon") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(oregon)

washington <- read.csv("data/wa.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Washington") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(washington)

alaska <- read.csv("data/ak.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Alaska") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(alaska)

# Combine all four states into one dataset # 
survey.responses <- rbind(idaho, oregon, washington) %>% 
  subset(.,incr_no_flood!="") # remove additional blank responses
  

#write.csv(survey.responses, "C:/Users/tarapozzi/Documents/Manuscript/survey.responses.csv")

```

### *3.1 Survey design*
We conducted eight, semi-structured interviews with stakeholders (e.g. flood risk managers, state officials, academics) to identify common themes and the extent of relevant predictors to measure. These interviews were conducted prior to the finalization of the survey instrument and directly informed our approach to ensure our work could identify opportunities and barriers to lidar adoption. In addition, we pre-tested our survey instrument with three industry professionals (two women and one man) to provide feedback on question clarity and survey flow. The second pre-test was with six (?) university students and staff to give additional feedback on the survey. We modified the survey based on this feedback. The final draft of the survey was then approved by the Institutional Review Board (IRB) at Boise State University. 

The finalized survey consisted of four main parts (see Appendix A). The first part focused on gathering information about the respondent's experience and beliefs about their FRM community. The first section consisted of a screening question to assess if the respondent was qualified by asking if they were a flood risk manager. The second section gathered background information about the number of years in the profession and the third section asked the respondent about their local floodplain. This section ask the respondent if their community participated in the NFIP, as well as information about flooding damage history and perceived future flood damage in the next 30 years. The fourth section examined the current mapping data in the respondent's FRM community. This included questions about floodplain map accuracy, flooding outside the designated FEMA flood zone, and level of preparation for a flood event. The next section asked the respondent to report their personal beliefs about the change in the number and/or severity of flood events in their FRM community in the future. 

The next part of the survey centered around the respondent's relationship with lidar for FRM. The first section in this part focused on the respondent's relationship with lidar for FRM. Lidar use, in this context, referred to work with either raw lidar data (e.g. LAS or point clouds) or lidar-derived data (e.g. DEM, DSM). If the respondent replied yes, then they were asked questions about where they received it from and how they apply it to FRM, as well as continued educational opportunities. If they selected no, they were asked about barriers they faced to using lidar and if they would like to partake in education opportunities. 

The third part of the survey gathered information about the respondent's network. It focused on the respondent's FRM network connections. For each reported alter, the respondent answered if the alter used lidar, how much they communicated with them, and their expertise in FRM. 

The final part of the survey asked the respondent about their personal beliefs in risk-taking and trust. The first section in this part asked the respondent to report their general risk-taking attitude. The second section asked about the respondent's gender, age, and education level, as well as their personal beliefs regarding trust in the federal government, science, and involvement level of the federal government with FRM. 

### *3.2 Data collection*
The survey's target respondent was the floodplain manager or administrator from participating and non-participating NFIP communities in Idaho, Oregon, Washington, and Alaska. This also included individuals that may use lidar for FRM applications in conjunction with software applications such as Geographic Information System (GIS). The majority of sample respondents were municipal, state, and federal employees, as well as some private industry employees. In order to achieve our target population, the sample frame included several sources of contacts including NFIP coordinators, Association of State Floodplain Managers (ASFPM) recognized Certified Floodplain Mangers (CFM), county-level GIS administrators, the five largest cities and tribal GIS administrators if present, county and tribal emergency managers, the Federal Geospatial Data Coordination Contacts by State, and additional, relevant contacts for the 2019 Northwest Regional Floodplain Managers Association (NORFMA) Conference contact list. 

```{r, survey distribution, include=FALSE}
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
  filter(grepl("2", screen))

#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

a <- length(wa.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-13: 383 potential responses

# oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
  filter(grepl("2", screen))

#load distribution history
or.dist <- read.csv("data/or_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

b <- length(or.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 357-13: 344 potential responses


# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
c <- length(id.pot.responses$Email)
# 463 potential responses


#load distribution history
alaska.results <- read.csv("data/ak.csv")
ak.no <- alaska.results %>%
  filter(grepl("2", screen)) # 2 said no to screening

ak.dist <- read.csv("data/ak_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
d <- length(ak.dist$Email) - 2

total.sample.frame <-  a + b + c + d


```

``` {r, response rate, include=FALSE}

id.rr <- round((96/a)*100, digits=1) # a is the total number of potential responses

or.rr <- round((58/b)*100, digits=1)

wa.rr <- round((54/c)*100, digits=1)

ak.rr <- round((6/d)*100, digits=1)


```

Survey data was collected through a structured, online survey distributed through Qualtrics to email addresses in our sample frame. The survey was sent to Idaho, Oregon, Washington and Alaska between May and July 2020. The survey took an average of 10 to 15 minutes to complete. There were four to six email correspondence messages with potential survey participants over the course of four weeks to help increased our response rate. Table $\label{survey data collection}$ summarizes the potential respondents, number of usable survey responses, and response rate for each state. 

```{r, survey data collection table, echo=FALSE, fig.cap="\\label{survey data collection}"}
survey.data <- matrix(c(a, 96, id.rr, b, 58, or.rr, c, 54, wa.rr, d, 6, ak.rr), ncol=3, byrow=TRUE)
colnames(survey.data) <- c("Potential Respondents", "Number of Usable Responses", "Response Rate")
rownames(survey.data) <- c("Idaho", "Oregon", "Washington", "Alaska")
knitr::kable(survey.data, caption="Comparative survey distribution and collection.")
```


We did not include Alaska in our final statistical analysis because of insufficient number of responses. In addition, both Oregon and Washington had lower response rates than Idaho. It is common to see 10-25% response rate from detailed online surveys, which our response rates are within the typical bounds [@sauermannIncreasingWebSurvey2013]. We suspect the variation in response rates between states could be due to competing time availability during the COVID-19 pandemic which required extra time and energy from emergency managers, which was our target population for this survey. For this reason, we expected a lower survey response than typical (unnecessary to say?). Given this, there may be some implications of the representativeness of sample. 

### *3.3 Estimation Analysis*

A Hierarchical Bayesian approach with a Generalized Logistic Regression meets the model criteria for understanding risk because it is able to characterize non-linear, unpredictable outcomes. Furthermore, a logistic regression was used because the response variable, lidar use, is binary. Therefore, this analysis did not need account for ordered categorical response. This analysis uses a Markov Chain Monte Carlo algorithm to predict posterior distributions of each parameter's effect on lidar use. 

This model has two-levels of analysis. The first level of actors is represented by individual respondents and the second level is the cluster represented by each state. Furthermore, this model follows a binomial distribution curve, where the distribution of lidar use, y_{ij}, can be modeled as follows (https://idiom.ucsd.edu/~rlevy/pmsl_textbook/chapters/pmsl_8.pdf): 

\[b_i \approx N(0,\sigma_b)\]

\[\eta_i = \mu_\alpha + \beta x_{ij}+...+\beta_kx_{ij} + b_i\]

\[\pi_i = \frac{e^{\eta}_i}{1+e^{\eta}_i}\]

\[y_{ij} \approx Binom(1, \pi_i)\] (1)

where $x_{ij}$, predictors, are the ith rows of the known design matrices x, and $\beta$ is a vector of regression parameters. The Bayesian approach allows for adjustment of uncertainty associated with each parameter on the final outcome (lidar use). In order to do this, each parameter has to be assigned a prior belief of that parameter value. The values for these parameters are fit by sampling from these distributions to maximize the likelihood under this model[@kwonClimateInformedFlood2008]. The regression parameters, $\beta$, are normally distributed, \[\beta_k\approx N(\eta_{\beta k}, \sigma_k)\]. Additionally, the parameters of this distribution, $\eta_{\beta k}$ and $\sigma_k$, also have prior distributions assigned to them that are constrained by 0 and a positive value.

#### *Priors*

The primary model included all predictors of interest with a varying intercept due to location. Subsequent models were run, isolating each predictor and lidar use with vary intercept and slope. In addition, we ran a model comparison and assessed the overall performance through Leave-One-Out Cross-Validation (LOOCV). This process provides an absolute metric for the model's predictive ability. Lastly, because this model had categorical predictors, we plotted the predicted probability against the observed proportion for some binning of the data (Levy, 2012) [this text has a great example of how to plot this]. In addition, weakly informative prior distribution provides modest regularization and can reduce type I errors and improves out-of-sample prediction for regression models (McElreath 2015). Gelman et al. (2008) suggests the use of a Cauchy distribution with center 0 and scale 2.5 for nonhierarchical logistic regression models [@gelmanWeaklyInformativeDefault2008]. This study uses a cauchy distribution as recommended for models with random-intercept and low sample sizes [@lemoineMovingNoninformativePriors2019]. 

#### *Validation*

In order to assess the out-of-sample predictive ability of our  model, we ran a k-fold leave one-out-cross-validation (LOOCV). We chose this method because... add reasoning.

#### *Error*

We specified our model to compute 2,000 lidar use predictions based on our predictors. We interpreted the mean of these results as the projected lidar use. We used Bayesian R-squared to measure our overall model accuracy. However, this can be unreliable for small sample sizes
so we also calcuated the mean absolute error (MAE) of our model. 

#### *Exploratory Analysis*

The results of this model allowed us to explore the effect, considering a multitude of individual and collective predictors, on lidar use in Idaho, Oregon, and Washington. We hypothesized that the model would be helpful for understanding the general level of effect, however we expected the predictive capacity of our model to be limited considering the large number of predictors and small sample size of our study. 

# 4 RESULTS + DISCUSSION  (~2,000)

Table xx below compares the demographic and experience representation of survey responses from flood risk managers by region. We received the greatest number of responses from Idaho. This is potentially because of the survey's affiliation with Boise State University and therefore flood risk managers in Idaho took greater interest in this study. Washington the highest percentage of female respondents, second highest percentage of respondents with a bachelor's degree or higher, and longest average length of flood risk manager experience. The results show no significant distortions of representativeness found for age, gender, geographical area, or level of education. 


``` {r, survey demographics, include=FALSE}
id.sample.size <- count(idaho$location=="Idaho")
or.sample.size <- count(oregon$location=="Oregon")
wa.sample.size <- count(washington$location=="Washington")

count(idaho$age)
count(oregon$age)
count(washington$age)
# age breakdown
id.age.2 <- round((2/96)*100, digits=0) # the denominator is the sample size
id.age.3 <- round((17/96)*100, digits=0) # the numerator is from 
id.age.4 <- round((27/96)*100, digits=0)
id.age.5 <- round((48/96)*100, digits=0)

or.age.2 <- round((2/58)*100, digits=0)
or.age.3 <- round((9/58)*100, digits=0)
or.age.4 <- round((17/58)*100, digits=0)
or.age.5 <- round((25/58)*100, digits=0) # some people didn't respond

wa.age.2 <- round((3/54)*100, digits=0)
wa.age.3 <- round((10/54)*100, digits=0)
wa.age.4 <- round((17/54)*100, digits=0)
wa.age.5 <- round((23/54)*100, digits=0)

# gender breakdown
id.gender.male <- round((59/96)*100, digits=0)
id.gender.female <- round((37/96)*100, digits=0)

or.gender.male <- round((35/58)*100, digits=0)
or.gender.female <- round((20/58)*100, digits=0)

wa.gender.male <- round((30/54)*100, digits=0)
wa.gender.female <- round((24/54)*100, digits=0)

#education breakdown
id.edu.ba.ma <- round((37+29)/96*100, digits=0) # represents the number of respondents with either a bachelors or advanced degree
or.edu.ba.ma <- round((21+26)/58*100, digits=0) 
wa.edu.ba.ma <- round((17+24)/54*100, digits=0)

#years in the industry
idaho$years <- as.numeric(idaho$years)
id.years <- round(summarise(idaho, avg=mean(years)), digits=1)

oregon$years <- as.numeric(oregon$years)
or.years <- round(summarise(oregon, avg=mean(years, na.rm=TRUE)), digits=1) ### THIS ISNT WORKING

washington$years <- as.numeric(washington$years)
wa.years <- round(summarise(washington, avg=mean(years)), digits=1)
```

```{r, survey demographics table, echo=FALSE, fig.cap="\\label{demographics}"}
survey.dem <- matrix(c(96, 58, 54, "39%", "34%", "44%", "69%","81%", "76%",id.years, or.years, wa.years), ncol=3, byrow=TRUE)
colnames(survey.dem) <- c("Idaho", "Oregon", "Washington")
rownames(survey.dem) <- c("Sample Size", "Female", "University Education", "Average Flood Risk Experience (years)")
knitr::kable(survey.dem, caption="Comparative descriptive statistics for survey demographics across Idaho, Oregon, and Washington.")
```

```{r, data sim, include=FALSE}
## STEP 1: Data Simulation ##
set.seed(124) 
N=180 # number of survey respondents # played around with n=720 and that made results significatly better
K=8 # number of predictors

## 1) Set the intercept ##

intercept=0 ## mean value of lidar use when all predictors are equal to 0

# for random effects, we could look at location

location_mean <- rep(rnorm(c(0,0.1,0.2), sd=0.1), times=60) # 3 states drawn 60 times (bsaed on a smaple size of 50)

intercept_v <- location_mean

## 2) Set the predictor variables ##
## set simulate experience with binomial for each type of experience
experience_1 <- rbinom(N, 1, .5) # damage to property in community # yes (1) no (0)
future_1 <- sample(1:5, N, replace=TRUE) # damage to property in community in the future 1=0% to 5=100%
science_trust <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
incr_sev_flood<- sample(1:3, N, replace=TRUE) # no (1) stay the same (2) yes (3)
soep <-  sample(0:10, N, replace=TRUE) # range of risk preference from 0 to 10, where 0 = I generally prefer to take risks to 10 = I generally prefer to avoid risks
alter_lidar_prop <- runif(N, 0, 1) # this simulates the a range of potential proportion of lidar users in alters
alter_exp_mean <- runif(N, 0, 10) # this will rep the mean expertise of alters 
alter_comm_mean <- runif(N, 1, 6) # reps the mean communication of alters with respondent

#location <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

# Make data frame with raw data
raw.data2 <- data.frame(location_mean, experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)

raw.data2 <- tibble::rowid_to_column(raw.data2, "ID")

## Now let's set the effect size ##
b_experience_1 <- .2
b_future_1 <- -.6
b_incr_sev_flood <- 0.6
b_science_trust <- 0.1
b_soep <-  0
b_alter_lidar_prop <- 1 
b_alter_comm_mean <- 0.1
b_alter_exp_mean <- 0.1


# 4) Set the response variable (aka deterministic part) ##
p2 <- intercept + experience_1*b_experience_1 + future_1*b_future_1 + incr_sev_flood*b_incr_sev_flood  + science_trust*b_science_trust + 
  soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_comm_mean*b_alter_comm_mean + alter_exp_mean*b_alter_exp_mean 

p_varying2 <- p <- intercept_v + experience_1*b_experience_1 + future_1*b_future_1 + incr_sev_flood*b_incr_sev_flood + science_trust*b_science_trust + 
  soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_comm_mean*b_alter_comm_mean + alter_exp_mean*b_alter_exp_mean 

pr2 <- plogis(p2) # convert from log odds to probability

pr_v <- plogis(p_varying2)

lidaruse2 <- rbinom(N,1,pr2)

lidaruse_v2 <- rbinom(N,1,pr_v)

## 5) Combine data into dataframe ##

sim.data2 <- data.frame(lidaruse2, lidaruse_v2, location_mean, experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)
write.csv(sim.data2, "sim_data2.csv")

#### Descriptive Stasim.data ####
# Check for correlations
cor(sim.data2) # nothing > |.5| so we are good!

# plot the raw data & check for outliers
boxplot(sim.data2[-24])

#### Check out the model & see if it returns out effects

model_v2 <- stan_glmer(lidaruse_v2 ~ incr_sev_flood + experience_1 + future_1 + science_trust + alter_lidar_prop + alter_comm_mean + alter_exp_mean +(1|location_mean), data=sim.data2, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 

model_2 <- stan_glm(lidaruse2 ~ incr_sev_flood + experience_1 + future_1 + science_trust + soep + alter_lidar_prop + alter_comm_mean + alter_exp_mean, data=sim.data2, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 


plot(model_2)
coef(model_2)


plot(model_v2)
coef(model_v2)

loo_compare(loo(model_2), loo(model_v2))
## by running several scenarios, it seems like I need to have a large sample size in order to recover the effect size I specified. And when I do a large sample size, the varying effects model returns slight  more accurate results
## with n=180, there the effect size is off by +/-.2, this seems good enough?
```

```{r, recode, include=FALSE}
survey.responses$lidaruse <- revalue(survey.responses$lidaruse, c("1"="1", "2"="0", "3"="0"))
idaho$lidaruse <- revalue(idaho$lidaruse, c("1"="1", "2"="0", "3"="0"))
oregon$lidaruse <- revalue(oregon$lidaruse, c("1"="1", "2"="0", "3"="0"))
washington$lidaruse <- revalue(washington$lidaruse, c("1"="1", "2"="0", "3"="0"))

# yes=1, no=0

# Direct Experiences
survey.responses$experience_1 <- revalue(survey.responses$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
survey.responses$experience_2 <- revalue(survey.responses$experience_2, c("1"="1", "2"="0"))
survey.responses$experience_3 <- revalue(survey.responses$experience_3, c("1"="1", "2"="0"))
survey.responses$experience_4 <- revalue(survey.responses$experience_4, c("1"="1", "2"="0"))
survey.responses$experience_5 <- revalue(survey.responses$experience_5, c("1"="1", "2"="0"))

idaho$experience_1 <- revalue(idaho$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
idaho$experience_2 <- revalue(idaho$experience_2, c("1"="1", "2"="0"))
idaho$experience_3 <- revalue(idaho$experience_3, c("1"="1", "2"="0"))
idaho$experience_4 <- revalue(idaho$experience_4, c("1"="1", "2"="0"))
idaho$experience_5 <- revalue(idaho$experience_5, c("1"="1", "2"="0"))

washington$experience_1 <- revalue(washington$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
washington$experience_2 <- revalue(washington$experience_2, c("1"="1", "2"="0"))
washington$experience_3 <- revalue(washington$experience_3, c("1"="1", "2"="0"))
washington$experience_4 <- revalue(washington$experience_4, c("1"="1", "2"="0"))
washington$experience_5 <- revalue(washington$experience_5, c("1"="1", "2"="0"))

oregon$experience_1 <- revalue(oregon$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
oregon$experience_2 <- revalue(oregon$experience_2, c("1"="1", "2"="0"))
oregon$experience_3 <- revalue(oregon$experience_3, c("1"="1", "2"="0"))
oregon$experience_4 <- revalue(oregon$experience_4, c("1"="1", "2"="0"))
oregon$experience_5 <- revalue(oregon$experience_5, c("1"="1", "2"="0"))


# closer the experience, higher the number for yes

# Trust-- reverse code
survey.responses$gov_trust <- revalue(survey.responses$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$gov_trust <- revalue(idaho$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$gov_trust <- revalue(oregon$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$gov_trust <- revalue(washington$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

survey.responses$science_trust <- revalue(survey.responses$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$science_trust <- revalue(idaho$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$science_trust <- revalue(oregon$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$science_trust <- revalue(washington$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

# Gov Involvement
survey.responses$gov_involve <- revalue(survey.responses$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
idaho$gov_involve <- revalue(idaho$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
oregon$gov_involve <- revalue(oregon$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
washington$gov_involve <- revalue(washington$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))

# 5=completely involved
# 4=mostly involved
# 3=moderately involved
# 2=somewhat involved
# 1=not at all involved

# Risk Perception
# future flood risk
survey.responses$future_1<- revalue(survey.responses$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_2<- revalue(survey.responses$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_3<- revalue(survey.responses$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_4<- revalue(survey.responses$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_5<- revalue(survey.responses$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

idaho$future_1<- revalue(idaho$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_2<- revalue(idaho$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_3<- revalue(idaho$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_4<- revalue(idaho$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_5<- revalue(idaho$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

oregon$future_1<- revalue(oregon$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_2<- revalue(oregon$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_3<- revalue(oregon$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_4<- revalue(oregon$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_5<- revalue(oregon$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

washington$future_1<- revalue(washington$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_2<- revalue(washington$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_3<- revalue(washington$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_4<- revalue(washington$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_5<- revalue(washington$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
# 100% chance of happening
# 75% chance of happening
# 50% chance of happening
# 25% chance of happening
# 0% chance of happening
# 1-5 to represent closeness

#increase number of floods
survey.responses$incr_no_flood <- revalue(survey.responses$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_no_flood <- revalue(idaho$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_no_flood <- revalue(oregon$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_no_flood <- revalue(washington$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3# decrease=1# stay the same=2

#increase severity of floods
survey.responses$incr_sev_flood <- revalue(survey.responses$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_sev_flood <- revalue(idaho$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_sev_flood <- revalue(oregon$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_sev_flood <- revalue(washington$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3
# decrease=1
# stay the same=2

# Demographics
# age is ordered correctly: 1= less than 20, 2=20-29 years, 3=30-39 years, 4=40-49 years, 5=50+ years
# education is ordered correctly: 1= some high school, 2= high school diploma, 3=college edu, no grad, 4= associates, 5=bachelors, 6= advanced
# gender is ordered fine: 1= male, 2=female

# Structural Barriers
# too expensive
survey.responses$barrier_1<- revalue(survey.responses$barrier_1, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

#lack of expertise
survey.responses$barrier_2<- revalue(survey.responses$barrier_2, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# sparse population
survey.responses$barrier_3<- revalue(survey.responses$barrier_3, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low rate of economic development
survey.responses$barrier_4<- revalue(survey.responses$barrier_4, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low flooding risk
survey.responses$barrier_5<- revalue(survey.responses$barrier_5, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# lack of political support
survey.responses$barrier_6<- revalue(survey.responses$barrier_6, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# add a column for barrier 7 which is if lidar is present or not

# SOEP
survey.responses$soep_1<- revalue(survey.responses$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
idaho$soep_1<- revalue(idaho$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
oregon$soep_1<- revalue(oregon$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
washington$soep_1<- revalue(washington$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))

# 10: risk loving
# 0: risk averse

#Gender all good, except "3" for other
survey.responses$gender <- revalue(survey.responses$gender, c("1"="1", "2"="2", "3"="NA"))
idaho$gender <- revalue(idaho$gender, c("1"="1", "2"="2", "3"="NA"))
oregon$gender <- revalue(oregon$gender, c("1"="1", "2"="2", "3"="NA"))
washington$gender <- revalue(washington$gender, c("1"="1", "2"="2", "3"="NA"))

#age doesn't need recoding

# Edcuation
survey.responses$education <- revalue(survey.responses$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
idaho$education <- revalue(idaho$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
oregon$education <- revalue(oregon$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
washington$education <- revalue(washington$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
# 1: some high school
# 6: advanced degree

# prepared 
survey.responses$prepared <- revalue(survey.responses$prepared, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all prepared
#5: completely prepared

#usefulness
survey.responses$usefulid <- revalue(survey.responses$usefulid, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all useful
#5: very useful

#flood zone
survey.responses$flood_zone <- revalue(survey.responses$flood_zone, c("1"="1", "2"="0"))
#1: flooding outside of flood zone
#2: no flooding outside


# NETWORK PROXIES
# amount of commmunication 
survey.responses$comm_1 <- revalue(survey.responses$comm_1, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_2 <- revalue(survey.responses$comm_2, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_3 <- revalue(survey.responses$comm_3, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_4 <- revalue(survey.responses$comm_4, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_5 <- revalue(survey.responses$comm_5, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_6 <- revalue(survey.responses$comm_6, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_7 <- revalue(survey.responses$comm_7, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_8 <- revalue(survey.responses$comm_8, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
#1: a few times a year
#2: once a month
#3: 2-3 times a month
#4: once a week
#5: several times a week
#6: several times a day

# alter lidar use
survey.responses$lidar_1 <- revalue(survey.responses$lidar_1, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_2 <- revalue(survey.responses$lidar_2, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_3 <- revalue(survey.responses$lidar_3, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_4 <- revalue(survey.responses$lidar_4, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_5 <- revalue(survey.responses$lidar_5, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_6 <- revalue(survey.responses$lidar_6, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_7 <- revalue(survey.responses$lidar_7, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_8 <- revalue(survey.responses$lidar_8, c("1"="1", "2"="2", "3"="3"))

#1: yes
#2: no
#3: i don't know
 
#expertise
# no need to revalue
#0: no expertise at all
#10: lots of expertise 
```
```{r, numeric conversion, include=FALSE}
survey.responses <- survey.responses %>%
  mutate_at(vars('years','lidaruse', 'interestlid','LocationLatitude', 'LocationLongitude', 'years', 'experience_1', 'experience_2', 'experience_3', 
                 'experience_4', 'experience_5', 'future_1', 'future_2', 'future_3', 'future_4', 'future_5', 
                 'currentmap', 'flood_zone', 'prepared', 'incr_no_flood', 'incr_sev_flood', 'barrier_1', 'barrier_2', 
                 'barrier_3', 'barrier_4', 'barrier_5', 'barrier_6', 'soep_1', 'age', 'gender', 
                 'gov_trust', 'gov_involve', 'education', 'science_trust', 'toolslid', 'toolslid1', 'usefulid', 'no_alters',
                 'alter_names_1_TEXT', 'alter_names_2_TEXT', 'alter_names_3_TEXT', 'alter_names_4_TEXT', 'alter_names_5_TEXT', 'alter_names_6_TEXT',
                 'alter_names_7_TEXT', 'alter_names_9_TEXT', 'comm_1', 'lidar_1', 'expertise_1_1', 'comm_2', 'lidar_2', 'expertise_2_1', 'comm_3', 'lidar_3', 'expertise_3_1',
                'comm_4', 'lidar_4', 'expertise_4_1','comm_5', 'lidar_5', 'expertise_5_1', 'comm_6', 'lidar_6', 'expertise_6_1', 'comm_7', 'lidar_7','expertise_7_1',
                'comm_8', 'lidar_8', 'expertise_8_1'), as.numeric) 
```

```{r, network proxies, include=FALSE}
### Network Proxies

# proportion of alters that use lidar
network.variables <- c("Email", "no_alters","comm_1","lidar_1","expertise_1_1","comm_2","lidar_2","expertise_2_1","comm_3","lidar_3","expertise_3_1",
                "comm_4","lidar_4","expertise_4_1","comm_5","lidar_5","expertise_5_1","comm_6","lidar_6","expertise_6_1","comm_7","lidar_7","expertise_7_1",
                "comm_8","lidar_8","expertise_8_1")

network.subset <- survey.responses[network.variables] %>%
                  drop_na(no_alters) 

lidar.variables <- c("Email", "lidar_1","lidar_2","lidar_3",
                     "lidar_4","lidar_5","lidar_6","lidar_7",
                     "lidar_8")

lidar.subset <- survey.responses[lidar.variables] 

lidar.subset$no_lidar_alters <-  rowSums(lidar.subset == 1, na.rm=TRUE) # this adds allthe lidar users in the network (rep by 1)

# merge number of alters that use lidar with original dataset

survey.responses <- join(survey.responses, lidar.subset[, c("Email","no_lidar_alters")], by= "Email", type="left")


# now calculate the proportion of lidar users for each respondent 

survey.responses$prop_lidar_network <- survey.responses$no_lidar_alters/survey.responses$no_alters # calculates the proportion of lidar users in the network


# let's look at average amount of communication respondent has with network 

comm.variables <- c("Email", "comm_1","comm_2","comm_3",
                    "comm_4","comm_5","comm_6","comm_7",
                    "comm_8")

comm.subset <- survey.responses[comm.variables] 

comm.subset$comm_total <-  rowSums(comm.subset[,2:9], na.rm=TRUE)
comm.subset$comm_ave <- round(rowMeans(comm.subset[,2:9], na.rm=TRUE), digits=0)


survey.responses <- join(survey.responses, comm.subset[, c("Email", "comm_total", "comm_ave")], by= "Email", type="left")


# now let's calculate total expertise in a responsdent's network
expertise.variables <- c("Email", "expertise_1_1","expertise_2_1","expertise_3_1",
                         "expertise_7_1","expertise_6_1","expertise_5_1","expertise_4_1",
                         "expertise_8_1")

expertise.subset <- survey.responses[expertise.variables]


expertise.subset$expert_total <-  rowSums(expertise.subset[,2:9], na.rm=TRUE) 
expertise.subset$expert_ave <- round(rowMeans(expertise.subset[,2:9], na.rm=TRUE), digits=0)

survey.responses <- join(survey.responses, expertise.subset[, c("Email", "expert_total", "expert_ave")], by= "Email", type="left")

```

```{r, model specific data, include=FALSE}
# Dataset for the model
model.variables <- c("lidaruse", "experience_1", "future_1", "incr_sev_flood",  "soep_1", "science_trust","location","prop_lidar_network", "comm_total", "expert_total")

model.data <- survey.responses[model.variables]

str(model.data)

```

```{r, na omit, include=FALSE}
model.data.na.omit <- na.omit(model.data)
n = nrow(model.data.na.omit)
n # 145
```

```{r, lidar use stats, include=FALSE}
count(idaho$lidaruse==1)
id.lidaruse <- round((48/96)*100, digits=1)

count(oregon$lidaruse==1)
or.lidaruse <- round((36/58)*100, digits=1)

count(washington$lidaruse==1)
wa.lidaruse <- round((35/54)*100, digits=1)

```

## *4.1 Descriptive Results and Discussion*

Table xx quantifies survey respondent's responses to individul and collective predictors of interest. Flood risk manager's reported the highest direct experience with flood damage in their community, compared to Oregon which reported the least, although the majority of flood risk managers in all three states have had direct experience with flood damage in their communities. Interestingly, the percent of flood risk manager's who perceive future flood damage in their communities significantly dropped with only 28% of flood risk manager's in Idaho perceiving the future risk. And despite this low belief in future damage, a higher number of reported a perceived increase in flood severity. This descrepancy could indicate that despite knowledge of increased flood risk, manager's feel as if their communities are adapting or have adapted to handle this increase in flood severity. We found that flood risk managers in Washington tend to be less risk-averse than managers in Oregon and Idaho. This is interesting and is hard to say exactly why... any thoughts? Overall, all three states reported a high trust in the accuracy of FRM scientific proeducts (e.g. topographic data, floodplain mapping, floodplain modeling) with Washington reporting the highest percentage of trust. 

The collective predictors of lidar use varied slightly among the states, however there were no significant differences among the states.


``` {r, survey variable decriptions, echo=FALSE, fig.cap="\\label{survey variables}"}

#Direct experience with flood damage in community
id.experience <- round((table(idaho$experience_1 == 1)/length(idaho$experience_1))*100, digits=1)
id.experience <- 79.2
or.experience <- round((table(oregon$experience_1 == 1)/length(oregon$experience_1))*100, digits=1)
or.experience <- 72.4
wa.experience <- round((table(washington$experience_1 == 1)/length(washington$experience_1))*100, digits=1) 
wa.experience <- 85.2
# 1=yes

#Over 50% chance of direct experience with future flood damage in the community
id.future <- round((table(idaho$future_1 > .25 )/length(idaho$future_1))*100, digits=1)
id.future <- 28.1
or.future <- round((table(oregon$future_1 > .25)/length(oregon$future_1))*100, digits=1)
or.future <- 39.7
wa.future <- round((table(washington$future_1 > .25)/length(washington$future_1))*100, digits=1) 
wa.future <- 48.1
# includes those who selected 50, 75, and 100% chance of event happening

#Perceived Increase in Flood Severity
id.incr.sev.flood <- (table(idaho$incr_sev_flood == 3)/length(idaho$incr_sev_flood))*100
id.incr.sev.flood <- 38.5
or.incr.sev.flood <- (table(oregon$incr_sev_flood == 3)/length(oregon$incr_sev_flood))*100
or.incr.sev.flood <- 41.4
wa.incr.sev.flood <- (table(washington$incr_sev_flood == 3)/length(washington$incr_sev_flood))*100
wa.incr.sev.flood <- 57.4
# 3=increasing

#Risk-taking attitude
#idaho$soep_1 <- as.numeric(idaho$soep_1)
#id.soep <- round(summarise(idaho, avg=mean(soep_1, na.rm=TRUE)), digits=1)
id.soep <- 2.8
#oregon$soep_1 <- as.numeric(oregon$soep_1)
#or.soep <- round(summarise(oregon, avg=mean(soep_1, na.rm=TRUE)), digits=1)
or.soep <- 3.3
#washington$soep_1 <- as.numeric(washington$soep_1)
#wa.soep <- round(summarise(washington, avg=mean(soep_1, na.rm=TRUE)), digits=1)
wa.soep <- 3.7
#0-risk averse to 10-risk loving

#Trust in accuracy of FRM scientific products
id.science.trust <- (table(idaho$science_trust > 3)/length(idaho$science_trust))*100
id.science.trust <- 82.3
or.science.trust <- (table(oregon$science_trust > 3)/length(oregon$science_trust))*100
or.science.trust <- 81.0
wa.science.trust <- (table(washington$science_trust > 3)/length(washington$science_trust))*100
wa.science.trust <- 90.7
#4-somewhat trust, 5-completely trust

#Proportion of lidar users in FRM network
idaho <- join(idaho, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#id.prop.lidar <- (round(summarise(idaho, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
id.prop.lidar <- 35.0
oregon <- join(oregon, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#or.prop.lidar <- (round(summarise(oregon, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
or.prop.lidar <- 40.0
washington <- join(washington, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#wa.prop.lidar <- (round(summarise(washington, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
wa.prop.lidar <- 42.0
#Total communication in FRM network
#id.comm.network <- (round(summarise(idaho, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
id.comm.network <- 3
#or.comm.network <- (round(summarise(oregon, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
or.comm.network <- 3
#wa.comm.network <- (round(summarise(washington, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
wa.comm.network <- 3

#Perceived expertise in FRM network
#id.expert.network <- (round(summarise(idaho, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
id.expert.network <- 6.6
#or.expert.network <- (round(summarise(oregon, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
or.expert.network <- 7.3
#wa.expert.network <- (round(summarise(washington, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
wa.expert.network <- 6.7

#Use of lidar for FRM
#id.lidaruse <- (table(idaho$lidaruse == 1)/length(idaho$lidaruse))*100
id.lidaruse <- 50.0
#or.lidaruse <- (table(oregon$lidaruse == 1)/length(oregon$lidaruse))*100
or.lidaruse <- 62.1
#wa.lidaruse <- (table(washington$lidaruse == 1)/length(washington$lidaruse))*100
wa.lidaruse <- 64.8

survey.variable.stats <- matrix(c(id.experience, or.experience, wa.experience, id.future, or.future, wa.future, id.incr.sev.flood, or.incr.sev.flood, wa.incr.sev.flood, id.soep, or.soep, wa.soep, id.science.trust, or.science.trust, wa.science.trust, id.prop.lidar, or.prop.lidar, wa.prop.lidar, id.comm.network, or.comm.network, wa.comm.network, id.expert.network, or.expert.network, wa.expert.network, id.lidaruse, or.lidaruse, wa.lidaruse), ncol=3, byrow=TRUE)
colnames(survey.variable.stats) <- c("Idaho", "Oregon", "Washington")
rownames(survey.variable.stats) <- c( "Direct experience with flood damage in community (%)", "Direct experience with future flood damage in the community (%)","Perceived Increase in Flood Severity (%)", "Average risk-taking attitude (0 to 10 with 10 being risk-loving)", "Trust in accuracy of FRM scientific products (%)", "Proportion of lidar users in FRM network (%)", "Average amount of communication with FRM network* ", "Perceived expertise in FRM network (0 to 10 with 10 being of highest expertise)", "Use lidar for FRM (%)" )
knitr::kable(survey.variable.stats, caption="Descriptive statistics of survey question responses by State.")
```

Washington reported the highest amount of lidar use in FRM with almost 65% of respondent's using lidar. This trend was expected because of the established lidar acquistion and coordinated lidar program in Washington. Established in the Department of Natural Resources, the Washington State General Fund provides funding for two permanent lidar positions, a lidar manager and a lidar specialist. These positions provide assistance for lidar acquisition and coordination in the state. In addition, Washington has focused on dissemating interactive (e.g. Washington Story Map) information on lidar to educate the public and advocate for continued lidar investment at the state-level. Oregon and Idaho also have established lidar acquisition and coordination efforts, however they do not have a permanently funded position to manage lidar. Granted, Oregon reported a similar use to Washington with 62% of respondents using lidar. However, only 50% of survey respondents in Idaho reported using lidar. This is significantly less than Washington and Oregon. In an effort to explore this variation at a more nuanced level, this next section explores the results of our Generalized Logistic Multilevel Model.

## *4.3 Estimation Results and Discussion* 

#### *Missing data values*
Survey data commonly suffers from item nonresponse that can result in missing data. Typical methods for handling missing data are reweight, imputation, and dropping responses with missing data. Interestingly, the network section in our survey was more suceptible to  nonresponse from respondents. Of the 208 usable responses we received, 50 of them did not fill out the network section. This could have been due to response fatigue since the network section came at the end of the survey. Another possible explanation is that this part was confusing or uncomfortable for respondents to fill out. In contract, the rest of the survey from which the individual predictors of lidar use were collected had less than 10% of responses with missing data. Since our model considers both individual and collective predictors and needs equal size data lengths for each predictor in order to run, we dropped almost 25% of our data responses. Since we used the drop technique for handling our missing data, it is likely that our results underestimate the effect size of our predictors and intercept [@langkampTechniquesHandlingMissing2010]. 

```{r, full model, include=FALSE, eval=FALSE}
#add number of alters?
full.mod <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location), data=model.data.na.omit, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 
summary(full.mod)                                                           
plot(full.mod)
plot(full.mod, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod)) # best r2 value
pp_check(full.mod) # this checks the posterior predictive ability. looks good


# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(0, 2.5),
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 

# informed priors model with cauchy distribution, this is based on the gelman et al. 2008 paper
full.mod.priors2 <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(location=c(0,0,0,0,0,0,0,0), 
                                          scale=c(3.7,6.4,1.1,8.4,3.3,7.0,.3,.1)),
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors2)
plot(full.mod.priors2)
plot(full.mod.priors2, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod.priors2))
pp_check(full.mod.priors2) 


# null model

null.mod <- stan_glm(lidaruse ~ 1, data=model.data.na.omit, family=binomial(link=logit))

## Model comparison

loo_compare(loo(full.mod), loo(full.mod.priors), loo(full.mod.priors2))
# Based on the LOOIC comparison, full mod with prior specification based on Gelman procedure has the best predictive power

```

```{r, final model, include=FALSE}
# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(0, 2.5),
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 

```

Through the use of the programming language R and program Rstudio, we used Bayesian theory to analyze the effect of individual and collective predictors of lidar adoption. We used four Monte Carol Markov Chains (MCMC) with 1,000 iterations for warmup and an additional 1,000 iterations for the model. Additionally, the convergence was checked by visually inspecting the MCMC trace plots of the model parameters. We assessed effective sample size and checked model convergence, indicated by R-hat statistics close to 1 and stable, well-mixed chains  [@gelmanBayesianWorkflow2020]. 

$\label{tab:summary_stats}$ displays the results from a varying intercept model that considers the effect of individual and collective predictors on lidar use, while using partial pooling for location. Partial pooling is the benefit of using a hierarchical model and allows the model to appropriately generalize across similar but not identical settings [@gelmanBayesianWorkflow2020]. This worked well in our study because we were looking at the regional variation of lidar adoption. In addition, this methodology allowed for propagation of uncertainty throughout the model, which is important because...? 

```{r, model summary table, echo=FALSE, fig.cap="\\label{summary_stats}"}
#these are based on summary(full.mod) which returned a slightly better result that fullmod.priors
summary.stats <- matrix(c(-2.7, 1.5, -5.3, -0.4,
                          0.5, 0.6, -0.4, 1.5,
                          0.6, 0.7, -0.5, 1.9,
                          0.3, 0.4, -0.3, 0.8, 
                          0.1, 0.1, -0.1, 0.2, 
                          -0.1, 0.3, -0.6, 0.3,
                          4.5, 0.8, 3.2, 5.9, 
                          0.0, 0.0, 0.0, 0.1, 
                          0.0, 0.0, 0.0, 0.0), ncol=4, byrow=TRUE) #this is the plogis values for mean and SD ### UPDATE THESE NUMBERS
colnames(summary.stats) <- c( "Mean (log odds)", "S.D.", "5%", "95%")
rownames(summary.stats) <- c("Intercept", "Direct Experience with Flood Damage in Community", "Future Experience with Flood Damage in Community", "Increase in Flood Severity", "Risk Attitude", "Trust in Accuracy of FRM Scientific Products", "Proportion of Lidar Users in Respondent's FRM Network", "Level Communication with Respondent and their FRM Network", "Total Expertise in Respondent's FRM Network")
knitr::kable(summary.stats, caption="Estimation results from the model")
```

Our model suggests that the proportion of lidar users in a respondent's FRM network has the most significant effect on lidar use. Direct experience, future risk perception, and knowledge have moderate effect on lidar use. Risk attitude, trust in science, network communication and expertise have minimal effect on lidar use. 

```{r, posterior distribution, echo=FALSE, fig.cap="\\lablel{ppd}"}

areas.plot <- plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)

ci.plot <- plot(full.mod.priors)

ggpubr::ggarrange(areas.plot, ci.plot)

```
** calculate probability of lidar use based on predictor

Our results suggest that the respondent's social network has the largest effect on lidar use. More specifically, the proportion of respondent's alters that use lidar is significantly correlated with lidar use. Furthermore, Washington had a stronger correlation. Oregon had the largest correlation of 'r b.or.prop.lidar', where as Washington had the smallest of 'r b.id.prop.lidar'. These results are in line with what we expected to see because of the strong influence social learning has on technology adoption. 

```{r, prop lidar users plot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:prop.lidar.plot"}
ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Proportion of Lidar Users in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

```

Add a chart showing the variation 


## *Model Validation and Error*
The model has a Bayesian R-squared value of 'r r2.final.model`. 

predictive figure?


```{r, proportion of lidar users, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:prop.lidar.plot"}
prop.lidar.plot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Proportion of Lidar Users in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

prop.lidar.boxplot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Proportion of Lidar Users in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(prop.lidar.plot, prop.lidar.boxplot)

```

#### **Total Expertise of Alters in Network**
```{r, total expertise of FRM network model, include=FALSE}
expert.mod <- stan_glmer(lidaruse ~ (1+expert_total|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(expert.mod)
median(bayes_R2(expert.mod))
plot(expert.mod, "areas", prob = 0.95, prob_outer = 1)

b.id.expert <- 0.0
b.or.expert <- 0.0
b.wa.expert <- 0.1

```
The results on this are a bit confusing. While the slopes are 0, the r2 suggests that it contributes a significant amount to the variation? 

```{r, expertise, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:expertise.plot"}
expertise.plot <- ggplot(model.data.na.omit, aes(x = expert_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Perceived Expertise in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

expertise.boxplot <- ggplot(model.data.na.omit, aes(x = expert_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Total Perceived Expertise in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(expertise.plot, expertise.boxplot)

```

### *Moderate Predictors of Lidar Adoption*

#### **Total Communication with Alters in Network**
```{r, total communication with FRM network model, include=FALSE}
comm.mod <- stan_glmer(lidaruse ~ (1+comm_total|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(comm.mod)
median(bayes_R2(comm.mod))
plot(comm.mod, "areas", prob = 0.95, prob_outer = 1)

b.id.comm <- 0.0
b.or.comm <- 0.0
b.wa.comm <- 0.1

```

Similar results to expertise, just lower r2 value. 

```{r, comm in network, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:comm.plot"}
comm.plot <- ggplot(model.data.na.omit, aes(x = comm_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Communication with Alters in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

comm.boxplot <- ggplot(model.data.na.omit, aes(x = comm_total, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
               width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Total Communication with Alters in Respondents FRM Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
 theme_bw() +
 theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(comm.plot, comm.boxplot)

```

#### **Awareness of Climate Change**

```{r, awareness model, include=FALSE}
awareness.mod <- stan_glmer(lidaruse ~ (1+incr_sev_flood|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(awareness.mod)
plot(awareness.mod, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(awareness.mod))

b.id.awareness <- 0.2
b.or.awareness <- 0.3
b.wa.awareness <- 0.6

```

Awareness was measured by asking the survey respondent about the perception of the average severity of flood damage in their community decreasing, staying the same, or increasing. Overall, there was a positive correlation between awareness of future risk and lidar use. Washington had largest slope of  This could be due to Washington having seen significantly more damage and portion of population at risk to floods shown in $\label{tab:state_comp}$. 

```{r, awareness, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label:awarnessplot"}
awareness.plot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

awareness.boxplot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(awareness.plot, awareness.boxplot)

```

#### **Future Flood Damage**
```{r, future damage model, include=FALSE}
future1.mod <- stan_glmer(lidaruse ~ (1+future_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(future1.mod)
median(bayes_R2(future1.mod))

b.id.future <- 0.5
b.or.future <- 0.4
b.wa.future <- 0.6

```


Building off the first predictor, awareness, it is important for a community keep awarness high for it often has influence on how individuals may respond to risk. This analysis also investigated this awareness by examining the perceived future risk of damage from flooding. The survey measured this by asking the respondent how likely they think damage to property in their community will happen in the next 30 years and could choose 0%, 25%, 50%, 75%, or 100% of happening. Washington had the strongest correlation of future flood risk and lidar use with a slope of 'r b.wa.future' and Oregon with the smallest of 'r b.or.future'.

```{r, future damage, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:memplot"}
future1.plot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

future1.boxplot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

ggpubr::ggarrange(future1.plot, future1.boxplot)

```

#### **Flooding Outside of the Flood Zone**
This study also examined the awareness of the flood risk manager on historical flooding outside the designated FEMA flood zone in their community. This question was helpful to gauge the accuracy of floodplain maps. The survey measured this by asking the respondent if they were aware of floods happening outside of the designated flood zone on their community flood maps with a binary response of yes or no. Oregon had the strongest correlation of reporting flooding outside the flood zone and adopting lidar with a slope of 'r b.or.flood zone'.

```{r, flood zone model, include=FALSE}

floodzone.mod <- stan_glmer(lidaruse ~ (1+flood_zone|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(floodzone.mod)
median(bayes_R2(floodzone.mod))


b.id.floodzone <- 0.5
b.or.floodzone <- 0.8
b.wa.floodzone <- 0.5

```

```{r, floodzone damage, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:floodzone.plot"}
floodzone.plot <- ggplot(model.data.na.omit, aes(x = flood_zone, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

floodzone.boxplot <- ggplot(model.data.na.omit, aes(x = flood_zone, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(floodzone.plot, floodzone.boxplot)

```


#### **Risk Attitude**

```{r, risk attitude model, include=FALSE}
soep1.mod <- stan_glmer(lidaruse ~ (1+soep_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(soep1.mod)

b.id.soep <- 0.0
b.or.soep <- 0.0
b.wa.soep <- 0.1

```


As discussed earlier, there is a risk inherent with adopting a new technology such as lidar due to variable time delay and reward. This was measured by asking respondents to report their general risk tolerance between 0, risk averse, to 10, risk-loving. Washington was the only state that displayed a correlation between risk-taking attitude and lidar with a slope of 'r b.wa.soep'. In addition, Washington also reported the least risk aversion, on average, and therefore may explain flood risk managers willingness to take on the risk of adopting a new technology.

```{r, risk attitude, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:soepplot"}
soep1.plot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

soep1.boxplot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Flooding Outside of Flood Zone', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(soep1.plot, soep1.boxplot)
```

### *Minor predictors of lidar use*

```{r, experience model, include=FALSE}
exp.mod <- stan_glmer(lidaruse ~ (1+experience_1|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(exp.mod)

b.id.exp <- 0.1
b.or.exp <- 0.1
b.wa.exp <- 0.1

```

#### **Direct Experience**

Direct experience has been studied extensively in the past as a significant predictor of individual risk perception and behavior. Our survey asked respondents to report if they had experienced damage in their community, with yes or no responses. Our results found a minor effect of experience on lidar use and all three states had the same slope of 0.1. Previous research has found variable effects of experience on behavior and suggest that measuring the intensity of the event experience could provide a more informative measure. 


```{r, experience, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:expplot"}
exp.plot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

exp.boxplot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(exp.plot, exp.boxplot)
```

#### **Trust in Scientific FRM Products**

```{r, science trust model, include=FALSE}
science.mod <- stan_glmer(lidaruse ~ (1+science_trust|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(science.mod)

b.id.science <- -0.1
b.or.science <- 0.0
b.wa.science <- 0.1 

```

The survey measured this by asking respondents to report how much they trusted the usefulness of FEMA's flood risk management scientific products on a likert scale from not at all to completely. Our results found that Idaho had a negative slope of 'r b.id.science' on lidar use, where as Washington had a positive slope of 'r b.wa.science'. This is an interesting contradiction between the two states...

```{r, science trust, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:outeffplot"}
science.plot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
 scale_x_discrete(limits=c(1, 5), labels=c("Strongly distrust", "Strongly trust")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

science.boxplot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Strongly distrust", "Neither trust nor distrust", "Strongly trust")) +
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(science.plot, science.boxplot)
```

#### **Prepared**

```{r, prepared model, include=FALSE}
prep.mod <- stan_glmer(lidaruse ~ (1+prepared|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(prep.mod)

b.id.prep <- 0.0
b.or.prep <- 0.1
b.wa.prep <- 0.1

```

The survey measured perceived community preparedness by asking respondents to report how prepared they felt their community was for a flood event with likert response options from not at all prepared to completely prepared. Both Oregon and Washington had a positive slope of 'r b.or.prep' between preparedness and lidar use, where Idaho had no relationship. 

```{r, prepared, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:prepplot"}
prep.plot <- ggplot(model.data.na.omit, aes(x = prepared, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Level of preparedness', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

prep.boxplot <- ggplot(model.data.na.omit, aes(x = prepared, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Level of preparedness', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(prep.plot, prep.boxplot)
```

#### **Trust in Government FRM Products**

```{r, gov trust model, include=FALSE}
gov.mod <- stan_glmer(lidaruse ~ (1+gov_trust|location), data=model.data.na.omit, family=binomial(link=logit), adapt_delta = 0.99)

summary(gov.mod)

b.id.gov <- 0.0
b.or.gov <- -0.1
b.wa.gov <- 0.0

```

Often times trust in the government has been measured in relation to ability to adapt. Although, most research has been conducted in terms of the trust the public has for government officials to protect them from floods. This survey asked flood risk managers how much they trust the usefulness of the products the federal government develops for FRM. Idaho and Washington both did not report an effect of this on lidar use, however Oregon had a negative slope of 'r b.or.gov'. Overall, there was a not a significant relationship between trust in government products and lidar use. This could be because flood risk managers in Oregon feel that these products are sufficient and they do not need additional data from lidar.  

```{r, gov trust, echo=FALSE,message=FALSE, warning=FALSE, fig.cap="\\label:govplot"}
gov.plot <- ggplot(model.data.na.omit, aes(x = gov_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Trust in government FRM products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

gov.boxplot <- ggplot(model.data.na.omit, aes(x = gov_trust, y = lidaruse, colour = location)) +
  geom_jitter(width=0.2, height=0.2, color="grey")+
  geom_point(stat = 'summary', fun.y = 'mean') +
  geom_errorbar(stat = 'summary', fun.data = 'mean_se', 
                width=0, fun.args = list(mult = 1.96)) +
  facet_grid( location ~ . ) +
  labs(x = 'Trust in government FRM products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1, 3, 5), labels=c("Not at all", "Moderately", "Completely")) + 
  theme_bw() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )
ggpubr::ggarrange(gov.plot, gov.boxplot)
```


Shortfall minimization is a critical component of decision-making under risk. As discussed previously, risk can be minimized by resource pooling and social learning. Since lidar is an expensive investment, funding opportunities and collaborations can help minimize and share the risk of investing in lidar. For example, the Washington Geological Survey was granted funding from 2015-2021 for the collection and distribution of lidar data and lidar-derived products. Therefore, individuals who adopt this technology in Washington have the potential to minimize their risk of investment by pooling resources. Due to this resource pooling, we expect Washington to see an increase in lidar adoption. In addition, social learning can influence the norms and beliefs that a flood risk manager holds about appropriate adaptive behavior. Research has shown that collective memory provides context and is an important indicator of a communityâ€™s flood risk culture [@viglioneInsightsSociohydrologyModelling2014; @wachingerRiskPerceptionParadoxImplications2013]. Collective memory represents a communityâ€™s ability to keep awareness of previous events that shape their decisions for the future. Often times, flood risk managers develop their collective memory based on information they learn from their peers and experience within their community. This awareness influences an individual to take action based on a collective idea of risk, rather than from individual belief. 
- add section about awareness

## *Additional analyses*
**haven't touched this part of the analysis yet. I think feedback on the first part would be super helpful to direct this next section**

The following are parts of the analysis that I am not sure if I should include or not: 

```{r, exploratory factor analysis, include=FALSE}
#Do i need to do this?? https://www.r-bloggers.com/2018/05/exploratory-factor-analysis-in-r/

#Loading the dataset
#bfi_data=model.data
#Remove rows with missing values and keep only complete cases
#bfi_data=bfi_data[complete.cases(bfi_data),]
#Create the correlation matrix from bfi_data
#bfi_cor <- cor(bfi_data)
#Factor analysis of the data
#factors_data <- fa(r = bfi_cor, nfactors = 3)
#Getting the factor loadings and model analysis
#factors_data
```

```{r, aic, include=FALSE}
## Not sure how helpful this is...
# Full model using all predictors AIC or BIC # However, this is not a good approach for data with lots of na's
# https://bookdown.org/egarpor/PM-UC3M/app-nas.html

#model.data.na.omit <- na.omit(model.data)
#n = nrow(model.data.na.omit)
#n # 128

# k = log(n): penalty for BIC rather than AIC
#AIC.lm <- lm(lidaruse ~ ., data=model.data.na.omit,  family= binomial(link = "logit")) #I've been getting a warning about divergent transitions after warmup. In order to make this better, I am going to make the step size smaller, I also have eliminatead varying effects for the purposes of AIC # Okay looks like I can only run a LM, GLM doesn't provide step information for AIC

#AIC
#mod.AIC = step(AIC.lm, k=2) 
# the best model is with flood_zone, future_1, and incr_sev_flood
```

```{r, k fold, include=FALSE}

#kfold1<- kfold(mod1, K=10) 
#kfold1

#kfold2 <- kfold(mod2, K=10) 
#kfold2

#loo_compare(kfold1, kfold2) #mod2 is slightly better
```

```{r, LOOCV, include=FALSE}

#Result_G <- data.frame(matrix(nrow=nrow(model.data.na.omit), ncol=6))

#set the column names
#colnames(Result_G) <- c("MedianG", "LowerBound1_G", "LowerBound2_G", "UpperBound1_G", "UpperBound2_G", "R2_Bayes")

#do the leave one out cross validation
# create for loop function
#for(i in 1:length(model.data.na.omit$lidaruse)){ # for all the data points
 # sub_dat <- model.data.na.omit[-i,] #remove each one at a time
 # m_sub <-  stan_glmer(lidaruse ~ incr_sev_flood + soep_1 + future_1 + flood_zone +
                   #science_trust + (1|location), 
                  # prior = normal(location = c(0, 0, 0, 0, 0), 
                  # scale = c( 3.703526, 1.078133, 8.236779, 4.987931, 3.271231), 
                  # autoscale = FALSE), # this specifies prior between 0 and 1 because it is a binonmial response variable
                           #prior_intercept = normal (location = 0, 
                          # scale = 2.5, 
                          # autoscale = FALSE), # intercept can only be between 0 and 1 
                   #data=model.data.na.omit, family=binomial(link=logit), control = list(adapt_delta = 0.99))
  #post=posterior_predict(m_sub, model.data.na.omit[i,], draws=4000)
  #r2=r2_bayes(m_sub)# predict that point
 #Result_G[i,1]=quantile(post, 0.5) # fill a df with the credibility intervals
  #Result_G[i,2]=quantile(post, 0.25) 
  #Result_G[i,3]=quantile(post, 0.025)
  #Result_G[i,4]=quantile(post, 0.75)
  #Result_G[i,5]=quantile(post,0.975)
  #Result_G[i,6]=r2
#}

#write.csv(Result_G, "G:/Everyone-Temp/TaraPozzi/Survey/Analysis/data/Result_G.csv")
```

```{r, model plot, echo=FALSE, include=FALSE}
#pplot<-plot(mod2.priors, "areas", prob = 0.95, prob_outer = 1)
#pplot+ geom_vline(xintercept = 0)


# let's simulate data to see what how well our model is predicting
#incr_sev_flood.sim <- rep(c(1:3), each=129)

# set other predictor variables to their lowest affect at their minimum

#simdata <-add_fitted_draws(newdata=data.frame(incr_sev_flood= incr_sev_flood.sim,
                                              #soep_1=5,
                                             # future_1=1,
                                             # flood_zone=2,
                                             # science_trust=1),
                          # mod2.priors) 

## Plot the results 
#sev.plot <- ggplot(simdata, aes(as.factor(incr_sev_flood), .value)) + 
  #geom_boxplot() + 
  #labs(x = "Flood severity experienced \nby survey respondent", y = "Effect on probability of LiDAR adoption \n while holding other IVs at lowest values") +
 # theme_bw() 
  
```





# 5 CONCLUSION (~500)
**"The main conclusions of the study may be presented in a short Conclusions section, which may stand alone or form a subsection of a Discussion or Results and Discussion section."**

While this theoretical model has never been applied to understanding risk in hazard management, I think that it could provide improved insight into significant predictors of long-term risk mitigation. This paper examines alternative predictors to risk perception in an effort to address why risk perception may not align with long-term risk mitigation behavior, also called the "Risk Perception Paradox." From a behavioral ecology perspective, these predictors could be cultural and contextual factors that moderate risk perception's effect size on decision-making and behavior. This paper investigates how these predictors may affect lidar adoption in flood risk management in Idaho and Washington. This makes an interesting and effective case study because the benefits of this technology have a variable reward and time delay, two key factors that can feel risky in adopting this technology. Furthermore, the findings from this research could have important implications for the risk field of research and advance our understanding of the driving factors of an individual's long-term risk mitigation behavior. 

Future studies: need to do a longitudinal study to see how lidar adoption changes over time especially with target barrier reduction and increase network communication and expertise.
# 5 Literature Cited