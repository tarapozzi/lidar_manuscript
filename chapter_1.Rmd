---
title: "Chapter 1 Manuscript"
author:
- name: Tara Pozzi
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::word_document2: default
  bookdown::pdf_document2: default
thanks: '.....'
affiliation: Boise State University
runhead: XXXX
geometry: margin=1in
fontsize: 12pt
csl: crm.csl
bibliography: references.bib
indent: yes
colorlinks: yes
---

```{r setup, include=FALSE}
library(citr) # doesn't work in this version of r...
library(tidyr)
library(ggplot2)
library(ggplotgui) # doesn't work in this version of r 
library(ggrepel)
library(plotly)
library(RColorBrewer)
library(here)
library(tinytex)
library(psych)
library(pastecs)
library(rstanarm)
library(loo) 
library(tidybayes)
library(tidyverse)  # ggplot, dplyr, %>%, and friends
library(ggdag)  # Make DAGs with ggplot
library(dagitty)  # Do basic DAG math
library(broom)  # For converting model output to data frames
library(clusterSim)
library(caret) # model comparison
library(plyr) # helps with model comparison computation
library(performance)
library(rworldmap)
library(hrbrthemes)
library(ggplotAssist)
library(dplyr)
library(purrr)
library(forcats)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(rstan)
library(bookdown) # for cross-referencing
library(knitr) #for global options
library(formatR) 
```

```{r, global options, include=FALSE}
# Setup options for code decoration
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
highlight = TRUE)

# Setup options for code cache
opts_chunk$set(cache = 2, cache.path = "cache/")

# Setup options for plots
opts_chunk$set(fig.path = "Figures_MS/", dev = c("pdf", "png"),
dpi = 300)

## Locate figures as close as possible to requested position
## (=code)
opts_chunk$set(fig.pos = "H")

```

Abstract
========================
**"A concise and factual abstract is required. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself."**

# HIGHLIGHTS
*Come back and edit this when results and discussion are finished*

1. There is between state variation of lidar use between Idaho and Washington.

2. Data suggests a more variable environment lends to flood risk manager's making more risk-prone decisions.

3. Social learning plays a critical role in shortfall minimization and lidar adoption.

# Introduction
*climate change and population rates are dangerous bc of increased flooding*
Floods are one of the most frequent and destructive natural disasters in the United States [@pralleDrawingLinesFEMA2019;@RiskMappingAssessment]. Flood events disrupt the ecological, cultural, and economic landscapes causing incalculable expenses to our society. Often, resulting with vulnerable groups even more at risk in the future [@howellDamagesDoneLongitudinal2019]. Since the National Centers for Environmental Information (NCEI) began tracking natural disaster events in 1980, there has been an increase in flood events in the U.S., some of those with unprecedented amounts of rainfall [@informationnceiBilliondollarWeatherClimate]. This is because as temperatures rise there is an increase in the amount of water vapor in the atmosphere, which exacerbates the potential for extreme rainfall events. In addition to growing flood risk from climate change, there is an rising rate of population growth and urbanization in coastal and inland floodplains [@pralleDrawingLinesFEMA2019; @schanzeFloodRiskManagement2006]. In 2015, 21.8 million (6.87%) of the U.S. population were identified as being exposed to a 100-year flood; meaning they lived in a location that would be inundated by a flood event that has a 1 in 100 chance of happening each year [@qiangDisparitiesPopulationExposed2019]. In light of these challenges, understanding how to manage flood risk is critical.

*introducing concept of risk risk*
Risk has a variety of definitions based on the disciplinary domain in which the concept is being examined. In an everyday sense, risk can be considered the chance of a negative outcome occurring [@mishraDecisionMakingRiskIntegrating2014]. We see risk as inherently transdisciplinary and needs to encapsulate the full context of the topic for which it is being applied. Therefore we define risk, in a flood context, as the quantifiable chance of a flood event given the known, contextual (e.g. social, environmental, political) factors. 

*floodplain maps as a function of risk*
Communities understand their flood risk typically by using Federal Emergency Management Agency (FEMA) floodplain maps that estimate the extent of flood hazards through hydrologic and hydraulic models. These analyses require topography, rainfall and run-off frequency distributions, and flood control structures (e.g. diversion dams, levees, bridges). In addition, these floodplain maps are essential for communicating flood risk to vulnerable populations, helping communities mitigate and adapt to floods, and the functioning of insurance programs, such as the the FEMA's National Flood Insurance Program [@pralleDrawingLinesFEMA2019]. However, recent reports estimate that approximately 25% of the flood damage claims occur outside of FEMA mapped floodplains each year because these maps can be outdated and inaccurate [@ludyFloodRiskPerception2012]. 100-year flood events are based on historical rainfall patterns, however this probability can change based on local land use, river impoundments, the amount of impervious surfaces, and long-term climate patterns [@TechniquesMethods2019].

*lidar as the solution*
-	Lidar is super accurate and awesome
-	 But it’s new and people don’t know how to use it
-	 it’s expensive to obtain

Previous research confirms that high-resolution topographic data is critical for an accurate floodplain map [@aliAssessingImpactDifferent2015; @cookEffectTopographicData2009]. In the past, flood risk managers typically used 10-meter or 30-meter resolution terrain models. However higher-resolution terrain models (e.g. 1-meter or smaller) are now available due to Light Detection and Ranging (lidar). Lidar is a laser-based remote sensing technology that uses the reflection of light to measure elevation and features on the ground such as vegetation and structures. Raw lidar data points are used to form a three-dimensional (3D) point cloud. These 3D point clouds can then be used in a wide-array of hazard applications such as wildfire fuel load calculation or wildlife habitat viewshed identification. In addition, lidar-derived products, such as high-resolution terrain models, are widely used in flood risk management to model different flooding scenarios [@muhadiUseLiDARDerivedFlood2020].

Figure \@ref(fig:lidar_map) displays the footprint of available topographic and bathymetric lidar across the contiguous, lower 48 states as of February 2021. From this image, there is a clear decrease in the availability of publically-accessible lidar in the western U.S. including Washington, Idaho, Montana, Oregon, Nevada, Utah, California, Arizona, and New Mexico. As lidar becomes more available and increasingly popular, it is important to understand the factors that influence a flood risk manager's decision to adopt this new technology into their practice of long-term risk mitigation.

![(fig:lidar_map)"The U.S. Interagency Elevation Inventory displays all publically-available lidar data and lidar-derived products for the continguous, lower 48 states."](C:/Users/tarapozzi/Documents/lidar_manuscript/images/lidar_map_us.jpg)

This study examines the factors that catalyze decision-makers to use lidar for flood risk management. Past research emphasizes risk perception as a key driver of risk mitigation behavior [@birkholzRethinkingRelationshipFlood2014;@mishraDecisionMakingRiskIntegrating2014;@slovicPerceptionRisk1987]. Risk perception is a fundamental way to characterize an individual’s intuitive risk judgment and allows for the identification, characterization, and quantification of risk. However, previous research on risk perception has uncovered conflicting results. Past findings suggest individuals who perceive that natural hazards pose a greater risk also behave more cautiously [@vinhhungFloodRiskManagement2007]. Interestingly, studies also found the opposite in that individuals who perceive a greater risk engage in fewer mitigating behaviors [@bubeckReviewRiskPerceptions2012; @wachingerRiskPerceptionParadoxImplications2013]. This paradoxical behavior suggests that risk perception is more nuanced and moderated by individual and socio-cultural factors [@baerenklauUnderstandingTechnologyAdoption2005; @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012; @kahanCultureIdentityProtectiveCognition2007; @vanvalkengoedMetaanalysesFactorsMotivating2019].

While the field of hazards and disaster research is inherently multidisciplinary, there has been a recent, concerted effort for convergence research to integrate knowledge across disciplines and organizational boundaries to reduce disaster losses and promote collective well-being (Peek at al., 2020). In addition, previous flood risk research identified two main theoretical limitations of our existing intellectual understanding of risk-mitigating behavior. Firstly, there is limited predictive power of the theories applied so far (e.g. protection motivation theory) [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action (e.g. social beliefs) [@kuhlickeBehavioralTurnFlood2020].
In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating behavior; there needs to be a critical look at the underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. We employ cultural evolutionary theory to study lidar adoption because it offers a convergent research lens to compare individual and collective action and has the potential for prediction.  

In the next section we review cultural evolutionary theory in detail and explain how this theory is beneficial for understanding the underlying mechanisms that shape flood risk management behavior. Next, we apply this theoretical framework to our case study of lidar adoption for flood risk management.  This is followed by the methods section that explains our survey instrument development process and statistical approach for analyzing the survey data. The results from our analysis and a discussion about significant trends will follow. Finally, we discuss the implications of these results and need for further research. 

# Individual and collective predictors of risk-mitigation behavior

Previous research identified the importance of several individual factors as a function of risk mitigation behavior, however there has been limited research in the role of collective action [@kuhlickeBehavioralTurnFlood2020]. It is important to look at the combined effects of both individual and collective predictors in predicting risk-mitigation behavior so that we can understand the relative contribution of each predictor [@vanvalkengoedMetaanalysesFactorsMotivating2019]. In this study, we examine the influence of a collection of individual and collective predictors on a flood risk manager's adoption of lidar for long-term risk mitigation. The following section examines previous research into individual predictors of risk mitigation behavior and then explores how cultural evolutionary theory can help illuminate collective predictors of influence. 

## *Topical Review* 
Previous flood risk management research focused on flood risk perception as a critical factor of developing effective flood risk management strategies [@birkholzRethinkingRelationshipFlood2014]. However, recent research has re-examined risk perception's role in behavior and decision-making because of the difficulty connecting risk perception with management and the challenge of parsing out the connection of risk perception with underlying contextual factors [@rufatSwimmingAloneWhy2020]. For example, a study by Bubeck et al. (2012) found risk perception to be a weak predictor of precautionary behavior and suggests shifting focus towards flood-coping appraisal for explaining flood risk management behavior. In addition, Kellens et al. (2013) reviewed 57 empirically based peer-reviewed articles on flood risk perception and communication to assess overall trends in flood risk research. The authors found that the majority of studies were exploratory and did not apply a theoretical framework to examine risk perception [@kellensPerceptionCommunicationFlood2013a]. Of the studies that employed a theoretical framework, protection motivation theory (PMT) was the most common. PMT explains individual decisions about preparing for risk as a function of threat appraisal (e.g. likelihood of exposure to a flood, severity of exposure, and fear) and coping appraisal (e.g. self-efficacy, outcome efficacy, and outcome costs). The results of this review suggest future research should have more theoretical support and methodological openness; specifically, the use of a theoretical framework that emphasizes the effects of physical exposure and hazard experience [@kellensPerceptionCommunicationFlood2013a]. 

Collective factors of risk mitigation behavior are limited in the existing flood risk management literature, however there has been some initial evidence found of the influence of social networks on risk mitigation behavior [@lechowskaApproachesResearchFlood2021;@kuhlickeBehavioralTurnFlood2020;@bojovicUnderstandingDisseminationAdoption2020]. Social networks are of particular interest for our study because they are a way of measuring peer influence, the diffusion of ideas, practices, or technologies through network ties from social interactions [@muterSocialContagionRisk2013]. Peer influence is a helpful tool for behavior prediction based on an individual’s position in a social network [@daraganovaAutologisticActorAttribute2012; @levinProblemPatternScale1992]. Furthermore, the technology adoption literature has also applied network analysis to measure information exchange and diffusion through network relations [@pengResearchNoteDynamic2013]. The application of social networks to flood risk management decision-making is still in its infancy, however the findings from previous research with respect to social networks and technology adoption provide a compelling baseline for using it to understand peer influence in our study.

Additionally, recent research suggests the importance of context, local power relations, constraints, and opportunities that affect risk mitigating behavior calling for convergence research to understand the underlying assumptions of decision-making [@rufatSwimmingAloneWhy2020]. Given the current gaps of understanding in flood risk management research and the push for convergent research, we employ cultural evolutionary theory to employ a comprehensive theoretical baseline for flood risk mitigation behavior research that can be used across disciplines and scales. 

Secondly, current literature is pre-dominantly focused on the public's flood risk behavior, rather than flood risk managers themselves. There has been some previous work focused on emergency manager decision-making, however this area is understudied [@robertsDecisionBiasesHeuristics2019; @brodyExaminingClimateChange2010]. Our study is solely focused on addressing individual and collective predictors of risk mitigation behavior at the decision-maker level.

## *Culture and Risk*
Culture is information acquired by individuals through social learning [@henrichEvolutionCulturalEvolution2003]. Social learning is the observing, modeling, and imitating of behaviors, attitudes, and emotional reactions of others (Bandura, 1977). Social learning differs from individual learning, which is learned from the environment and nonsocial, but is not mutually exclusive [@perreaultBayesianApproachEvolution2012]. The process of culture creates a shared set of beliefs and norms among a group of individuals. Several researchers believe social learning has improved human adaptability so much that we are able to inhabit such a wide range of habitats, unlike other animal species [@creanzaCulturalEvolutionaryTheory2017].  

Behavioral adaptations display the variation of culture as a result of the evolutionary dynamics of cultural systems. Cultural evolutionary theory describes this process as the selection and transmission of culture over time. The selection process leads to variation of culture across temporal, spatial, and institutional scales and the transmission leads to adaptation (e.g. adoption of new technology). Reminiscent of genetic evolution, human culture evolves through the process of natural selection. This evolution results in between-group variation of adaptive behavior and cooperation and can lead to increased fitness or utility [@richersonCulturalGroupSelection2016; @henrichEvolutionCulturalEvolution2003]. Unlike genetic transmission, it is important to note cultural transmission can occur over a short time scale, within a generation, through social learning [@richersonCulturalGroupSelection2016]. Cultural evolutionary theory and social learning are increasingly popular theories that have been used to explain a wide range of phenomena such as natural resource management, sports strategy, and institutional variation [e.g. @reedWhatSocialLearning2010; @brooksApplyingCulturalEvolution2018; @mesoudiCulturalEvolutionFootball2019; @richersonCulturalGroupSelection2016]. 

In a similar vein, the cultural theory of risk is the transmission of risk information among a network of individuals through social learning [@douglasRiskCultureEssay1983]. Previous flood risk management research has suggested the use of cultural theory of risk to contextualize the relationship of risk perception as a function of cultural adherence and social learning [@birkholzRethinkingRelationshipFlood2014]. This theory has been employed in a couple empirical flood risk management studies so far and provides an intriguing underpinning of risk perception research [@shenFloodRiskPerception2009]. Cultural evolutionary theory is similar to cultural theory of risk, however it more broadly offers a way to understand the complex dynamics of cultural change through interactions between individuals and populations, such as is needed for flood risk management (Brooks et al., 2018).

## *Predictor Literature Review*
In order to select relevant individual and collective predictors of flood risk mitigation behavior *a priori*, we conducted a literature review of previous work that looked at the effect of the constructs outline in Figure \@ref(fig:predictor-litreview) on flood risk mitigation behavior.  

![ (#fig: predicitor-litreview) "Individual and collective constructs, with the motivation, effect, and supporting literature for each construct, that potentially influence risk mitigating behavior for flood risk management"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_lit_review.jpg)

# METHODS

## *Case study description*
This study examines the adoption of lidar in communities throughout Idaho, Oregon, and Washington which are expected to see an increase in precipitation and higher temperatures earlier in the year [@clarkChangesPatternsStreamflow2010; @ralphVisionFutureObservations2014; @idahoofficeofemergencymanagementStateIdahoHazard2018; @washingtonemergencymanagementdivisionWashingtonStateEnhanced2020; @slaterRecentTrendsFlood2016]. In addition, the Pacific Northwest is experiencing increasing urbanization rates and population growth, therefore changing flood risk in communities. 

While Idaho, Oregon, and Washington all reside in the same geographic region, each state has unique sets of challenges when it comes to flood risk management due to different landscapes, population growth and urbanization, and resource availability (e.g. funding for flood risk management, educational opportunities for flood risk managers). In addition, each state has their own lidar coordination and acquisition program which has contributed differential levels of lidar availability as seen in Figure \@ref(fig:case_study_extent).

![(fig:case_study_extent)"Publicly-available lidar in our case study extents of Idaho, Oregon, and Washington."](C:/Users/tarapozzi/Documents/lidar_manuscript/images/case_study_extent.jpg)
### **Idaho**
In 2019, Idaho was home to 1.79 million people across 82,643 square miles; 21.7 people per square mile [@CensusBureauQuickFactsb]. It is a land-locked state and can be broken down into three main areas: the panhandle in the north is filled with coniferous forests and lakes, the central section is filled with vast mountain ranges and alpine lakes, and the southern section, known as the Snake River Plain, is filled with sagebrush steppe and high desert environment. There is influence from the Pacific Ocean in the north and west side of Idaho, resulting in cloudy, humid, and wet winters, whereas the east is the opposite with wet summers and dry winters. Average annual rainfall ranges from 10" in the arid southwest regions to 50" at higher elevations in certain river basins [@d.o.oIdahoUSAClimate]. In addition, Idaho sees abundant amounts of snowfall in the mountains. 

While most of the population is concentrated in the southern part of the state, there is flooding across the entire state that impacts people and structures. Idaho is prone to riverine flooding, ice/debris jam flooding, levee/dam/canal breaks, stormwater, sheet or areal flooding, and mudflows [@idahoofficeofemergencymanagementStateIdahoHazard2018]. In 2021, Idaho had 145 NFIP participating communities across 44 counties [@CommunityStatusBook]. 

Lidar acquisition is coordinated by Boise State University's Idaho Lidar Consortium in conjunction with Idaho State University's GIS Research and Training Center, which stores lidar data for public use. There is no state-approved funding set aside for lidar acquisition and therefore, communities rely on using local funding in addition to applying for funding from USGS and/or FEMA. By the end of 2020, Idaho had 25% of the state covered with publically-available lidar. 

### **Oregon**
In 2019, Oregon had over 4.2 million residents across 95,988 square miles; 43.8 people per square mile [@CensusBureauQuickFactsa]. Oregon can be broken down into six main areas: the Coast Range, the Willamette Lowland, the Cascade Mountains, the Klamath Mountains, the Columbia Plateau, and the Basin and Range Region. There is a maritime influence across the entire state due to the Pacific Ocean. The Coast range is predominantly evergreen forests with many small coastal lakes. The mountain regions are typically several thousand feet about sea-level and have a range of dense forests and lakes. Eastern Oregon contains high desert environment with few steep mountains. 

Oregon's population is concentrated in the coastal region of the state. Oregon has an extensive history of multiple types of flooding including riverine flooding, flash floods, ice/debris jam flooding, coastal flooding, shallow area flooding, urban flooding, and playa flooding [@laytonStateInteragencyHazard2015]. In 2021, Oregon had 228 NFIP participating communities across 36 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the State of Oregon Department of Geology and Mineral Industries' Oregon Lidar Consortium. By the end of 2020, Oregon had 98% of Oregon's populated areas were covered with publically-available lidar, although eastern Oregon has much sparser coverage of lidar [@DOGAMILidarOregon].

### **Washington**
In 2019, Washington had over 7.6 million residents across 66,455 square miles; 114 people per square mile [@CensusBureauQuickFacts]. Washington can be broken down into six main areas: the Olympic Mountains, Coast Range, Puget Sound Lowlands, Cascade Mountains, Columbia Plateau, and Rocky Mountains. Most of the areas in the western and northern parts of Washington are predominately evergreen forests, where the eastern and southern parts of Washington are semiarid where grasses, sagebrush, and scattered shrubs can be found. Annual precipitation on the Pacific side of the Olympic Peninsula exceeds 150 inches, but places on the northwest of the peninsula receive less than 20 inches a year and on the eastern side receive less than 8 inches [@WashingtonStateCapital].

More than three-fourths of the population lives in Puget Sound Lowlands [@WashingtonStateCapital] Flooding in Washington typically occurs on a seasonal basis due to rainfall from atmospheric rivers, rainfall on snow, flash foods from storms, and winter storms causing storm surges and high tide [@washingtonemergencymanagementdivisionWashingtonStateEnhanced2020]. It is estimated that in 2021, Washington had 277 NFIP participating communities across 39 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the Washington State Department of Natural Resources and receives funding from the Washington State Legislature to acquire and upkeep lidar data for the state. Over 50% of the state has been flown with lidar data [@GerWaLidar].

### **Physical flood risk**
Since flooding is becoming an increasingly damaging and costly issue, there has been a rise in interest from non-governmental groups to predict flood risk at the property level for households and property owners to be aware of their true flood risk. First Street Foundation, a non-profit organization of modelers, researchers, and data scientists, created the first publicly-available flood risk model for the lower 48 states. According to First Street, nearly 70% of properties have more substantial flood risk than previously predicted by FEMA floodplain maps [@firststreet.orgMissionFirstStreet]. This study is focused on Idaho, Oregon, and Washington. Therefore, in an effort to understand the nature of physical flood risk in each of these states, we have compared the FEMA projections to the First Street projections as seen in Table \@ref(tab:comparison-table). It is important to note that FEMA report's Idaho with the least amount of risk compared to Oregon and Washington, however First Street reports it as having the most. This difference could be because there are still many locations in Idaho that are not mapped by FEMA and therefore building in floodplain areas could be more likely.

```{r comparison-table, echo=FALSE, fig.cap="\\label{tab_statecomp}"}
id.floodrisk <- read.csv("data/idaho_floodrisk_firststreet.csv")
id.fema.2020.total <- round(sum(id.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
id.fema.2020.pct <- round(mean(id.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
id.fs.2020.total <- round(sum(id.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
id.fs.2020.pct <- round(mean(id.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

or.floodrisk <- read.csv("data/oregon_floodrisk_firststreet.csv")
or.fema.2020.total <- round(sum(or.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
or.fema.2020.pct <- round(mean(or.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
or.fs.2020.total <- round(sum(or.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
or.fs.2020.pct <- round(mean(or.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

wa.floodrisk <- read.csv("data/washington_floodrisk_firststreet.csv")
wa.fema.2020.total <- round(sum(wa.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
wa.fema.2020.pct <- round(mean(wa.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
wa.fs.2020.total <- round(sum(wa.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
wa.fs.2020.pct <- round(mean(wa.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

state.comp <- matrix(c(id.fema.2020.total, or.fema.2020.total, wa.fema.2020.total, id.fema.2020.pct, or.fema.2020.pct, wa.fema.2020.pct, id.fs.2020.total, or.fs.2020.total, wa.fs.2020.total, id.fs.2020.pct, or.fs.2020.pct, wa.fs.2020.pct), ncol=3, byrow=TRUE)
colnames(state.comp) <- c("Idaho", "Oregon", "Washington")
rownames(state.comp) <- c("Total FEMA Properties at Risk (2020) ", "Percent FEMA Properties at Risk (2020)", "Total FS Properties at Risk (2020) ", "Percent FS Properties at Risk (2020)")
knitr::kable(state.comp, caption="Summary information about environmental and social differences between Idaho and Washington.")
```

## *Relevant predictors of lidar adoption*

Given the previous literature, as well as the nature of our case study we narrowed down our study to focus on eight constructs. Table \@ref(tab:individual) displays the five individual predictors that we selected for our study. We chose these factors because they aligned with repeated themes in our semi-structured interviews, in addition to each factor providing important information to help inform lidar coordination. 

![(#tab:individual) "Individual predictors of lidar adoption"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_individual.jpg)

In addition, we selected three collective factors reflected in Table \@ref(tab:collective). 

![(#tab:collective) "Collective predictors of lidar adoption"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_collective-1.jpg)
Several studies have implemented social network analysis to examine the the influence of social ties on communication in disaster management, however the effect of social networks on other topics in disaster management has been minimally explored [@bojovicUnderstandingDisseminationAdoption2020]. Bojovic et al. (2020) conducted a full network analysis on the diffusion of innovation and technologies for risk management, which was the first study of this topic in disaster management. The study focused on the identification of key actors to implement information dissemination through. 

![(#fig:egonet) "Two Ego Network Analysis Structures. Center represents the ego and the lines represent the ties between the ego and their alters[@borgattiNetworkAnalysisSocial2009]."](C:/Users/tarapozzi/Documents/lidar_manuscript/images/egonet.jpg)


Our study uses an ego network analysis, which is helpful for understanding the variation of behavior of individuals through identification of local social structures unique to the individual of interest (e.g. flood risk manager) [@hannemanChapterEgoNetworks2005]. We used an Open Ego Network because it we measured the ego, their reported connections, known as alters, and the flow of information due to the network tie.

```{r, survey data upload, include=FALSE}

idaho <- read.csv("data/id.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Idaho") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(idaho)

oregon <- read.csv("data/or.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses  
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Oregon") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(oregon)

washington <- read.csv("data/wa.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Washington") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(washington)

alaska <- read.csv("data/ak.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Alaska") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(alaska)

# Combine all four states into one dataset # 
survey.responses <- rbind(idaho, oregon, washington) %>% 
  subset(.,incr_no_flood!="") # remove additional blank responses
  

#write.csv(survey.responses, "C:/Users/tarapozzi/Documents/Manuscript/survey.responses.csv")

```

### *Survey design*
Prior to finalizing our survey instrument, we conducted eight, semi-structured interviews with stakeholders including flood risk managers, government officials, industry professionals, and academics. The interviews lasted about an hour and were occasionally recorded. These interviews were used to identify common themes, ensure that our survey questions were relevant, and confirm that we were adequately identifying facilitators and barriers to lidar adoption.*add interview instrument to appendix*

Once we created our survey instrument, we conducted an expert review with eight university students and staff to give feedback about the appropriateness of the survey (e.g. length, difficulty, and readability), question fit to research questions, and survey structure (e.g. question order, section transitions, survey logic). Next, the survey was tested as a pilot survey with a flood risk manager, an industry professional, and a lidar academic to provide additional feedback from the perspective of a potential, target respondent. 

The finalized survey consisted of four main parts (see Appendix B). The first part focused on gathering information about the respondent's experience and beliefs about their flood risk management community. The second section was centered around the respondent's relationship with lidar for flood risk management including if they used lidar, how they use lidar, and if they would like to take part in lidar workshops. The third part of the survey gathered information about the respondent's flood risk management network. Lastly, the final part of the survey asked the respondent about their personal beliefs in risk-taking, trust, and demographic questions such as education and gender.  

### *Data collection*
Our survey's target population included floodplain managers and administrators in Idaho, Oregon, Washington, and Alaska. This also included individuals that may use lidar for flood risk management applications in conjunction with software applications such as Geographic Information System (GIS). The majority of sample respondents were municipal, state, and federal employees, as well as some private industry employees. We constructed our sample frame using several publicly available lists of managers including NFIP coordinators, Association of State Floodplain Managers (ASFPM) recognized Certified Floodplain Mangers (CFM), county-level GIS administrators, the five largest cities and tribal GIS administrators if present, county and tribal emergency managers, the Federal Geospatial Data Coordination Contacts by State, and additional, relevant contacts for the 2019 Northwest Regional Floodplain Managers Association Conference contact list. 

```{r, survey distribution, include=FALSE}
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
  filter(grepl("2", screen))

#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

a <- length(wa.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-13: 383 potential responses

# oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
  filter(grepl("2", screen))

#load distribution history
or.dist <- read.csv("data/or_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

b <- length(or.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 357-13: 344 potential responses


# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
c <- length(id.pot.responses$Email)
# 463 potential responses


#load distribution history
alaska.results <- read.csv("data/ak.csv")
ak.no <- alaska.results %>%
  filter(grepl("2", screen)) # 2 said no to screening

ak.dist <- read.csv("data/ak_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
d <- length(ak.dist$Email) - 2

total.sample.frame <-  a + b + c + d


```

``` {r, response rate, include=FALSE}

id.rr <- round((96/a)*100, digits=1) # a is the total number of potential responses

or.rr <- round((58/b)*100, digits=1)

wa.rr <- round((54/c)*100, digits=1)

ak.rr <- round((6/d)*100, digits=1)


```

We delivered the survey online using Qualtrics to 1,257 email addresses in our sample frame between May and July 2020. The survey took an average of 10 to 15 minutes to complete. We used Dillman et al. (2014) guidelines for web and mobile survey implementation. We initially set an introductory email that stated what was being asked of respondents, why they were selected, and information about the intent, purpose, and outcomes of the survey [@dillmanInternetPhoneMail2014]. We sent three to five follow-up email correspondence messages over the course of four weeks to help increase our response rate. In addition, we stated the survey was anonymous and their information would be kept confidential. Table \@ref(tab:data-collection) summarizes the potential respondents, number of survey responses, and response rate for each state. 

```{r, data-collection, echo=FALSE, fig.cap="\\label{survey data collection}"}
survey.data <- matrix(c(a, 96, id.rr, b, 58, or.rr, c, 54, wa.rr, d, 6, ak.rr), ncol=3, byrow=TRUE)
colnames(survey.data) <- c("Potential Respondents", "Number of Responses", "Response Rate")
rownames(survey.data) <- c("Idaho", "Oregon", "Washington", "Alaska")
knitr::kable(survey.data, caption="Comparative survey distribution and collection.")
```

We did not include Alaska in our final statistical analysis because of an insufficient number of responses. In addition, both Oregon and Washington had lower response rates than Idaho. Our response rates are within the typical bounds for online surveys of 10-25% [@sauermannIncreasingWebSurvey2013].  

### *Data Analysis*

We used a Bayesian Generalized Logistic Regression (GLR) to estimate the relationship between our predictors of interest and our response, lidar use, because it is binary.The results of this model allowed us to explore the effect of a multitude of predictors on lidar use in Idaho, Oregon, and Washington. We hypothesized that the model would be helpful for understanding the level of predictor influence, however we expected the predictive capacity of our model to be limited considering the large number of predictors and small sample size of our study. 

The model followed a binomial distribution curve, where the distribution of lidar use, y_{ij}, was modeled as follows: 

\[b_i \approx N(0,\sigma_b)\]

\[\eta_i = \mu_\alpha + \beta x_{ij}+...+\beta_kx_{ij} + b_i\]

\[\pi_i = \frac{e^{\eta}_i}{1+e^{\eta}_i}\]

\[y_{ij} \approx Binom(1, \pi_i)\] (1)

where $x_{ij}$, predictors, are the ith rows of the known design matrices x, and $\beta$ is a vector of regression parameters. This Bayesian approach allowed for adjustment of uncertainty associated with each parameter on the final outcome, lidar use. In order to do this, each parameter had to be assigned a prior belief of that parameter value. The values for these parameters are fit by sampling from these distributions to maximize the likelihood under this model [@kwonClimateInformedFlood2008]. The regression parameters, $\beta$, are normally distributed, \[\beta_k\approx N(\eta_{\beta k}, \sigma_k)\]. Additionally, the parameters of this distribution, $\eta_{\beta k}$ and $\sigma_k$, also have prior distributions assigned to them that are constrained by 0 and a positive value.

We used four Monte Carol Markov Chains (MCMC) with 2,000 iterations for warmup and an additional 2,000 iterations for the model. Additionally, the convergence was checked by visually inspecting the MCMC trace plots of the model parameters. We assessed effective sample size and checked model convergence, indicated by R-hat statistics close to 1 and stable, well-mixed chains [@gelmanBayesianWorkflow2020]. In addition, this methodology allowed for propagation of uncertainty throughout the model.

#### **Priors**

We used a weakly informative prior distribution to provide modest regularization, reduce the chance of a Type I error, and improve the out-of-sample prediction for regression models (McElreath 2015). Gelman et al. (2008) suggests the use of a Cauchy distribution with center 0 and scale 2.5 for logistic regression models [@gelmanWeaklyInformativeDefault2008]. This study uses a Cauchy distribution as recommended for models with a low sample size [@lemoineMovingNoninformativePriors2019]. 

#### **Validation**

We assessed the overall model performance through Leave-One-Out Cross-Validation (LOOCV). This process provides an absolute metric for the model's predictive ability. Lastly, because this model had categorical predictors, we plotted the predicted probability against the observed proportion for some binning of the data using counterfactual plots (Levy, 2012).

#### **Error**

We specified our model to compute 2,000 lidar use predictions based on our predictors. We interpreted the mean of these results as the projected lidar use. We used Bayesian R-squared to measure our overall model accuracy. However, this can be unreliable for small sample sizes, so we also calculated the mean absolute error (MAE) of our model. All Pareto k estimates are good (k < 0.5). 


# RESULTS

We received the greatest number of responses from Idaho (Table \@ref(tab:survey-dem-table). Washington had the highest percentage of female respondents, second highest percentage of respondents with a Bachelor's degree or higher, and longest average length of flood risk manager experience. The results show slight differences in demographic factors summarized in Table \@ref(tab:survey-dem-table). We were unable to confirm if our survey sample demographics match those of the population in each state.


``` {r, survey demographics, include=FALSE}
id.sample.size <- count(idaho$location=="Idaho")
or.sample.size <- count(oregon$location=="Oregon")
wa.sample.size <- count(washington$location=="Washington")

count(idaho$age)
count(oregon$age)
count(washington$age)
# age breakdown
id.age.2 <- round((2/96)*100, digits=0) # the denominator is the sample size
id.age.3 <- round((17/96)*100, digits=0) # the numerator is from 
id.age.4 <- round((27/96)*100, digits=0)
id.age.5 <- round((48/96)*100, digits=0)

or.age.2 <- round((2/58)*100, digits=0)
or.age.3 <- round((9/58)*100, digits=0)
or.age.4 <- round((17/58)*100, digits=0)
or.age.5 <- round((25/58)*100, digits=0) # some people didn't respond

wa.age.2 <- round((3/54)*100, digits=0)
wa.age.3 <- round((10/54)*100, digits=0)
wa.age.4 <- round((17/54)*100, digits=0)
wa.age.5 <- round((23/54)*100, digits=0)

# gender breakdown
id.gender.male <- round((59/96)*100, digits=0)
id.gender.female <- round((37/96)*100, digits=0)

or.gender.male <- round((35/58)*100, digits=0)
or.gender.female <- round((20/58)*100, digits=0)

wa.gender.male <- round((30/54)*100, digits=0)
wa.gender.female <- round((24/54)*100, digits=0)

#education breakdown
id.edu.ba.ma <- round((37+29)/96*100, digits=0) # represents the number of respondents with either a bachelors or advanced degree
or.edu.ba.ma <- round((21+26)/58*100, digits=0) 
wa.edu.ba.ma <- round((17+24)/54*100, digits=0)

#years in the industry
idaho$years <- as.numeric(idaho$years)
id.years <- round(summarise(idaho, avg=mean(years)), digits=1)

oregon$years <- as.numeric(oregon$years)
or.years <- round(summarise(oregon, avg=mean(years, na.rm=TRUE)), digits=1) ### THIS ISNT WORKING

washington$years <- as.numeric(washington$years)
wa.years <- round(summarise(washington, avg=mean(years)), digits=1)
```

```{r, survey-dem-table, echo=FALSE, fig.cap="\\label{demographics}"}
survey.dem <- matrix(c(96, 58, 54, "39%", "34%", "44%", "69%","81%", "76%", "50% ", "43%", "43%", id.years, or.years, wa.years), ncol=3, byrow=TRUE)
colnames(survey.dem) <- c("Idaho", "Oregon", "Washington")
rownames(survey.dem) <- c("Sample Size", "Female", "University Education", "Age (50+ years)", "Average Flood Risk Experience (years)")
knitr::kable(survey.dem, caption="Comparative descriptive statistics for survey demographics across Idaho, Oregon, and Washington.")
```

```{r, data sim, include=FALSE}
## STEP 1: Data Simulation ##
set.seed(124) 
N=180 # number of survey respondents # played around with n=720 and that made results significantly better
K=8 # number of predictors

## 1) Set the intercept ##

intercept=0 ## mean value of lidar use when all predictors are equal to 0

# for random effects, we could look at location

## 2) Set the predictor variables ##
## set simulate experience with binomial for each type of experience
experience_1 <- rbinom(N, 1, .5) # damage to property in community # yes (1) no (0)
future_1 <- sample(1:5, N, replace=TRUE) # damage to property in community in the future 1=0% to 5=100%
science_trust <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
incr_sev_flood<- sample(1:3, N, replace=TRUE) # no (1) stay the same (2) yes (3)
soep <-  sample(0:10, N, replace=TRUE) # range of risk preference from 0 to 10, where 0 = I generally prefer to take risks to 10 = I generally prefer to avoid risks
alter_lidar_prop <- runif(N, 0, 1) # this simulates the a range of potential proportion of lidar users in alters
alter_exp_mean <- runif(N, 0, 10) # this will rep the mean expertise of alters 
alter_comm_mean <- runif(N, 1, 6) # reps the mean communication of alters with respondent

#location <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

# Make data frame with raw data
raw.data <- data.frame(experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)

raw.data <- tibble::rowid_to_column(raw.data, "ID")

## Now let's set the effect size ##
b_experience_1 <- .2
b_future_1 <- -.6
b_incr_sev_flood <- 0.6
b_science_trust <- 0.1
b_soep <-  0
b_alter_lidar_prop <- -1 
b_alter_comm_mean <- 0.1
b_alter_exp_mean <- 0.5


# 4) Set the response variable (aka deterministic part) ##
p <- intercept + experience_1*b_experience_1 + future_1*b_future_1 + incr_sev_flood*b_incr_sev_flood  + science_trust*b_science_trust + 
  soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_comm_mean*b_alter_comm_mean + alter_exp_mean*b_alter_exp_mean 

pr <- plogis(p) # convert from log odds to probability

lidaruse <- rbinom(N,1,pr)

## 5) Combine data into dataframe ##

sim.data <- data.frame(lidaruse, experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)
write.csv(sim.data, "sim_data.csv")

#### Descriptive Stasim.data ####
# Check for correlations
cor(sim.data) # nothing > |.5| so we are good!

# plot the raw data & check for outliers
boxplot(sim.data[-24])

#### Check out the model & see if it returns out effects
model <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + future_1 + science_trust + soep + alter_lidar_prop + alter_comm_mean + alter_exp_mean, 
                  prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                  prior_intercept = cauchy(0,10), 
                  data=sim.data, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 


plot(model)
coef(model)

## experience is not being recovered very well... or science trust and alter lidar prop
```

```{r, recode, include=FALSE}
survey.responses$lidaruse <- revalue(survey.responses$lidaruse, c("1"="1", "2"="0", "3"="0"))
idaho$lidaruse <- revalue(idaho$lidaruse, c("1"="1", "2"="0", "3"="0"))
oregon$lidaruse <- revalue(oregon$lidaruse, c("1"="1", "2"="0", "3"="0"))
washington$lidaruse <- revalue(washington$lidaruse, c("1"="1", "2"="0", "3"="0"))

# yes=1, no=0

# Direct Experiences
survey.responses$experience_1 <- revalue(survey.responses$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
survey.responses$experience_2 <- revalue(survey.responses$experience_2, c("1"="1", "2"="0"))
survey.responses$experience_3 <- revalue(survey.responses$experience_3, c("1"="1", "2"="0"))
survey.responses$experience_4 <- revalue(survey.responses$experience_4, c("1"="1", "2"="0"))
survey.responses$experience_5 <- revalue(survey.responses$experience_5, c("1"="1", "2"="0"))

idaho$experience_1 <- revalue(idaho$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
idaho$experience_2 <- revalue(idaho$experience_2, c("1"="1", "2"="0"))
idaho$experience_3 <- revalue(idaho$experience_3, c("1"="1", "2"="0"))
idaho$experience_4 <- revalue(idaho$experience_4, c("1"="1", "2"="0"))
idaho$experience_5 <- revalue(idaho$experience_5, c("1"="1", "2"="0"))

washington$experience_1 <- revalue(washington$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
washington$experience_2 <- revalue(washington$experience_2, c("1"="1", "2"="0"))
washington$experience_3 <- revalue(washington$experience_3, c("1"="1", "2"="0"))
washington$experience_4 <- revalue(washington$experience_4, c("1"="1", "2"="0"))
washington$experience_5 <- revalue(washington$experience_5, c("1"="1", "2"="0"))

oregon$experience_1 <- revalue(oregon$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
oregon$experience_2 <- revalue(oregon$experience_2, c("1"="1", "2"="0"))
oregon$experience_3 <- revalue(oregon$experience_3, c("1"="1", "2"="0"))
oregon$experience_4 <- revalue(oregon$experience_4, c("1"="1", "2"="0"))
oregon$experience_5 <- revalue(oregon$experience_5, c("1"="1", "2"="0"))


# closer the experience, higher the number for yes

# Trust-- reverse code
survey.responses$gov_trust <- revalue(survey.responses$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$gov_trust <- revalue(idaho$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$gov_trust <- revalue(oregon$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$gov_trust <- revalue(washington$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

survey.responses$science_trust <- revalue(survey.responses$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$science_trust <- revalue(idaho$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$science_trust <- revalue(oregon$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$science_trust <- revalue(washington$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

# Gov Involvement
survey.responses$gov_involve <- revalue(survey.responses$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
idaho$gov_involve <- revalue(idaho$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
oregon$gov_involve <- revalue(oregon$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
washington$gov_involve <- revalue(washington$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))

# 5=completely involved
# 4=mostly involved
# 3=moderately involved
# 2=somewhat involved
# 1=not at all involved

# Risk Perception
# future flood risk
survey.responses$future_1<- revalue(survey.responses$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_2<- revalue(survey.responses$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_3<- revalue(survey.responses$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_4<- revalue(survey.responses$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_5<- revalue(survey.responses$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

idaho$future_1<- revalue(idaho$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_2<- revalue(idaho$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_3<- revalue(idaho$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_4<- revalue(idaho$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_5<- revalue(idaho$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

oregon$future_1<- revalue(oregon$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_2<- revalue(oregon$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_3<- revalue(oregon$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_4<- revalue(oregon$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_5<- revalue(oregon$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

washington$future_1<- revalue(washington$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_2<- revalue(washington$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_3<- revalue(washington$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_4<- revalue(washington$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_5<- revalue(washington$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
# 100% chance of happening
# 75% chance of happening
# 50% chance of happening
# 25% chance of happening
# 0% chance of happening
# 1-5 to represent closeness

#increase number of floods
survey.responses$incr_no_flood <- revalue(survey.responses$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_no_flood <- revalue(idaho$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_no_flood <- revalue(oregon$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_no_flood <- revalue(washington$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3# decrease=1# stay the same=2

#increase severity of floods
survey.responses$incr_sev_flood <- revalue(survey.responses$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_sev_flood <- revalue(idaho$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_sev_flood <- revalue(oregon$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_sev_flood <- revalue(washington$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3
# decrease=1
# stay the same=2

# Demographics
# age is ordered correctly: 1= less than 20, 2=20-29 years, 3=30-39 years, 4=40-49 years, 5=50+ years
# education is ordered correctly: 1= some high school, 2= high school diploma, 3=college edu, no grad, 4= associates, 5=bachelors, 6= advanced
# gender is ordered fine: 1= male, 2=female

# Structural Barriers
# too expensive
survey.responses$barrier_1<- revalue(survey.responses$barrier_1, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

#lack of expertise
survey.responses$barrier_2<- revalue(survey.responses$barrier_2, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# sparse population
survey.responses$barrier_3<- revalue(survey.responses$barrier_3, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low rate of economic development
survey.responses$barrier_4<- revalue(survey.responses$barrier_4, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low flooding risk
survey.responses$barrier_5<- revalue(survey.responses$barrier_5, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# lack of political support
survey.responses$barrier_6<- revalue(survey.responses$barrier_6, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# add a column for barrier 7 which is if lidar is present or not

# SOEP
survey.responses$soep_1<- revalue(survey.responses$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
idaho$soep_1<- revalue(idaho$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
oregon$soep_1<- revalue(oregon$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
washington$soep_1<- revalue(washington$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))

# 10: risk loving
# 0: risk averse

#Gender all good, except "3" for other
survey.responses$gender <- revalue(survey.responses$gender, c("1"="1", "2"="2", "3"="NA"))
idaho$gender <- revalue(idaho$gender, c("1"="1", "2"="2", "3"="NA"))
oregon$gender <- revalue(oregon$gender, c("1"="1", "2"="2", "3"="NA"))
washington$gender <- revalue(washington$gender, c("1"="1", "2"="2", "3"="NA"))

#age doesn't need recoding

# Edcuation
survey.responses$education <- revalue(survey.responses$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
idaho$education <- revalue(idaho$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
oregon$education <- revalue(oregon$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
washington$education <- revalue(washington$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
# 1: some high school
# 6: advanced degree

# prepared 
survey.responses$prepared <- revalue(survey.responses$prepared, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all prepared
#5: completely prepared

#usefulness
survey.responses$usefulid <- revalue(survey.responses$usefulid, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all useful
#5: very useful

#flood zone
survey.responses$flood_zone <- revalue(survey.responses$flood_zone, c("1"="1", "2"="0"))
#1: flooding outside of flood zone
#2: no flooding outside


# NETWORK PROXIES
# amount of commmunication 
survey.responses$comm_1 <- revalue(survey.responses$comm_1, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_2 <- revalue(survey.responses$comm_2, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_3 <- revalue(survey.responses$comm_3, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_4 <- revalue(survey.responses$comm_4, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_5 <- revalue(survey.responses$comm_5, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_6 <- revalue(survey.responses$comm_6, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_7 <- revalue(survey.responses$comm_7, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_8 <- revalue(survey.responses$comm_8, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
#1: a few times a year
#2: once a month
#3: 2-3 times a month
#4: once a week
#5: several times a week
#6: several times a day

# alter lidar use
survey.responses$lidar_1 <- revalue(survey.responses$lidar_1, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_2 <- revalue(survey.responses$lidar_2, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_3 <- revalue(survey.responses$lidar_3, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_4 <- revalue(survey.responses$lidar_4, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_5 <- revalue(survey.responses$lidar_5, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_6 <- revalue(survey.responses$lidar_6, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_7 <- revalue(survey.responses$lidar_7, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_8 <- revalue(survey.responses$lidar_8, c("1"="1", "2"="2", "3"="3"))

#1: yes
#2: no
#3: i don't know
 
#expertise
# no need to revalue
#0: no expertise at all
#10: lots of expertise 
```
```{r, numeric conversion, include=FALSE}
survey.responses <- survey.responses %>%
  mutate_at(vars('years','lidaruse', 'interestlid','LocationLatitude', 'LocationLongitude', 'years', 'experience_1', 'experience_2', 'experience_3', 
                 'experience_4', 'experience_5', 'future_1', 'future_2', 'future_3', 'future_4', 'future_5', 
                 'currentmap', 'flood_zone', 'prepared', 'incr_no_flood', 'incr_sev_flood', 'barrier_1', 'barrier_2', 
                 'barrier_3', 'barrier_4', 'barrier_5', 'barrier_6', 'soep_1', 'age', 'gender', 
                 'gov_trust', 'gov_involve', 'education', 'science_trust', 'toolslid', 'toolslid1', 'usefulid', 'no_alters',
                 'alter_names_1_TEXT', 'alter_names_2_TEXT', 'alter_names_3_TEXT', 'alter_names_4_TEXT', 'alter_names_5_TEXT', 'alter_names_6_TEXT',
                 'alter_names_7_TEXT', 'alter_names_9_TEXT', 'comm_1', 'lidar_1', 'expertise_1_1', 'comm_2', 'lidar_2', 'expertise_2_1', 'comm_3', 'lidar_3', 'expertise_3_1',
                'comm_4', 'lidar_4', 'expertise_4_1','comm_5', 'lidar_5', 'expertise_5_1', 'comm_6', 'lidar_6', 'expertise_6_1', 'comm_7', 'lidar_7','expertise_7_1',
                'comm_8', 'lidar_8', 'expertise_8_1'), as.numeric) 
```

```{r, network proxies, include=FALSE}
### Network Proxies

# proportion of alters that use lidar
network.variables <- c("Email", "no_alters","comm_1","lidar_1","expertise_1_1","comm_2","lidar_2","expertise_2_1","comm_3","lidar_3","expertise_3_1",
                "comm_4","lidar_4","expertise_4_1","comm_5","lidar_5","expertise_5_1","comm_6","lidar_6","expertise_6_1","comm_7","lidar_7","expertise_7_1",
                "comm_8","lidar_8","expertise_8_1")

network.subset <- survey.responses[network.variables] %>%
                  drop_na(no_alters) %>%
                  mutate_at(vars('lidar_1', 'lidar_2','lidar_3', 'lidar_4', 'lidar_5','lidar_6', 'lidar_7', 'lidar_8'), as.factor)

### expertise and comm calculations

# alter uses lidar, calc their expertise and communication
alter.1.subset <- network.subset %>%
  subset(.,c(lidar_1)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_1|comm_1|expertise_1_1"))

alter.1.subset$lidar_expert_1 <- alter.1.subset$expertise_1_1
alter.1.subset$lidar_comm_1 <- alter.1.subset$comm_1

# alter doesn't use lidar, calculate their expertise and communication
alter.1.subset.no <- network.subset %>%
  subset(.,!c(lidar_1)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_1|comm_1|expertise_1_1"))

alter.1.subset.no$lidar_expert_1_no <- alter.1.subset.no$expertise_1_1
alter.1.subset.no$lidar_comm_1_no <- alter.1.subset.no$comm_1

# combine new data columns into a new data frame 
full.alter.1 <- join(alter.1.subset.no, alter.1.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_1|lidar_comm_1|lidar_comm_1_no|lidar_expert_1|lidar_expert_1_no"))
                
### now repeat this for alllllll the alters
# alter 2
alter.2.subset <- network.subset %>%
  subset(.,c(lidar_2)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_2|comm_2|expertise_2_1"))

alter.2.subset$lidar_expert_2 <- alter.2.subset$expertise_2_1
alter.2.subset$lidar_comm_2 <- alter.2.subset$comm_2

# alter doesn't use lidar, calculate their expertise and communication
alter.2.subset.no <- network.subset %>%
  subset(.,!c(lidar_2)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_2|comm_2|expertise_2_1"))

alter.2.subset.no$lidar_expert_2_no <- alter.2.subset.no$expertise_2_1
alter.2.subset.no$lidar_comm_2_no <- alter.2.subset.no$comm_2

# combine new data columns into a new data frame 
full.alter.2 <- join(alter.2.subset.no, alter.2.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_2|lidar_comm_2|lidar_comm_2_no|lidar_expert_2|lidar_expert_2_no"))

# alter 3
alter.3.subset <- network.subset %>%
  subset(.,c(lidar_3)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_3|comm_3|expertise_3_1"))

alter.3.subset$lidar_expert_3 <- alter.3.subset$expertise_3_1
alter.3.subset$lidar_comm_3 <- alter.3.subset$comm_3

# alter doesn't use lidar, calculate their expertise and communication
alter.3.subset.no <- network.subset %>%
  subset(.,!c(lidar_3)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_3|comm_3|expertise_3_1"))

alter.3.subset.no$lidar_expert_3_no <- alter.3.subset.no$expertise_3_1
alter.3.subset.no$lidar_comm_3_no <- alter.3.subset.no$comm_3

# combine new data columns into a new data frame 
full.alter.3 <- join(alter.3.subset.no, alter.3.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_3|lidar_comm_3|lidar_comm_3_no|lidar_expert_3|lidar_expert_3_no"))

# alter 4
alter.4.subset <- network.subset %>%
  subset(.,c(lidar_4)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_4|comm_4|expertise_4_1"))

alter.4.subset$lidar_expert_4 <- alter.4.subset$expertise_4_1
alter.4.subset$lidar_comm_4 <- alter.4.subset$comm_4

# alter doesn't use lidar, calculate their expertise and communication
alter.4.subset.no <- network.subset %>%
  subset(.,!c(lidar_4)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_4|comm_4|expertise_4_1"))

alter.4.subset.no$lidar_expert_4_no <- alter.4.subset.no$expertise_4_1
alter.4.subset.no$lidar_comm_4_no <- alter.4.subset.no$comm_4

# combine new data columns into a new data frame 
full.alter.4 <- join(alter.4.subset.no, alter.4.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_4|lidar_comm_4|lidar_comm_4_no|lidar_expert_4|lidar_expert_4_no"))

# alter 5
alter.5.subset <- network.subset %>%
  subset(.,c(lidar_5)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_5|comm_5|expertise_5_1"))

alter.5.subset$lidar_expert_5 <- alter.5.subset$expertise_5_1
alter.5.subset$lidar_comm_5 <- alter.5.subset$comm_5

# alter doesn't use lidar, calculate their expertise and communication
alter.5.subset.no <- network.subset %>%
  subset(.,!c(lidar_5)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_5|comm_5|expertise_5_1"))

alter.5.subset.no$lidar_expert_5_no <- alter.5.subset.no$expertise_5_1
alter.5.subset.no$lidar_comm_5_no <- alter.5.subset.no$comm_5

# combine new data columns into a new data frame 
full.alter.5 <- join(alter.5.subset.no, alter.5.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_5|lidar_comm_5|lidar_comm_5_no|lidar_expert_5|lidar_expert_5_no"))

# alter 6
alter.6.subset <- network.subset %>%
  subset(.,c(lidar_6)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_6|comm_6|expertise_6_1"))

alter.6.subset$lidar_expert_6 <- alter.6.subset$expertise_6_1
alter.6.subset$lidar_comm_6 <- alter.6.subset$comm_6

# alter doesn't use lidar, calculate their expertise and communication
alter.6.subset.no <- network.subset %>%
  subset(.,!c(lidar_6)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_6|comm_6|expertise_6_1"))

alter.6.subset.no$lidar_expert_6_no <- alter.6.subset.no$expertise_6_1
alter.6.subset.no$lidar_comm_6_no <- alter.6.subset.no$comm_6

# combine new data columns into a new data frame 
full.alter.6 <- join(alter.6.subset.no, alter.6.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_6|lidar_comm_6|lidar_comm_6_no|lidar_expert_6|lidar_expert_6_no"))

# alter 7
alter.7.subset <- network.subset %>%
  subset(.,c(lidar_7)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_7|comm_7|expertise_7_1"))

alter.7.subset$lidar_expert_7 <- alter.7.subset$expertise_7_1
alter.7.subset$lidar_comm_7 <- alter.7.subset$comm_7

# alter doesn't use lidar, calculate their expertise and communication
alter.7.subset.no <- network.subset %>%
  subset(.,!c(lidar_7)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_7|comm_7|expertise_7_1"))

alter.7.subset.no$lidar_expert_7_no <- alter.7.subset.no$expertise_7_1
alter.7.subset.no$lidar_comm_7_no <- alter.7.subset.no$comm_7

# combine new data columns into a new data frame 
full.alter.7 <- join(alter.7.subset.no, alter.7.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_7|lidar_comm_7|lidar_comm_7_no|lidar_expert_7|lidar_expert_7_no"))

# alter 8
alter.8.subset <- network.subset %>%
  subset(.,c(lidar_8)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_8|comm_8|expertise_8_1"))

alter.8.subset$lidar_expert_8 <- alter.8.subset$expertise_8_1
alter.8.subset$lidar_comm_8 <- alter.8.subset$comm_8

# alter doesn't use lidar, calculate their expertise and communication
alter.8.subset.no <- network.subset %>%
  subset(.,!c(lidar_8)=="1") %>% # keep only lidar users
  dplyr::select(matches("Email|lidar_8|comm_8|expertise_8_1"))

alter.8.subset.no$lidar_expert_8_no <- alter.8.subset.no$expertise_8_1
alter.8.subset.no$lidar_comm_8_no <- alter.8.subset.no$comm_8

# combine new data columns into a new data frame 
full.alter.8 <- join(alter.8.subset.no, alter.8.subset, by="Email", type="full") %>%
                dplyr::select(matches("Email|lidar_8|lidar_comm_8|lidar_comm_8_no|lidar_expert_8|lidar_expert_8_no"))



## now bring all the seperate alter data frames into the main one

network.lidar.nolidar.subset <- list(full.alter.1, full.alter.2, full.alter.3, full.alter.4, full.alter.5, full.alter.6, full.alter.7, full.alter.8 ) %>%
  reduce(full_join, by = "Email")


## let's calculate total expertise of alters that use lidar for each respondent
expert.cols <- c("lidar_expert_1", "lidar_expert_2", "lidar_expert_3", "lidar_expert_4", "lidar_expert_5", "lidar_expert_6", "lidar_expert_7", "lidar_expert_8")

network.lidar.nolidar.subset$total.alter.lidar.expertise <- rowSums(network.lidar.nolidar.subset[,expert.cols], na.rm = TRUE) 
  
## now calculate total expertise of alter that don't use lidar for each respondent
expert.no.cols <- c("lidar_expert_1_no", "lidar_expert_2_no", "lidar_expert_3_no", "lidar_expert_4_no", "lidar_expert_5_no", "lidar_expert_6_no", "lidar_expert_7_no", "lidar_expert_8_no")

network.lidar.nolidar.subset$total.alter.nolidar.expertise <- rowSums(network.lidar.nolidar.subset[,expert.no.cols], na.rm = TRUE) 

## now subtract lidar expertise - no lidar expertise to get the final "net" expertise value for our model

network.lidar.nolidar.subset$net_expertise <- (network.lidar.nolidar.subset$total.alter.lidar.expertise - network.lidar.nolidar.subset$total.alter.nolidar.expertise)


##### okay now let's do the same calculation for communication

## total comm for alters that use lidar for each respondent
comm.cols <- c("lidar_comm_1", "lidar_comm_2", "lidar_comm_3", "lidar_comm_4", "lidar_comm_5", "lidar_comm_6", "lidar_comm_7", "lidar_comm_8")

network.lidar.nolidar.subset$total.alter.lidar.comm<- rowSums(network.lidar.nolidar.subset[,comm.cols], na.rm = TRUE) 

## total comm for alters that do not use lidar for each respondent
comm.no.cols <- c("lidar_comm_1_no", "lidar_comm_2_no", "lidar_comm_3_no", "lidar_comm_4_no", "lidar_comm_5_no", "lidar_comm_6_no", "lidar_comm_7_no", "lidar_comm_8_no")

network.lidar.nolidar.subset$total.alter.nolidar.comm<- rowSums(network.lidar.nolidar.subset[,comm.no.cols], na.rm = TRUE) 


## now subtract lidar comm - no lidar comm to get the final "net" comm value for our model

network.lidar.nolidar.subset$net_comm <- (network.lidar.nolidar.subset$total.alter.lidar.comm - network.lidar.nolidar.subset$total.alter.nolidar.comm)


## now rejoin this information with survey.responses

survey.responses <- join(survey.responses, network.lidar.nolidar.subset[, c("Email", "net_expertise", "net_comm")], by= "Email", type="left")

```

```{r, model specific data, include=FALSE}
# Dataset for the model
model.variables <- c("lidaruse", "experience_1", "future_1", "incr_sev_flood",  "soep_1", "science_trust","location","prop_lidar_network", "net_comm", "net_expertise")

model.data <- survey.responses[model.variables]

str(model.data)

```

```{r, na omit, include=FALSE}
model.data.na.omit <- na.omit(model.data)
n = nrow(model.data.na.omit)
n # 145
```

```{r, lidar use stats, include=FALSE}
count(idaho$lidaruse==1)
id.lidaruse <- round((48/96)*100, digits=1)

count(oregon$lidaruse==1)
or.lidaruse <- round((36/58)*100, digits=1)

count(washington$lidaruse==1)
wa.lidaruse <- round((35/54)*100, digits=1)

```

## *Descriptive Results*

Table \@ref(tab:survey-variable-table) summarizes survey respondent's responses to individual and collective predictors of interest. These results highlight that over 70% of flood risk managers, in all three states, had direct experience with flood damage in their communities. Figure \@ref(exp-barchart) describes, in finer detail, the types of experiences flood risk managers have had with flooding. 

```{r, exp-barchart, echo=FALSE}

# Local floodplain information-- direct experience
count(survey.responses, "experience_1")
count(survey.responses, "experience_2")
count(survey.responses, "experience_3") 
count(survey.responses, "experience_4")
count(survey.responses, "experience_5") 

tidy.exp <- data.frame("exp"=c(1,1,2,2,3,3,4,4,5,5), 
                            "category"=c(0,1,0,1,0,1,0,1,0,1),
                            "count"=c(44,162,170,34,194,11,203,0,152,51))
tidy.exp$exp <- as.character(tidy.exp$exp)
tidy.exp$category <- as.character(tidy.exp$category)
str(tidy.exp)

ggplot(tidy.exp, aes(fill=category, y=count, x=exp)) +
  geom_bar(position="fill", stat="identity", width=0.3) + 
  #geom_text(aes(label=..count..),stat="count",position=position_fill(vjust=0.5)) +
  scale_x_discrete(limits=c("1", "5", "2", "3", "4"),
                   labels=c( "damage to property in your community?","disruption to your electric, water, \n phone, and other basic services?","deaths and injury to people in your community?", "damage to your home?","deaths or injuries to you or members \n of your immediate family?"))+
  #scale_y_discrete(breaks=c("0", "0.25", "0.50", "0.75", "2"),
                   #labels=c( "0%", "25%", "50%", "75%", "100%"))+
  scale_fill_manual(values=c("darkred", "gray75"), 
                    limits=c("1", "0"),
                    labels=c("Yes", "No"))+
  coord_flip() +
  labs(fill="")+
  xlab("")+
  ylab("Percentage of Responses")+
  ggtitle("Have you ever experienced a flood that caused...") +
  theme(text = element_text(size=20),
        axis.text.x = element_text(hjust=1))

```

Despite the majority of flood risk managers experiencing floods in the past, only 28% - 48% of respondents reported that they expect flood damage in their communities in the future.

```{r, future-barchart, echo=FALSE}
## future experience
count(survey.responses, "future_1") 
count(survey.responses, "future_2")
count(survey.responses, "future_3") 
count(survey.responses, "future_4")
count(survey.responses, "future_5") 

tidy.future <- data.frame("future"=c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5), 
                          "category"=c(1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5),
                           "count"=c(3,46,40,39,76,55,91,27,13,14,136,47,11,5,3,141,53,3,1,3,39,97,35,20,12))
tidy.future$future <- as.character(tidy.future$future)
tidy.future$category <- as.character(tidy.future$category)
str(tidy.future)

ggplot(tidy.future, aes(fill=category, y=count, x=future)) +
  geom_bar(position="fill", stat="identity", width=0.3) + 
  scale_x_discrete(limits=c("1", "5", "2", "3", "4"),
                   labels=c( "damage to property in your community?","disruption to your electric, water, \n phone, and other basic services?","deaths and injury to people in your community?", "damage to your home?","deaths or injuries to you or members \n of your immediate family?"))+
  scale_fill_manual(guide=guide_legend(reverse=TRUE),
                    values=c("gray75", "bisque2", "chocolate", "brown", "darkred"), 
                    limits=c("1", "2", "3", "4", "5"),
                    labels=c("0%", "25%", "50%", "75%", "100%"))+
  coord_flip() +
  labs(fill="")+
  xlab("")+
  ylab("Percentage of Responses")+
  ggtitle("Thinking about your community in the next 30 years, how likely is it that a flood will cause...")
  theme(text = element_text(size=20),
        axis.text.x = element_text(hjust=1))

```

In addition, between 38.% - 57.4% of respondent's reported they believe there will be an increase in flood severity. This results illuminate a discrepancy between experienced flooding and perceived flooding risk. This discrepancy could indicate that despite knowledge of increased flood risk, manager's feel as if their communities are adapting or have adapted to handle an increase in flood severity. 

We found that flood risk managers in Washington tend to be less risk-averse than managers in Oregon and Idaho. In addition, all three states reported a high trust in the accuracy of flood risk management scientific products (e.g. topographic data, floodplain mapping, floodplain modeling) with Washington reporting the highest percentage of trust. 

``` {r, survey-variable-table, echo=FALSE, fig.cap="\\label{survey variables}"}

#Direct experience with flood damage in community
id.experience <- round((table(idaho$experience_1 == 1)/length(idaho$experience_1))*100, digits=1)
id.experience <- 79.2
or.experience <- round((table(oregon$experience_1 == 1)/length(oregon$experience_1))*100, digits=1)
or.experience <- 72.4
wa.experience <- round((table(washington$experience_1 == 1)/length(washington$experience_1))*100, digits=1) 
wa.experience <- 85.2
# 1=yes

# 25% chance of direct experience with future flood damage in the community
count(idaho$future_1) # 94/96
id.future <- 97.9
count(oregon$future_1) #56/58
or.future <- 96.6
count(washington$future_1) #53/54
wa.future <- 98.1
# includes those who selected 50, 75, and 100% chance of event happening

#Perceived Increase in Flood Severity
id.incr.sev.flood <- (table(idaho$incr_sev_flood == 3)/length(idaho$incr_sev_flood))*100
id.incr.sev.flood <- 38.5
or.incr.sev.flood <- (table(oregon$incr_sev_flood == 3)/length(oregon$incr_sev_flood))*100
or.incr.sev.flood <- 41.4
wa.incr.sev.flood <- (table(washington$incr_sev_flood == 3)/length(washington$incr_sev_flood))*100
wa.incr.sev.flood <- 57.4
# 3=increasing

#Risk-taking attitude
#idaho$soep_1 <- as.numeric(idaho$soep_1)
#id.soep <- round(summarise(idaho, avg=mean(soep_1, na.rm=TRUE)), digits=1)
id.soep <- 2.8
#oregon$soep_1 <- as.numeric(oregon$soep_1)
#or.soep <- round(summarise(oregon, avg=mean(soep_1, na.rm=TRUE)), digits=1)
or.soep <- 3.3
#washington$soep_1 <- as.numeric(washington$soep_1)
#wa.soep <- round(summarise(washington, avg=mean(soep_1, na.rm=TRUE)), digits=1)
wa.soep <- 3.7
#0-risk averse to 10-risk loving

#Trust in accuracy of flood risk management scientific products
id.science.trust <- (table(idaho$science_trust > 3)/length(idaho$science_trust))*100
id.science.trust <- 82.3
or.science.trust <- (table(oregon$science_trust > 3)/length(oregon$science_trust))*100
or.science.trust <- 81.0
wa.science.trust <- (table(washington$science_trust > 3)/length(washington$science_trust))*100
wa.science.trust <- 90.7
#4-somewhat trust, 5-completely trust

#Proportion of lidar users in flood risk management network
idaho <- join(idaho, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#id.prop.lidar <- (round(summarise(idaho, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
id.prop.lidar <- 35.0
oregon <- join(oregon, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#or.prop.lidar <- (round(summarise(oregon, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
or.prop.lidar <- 40.0
washington <- join(washington, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#wa.prop.lidar <- (round(summarise(washington, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
wa.prop.lidar <- 42.0
#Total communication in flood risk management network
#id.comm.network <- (round(summarise(idaho, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
id.comm.network <- 3
#or.comm.network <- (round(summarise(oregon, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
or.comm.network <- 3
#wa.comm.network <- (round(summarise(washington, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
wa.comm.network <- 3

#Perceived expertise in flood risk management network
#id.expert.network <- (round(summarise(idaho, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
id.expert.network <- 6.6
#or.expert.network <- (round(summarise(oregon, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
or.expert.network <- 7.3
#wa.expert.network <- (round(summarise(washington, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
wa.expert.network <- 6.7

#Use of lidar for flood risk management
#id.lidaruse <- (table(idaho$lidaruse == 1)/length(idaho$lidaruse))*100
id.lidaruse <- 50.0
#or.lidaruse <- (table(oregon$lidaruse == 1)/length(oregon$lidaruse))*100
or.lidaruse <- 62.1
#wa.lidaruse <- (table(washington$lidaruse == 1)/length(washington$lidaruse))*100
wa.lidaruse <- 64.8

survey.variable.stats <- matrix(c(id.experience, or.experience, wa.experience, id.future, or.future, wa.future, id.incr.sev.flood, or.incr.sev.flood, wa.incr.sev.flood, id.soep, or.soep, wa.soep, id.science.trust, or.science.trust, wa.science.trust, id.prop.lidar, or.prop.lidar, wa.prop.lidar, id.comm.network, or.comm.network, wa.comm.network, id.expert.network, or.expert.network, wa.expert.network, id.lidaruse, or.lidaruse, wa.lidaruse), ncol=3, byrow=TRUE)
colnames(survey.variable.stats) <- c("Idaho", "Oregon", "Washington")
rownames(survey.variable.stats) <- c( "Direct experience with flood damage in community (%)", "Direct experience with future flood damage in the community (%)","Perceived Increase in Flood Severity (%)", "Average risk-taking attitude (0 to 10 with 10 being risk-loving)", "Trust in accuracy of flood risk management scientific products (%)", "Proportion of lidar users in flood risk management network (%)", "Average amount of communication with flood risk management network* ", "Perceived expertise in flood risk management network (0 to 10 with 10 being of highest expertise)", "Use lidar for flood risk management (%)" )
knitr::kable(survey.variable.stats, caption="Descriptive statistics of survey question responses by State.")
```

The collective predictors of lidar use varied slightly among the states. Respondents in Washington, on average, had 42% of the people a respondent reported sharing information with also individuals who also use lidar compared to 35% in Idaho. Similarly, Washington reported the highest amount of lidar use in flood risk management with almost 65% of respondent's using lidar. Idaho reported the lowest amount of lidar users, 50%. 

## *Estimation Results* 

Our GLR model allowed us to explore the effect of a multitude of predictors on lidar use in Idaho, Oregon, and Washington. We had item-nonresponse in the survey, in particular for the network section, and we dropped incomplete responses to conduct our statistical modeling. Of the 208 usable responses we received, 50 of them did not fill out the network section. Since our model considers both individual and collective predictors, and needs equal size data lengths for each predictor in order to run the model, we dropped almost 25% of our data responses, which may result in effect size underestimation [@langkampTechniquesHandlingMissing2010]. 

```{r, final model, include=FALSE}
# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + net_comm + net_expertise + location,
                           prior = cauchy(0, 2.5), # based on gillman recommendation
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

full.mod.nopriors <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + net_comm + net_expertise + location,
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 
mae(full.mod.priors, model.data.na.omit)
rmse(full.mod.priors, model.data.na.omit)

loo_compare(loo(full.mod.priors), loo(full.mod.nopriors)) # full model with prior specification is better
```

Table \@ref(tab:model-summary-table) displays the results from our GLR model that considers the effect of individual and collective predictors on lidar use.   

```{r, model-summary-table, echo=FALSE, fig.cap="\\label{summary_stats}"}
#these are based on summary(full.mod) which returned a slightly better result that fullmod.priors
summary.stats <- matrix(c(-2.6, 1.3, -4.9, -0.4,
                          0.3, 0.4, -0.3, 1.2,
                          0.4, 0.6, -0.4, 1.4,
                          0.2, 0.3, -0.2, 0.7, 
                          0.1, 0.1, -0.1, 0.2, 
                          -0.1, 0.2, -0.5, 0.3,
                          4.3, 0.8, 3.0, 5.7, 
                          0.0, 0.0, 0.0, 0.1, 
                          0.0, 0.0, 0.0, 0.0), ncol=4, byrow=TRUE) #this is the plogis values for mean and SD ### UPDATE THESE NUMBERS
colnames(summary.stats) <- c( "Mean (log odds)", "S.D.", "5%", "95%")
rownames(summary.stats) <- c("Intercept", "Direct Experience with Flood Damage in Community", "Future Experience with Flood Damage in Community", "Increase in Flood Severity", "Risk Attitude", "Trust in Accuracy of flood risk management Scientific Products", "Proportion of Lidar Users in Respondent's flood risk management Network", "Level Communication with Respondent and their flood risk management Network", "Total Expertise in Respondent's flood risk management Network")
knitr::kable(summary.stats, caption="Estimation results from the model")
```

```{r, posterior-distribution, echo=FALSE, fig.cap="Posterior Predictive Distribution for each predictor variable."}

library(bayesplot)

color_scheme_set("teal")

areas.plot <- mcmc_areas(full.mod.priors, pars = c("incr_sev_flood", "experience_1", "soep_1", "future_1", "science_trust", "prop_lidar_network", "comm_total", "expert_total"), 
                         prob=0.8,#80% intervals
                         prob_outer=0.99, #99%
                         point_est= "mean"
                         ) +
  scale_y_discrete(expand = c(0, 0)) +
  labs(x = "Posterior Predictive Distribution", y = "Predictors") +
  theme( axis.text=element_text(size=12), axis.title=element_text(size=14),plot.title = element_text(hjust = 0.5), strip.text = element_text(size=14))

ci.plot <- mcmc_intervals(full.mod.priors, pars = c("incr_sev_flood", "experience_1", "soep_1", "future_1", "science_trust", "prop_lidar_network", "comm_total", "expert_total")) +
  labs(x = "Parameter Estimate Confidence Interval") +
  theme( axis.text=element_text(size=12), axis.title=element_text(size=14),plot.title = element_text(hjust = 0.5), strip.text = element_text(size=14))

ggpubr::ggarrange(areas.plot, ci.plot)

```

Figure \@ref(fig:posterior-distribution) displays the Posterior Predictive Distribution for each predictor and the intercept. We considered predictors that had parameter estimates whose 90% credible interval did not overlap with zero to be important. These results suggest that the proportion of lidar users in a respondent's flood risk management network had a non-zero effect on lidar use.

```{r, model-plot, echo=FALSE, include=TRUE, fig.cap= "The effect of lidar proportion in respondent's network on lidar adoption, while holding all other variables at their minimum." }
### let's try simulating continuous variable

proplidar.sim <- seq(min(model.data.na.omit$prop_lidar_network), max(model.data.na.omit$prop_lidar_network), length.out=180)

location.sim <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

preds <- add_fitted_draws(newdata=data.frame(prop_lidar_network=proplidar.sim,
                                              soep_1=5,
                                              incr_sev_flood=2, 
                                              future_1=0,
                                              experience_1=0,
                                              science_trust=3,
                                              net_comm=0,
                                              net_expertise=0, 
                                              location=location.sim),
                          draws = 200,
                          full.mod.priors)

model_plot <- ggplot(preds, aes(x = prop_lidar_network, group=location, col=location)) +
  stat_lineribbon(aes(y = .value), .width = c(.95, .5)) +
  scale_fill_brewer(palette = "Greys") +
  labs(x = "Proportion of Lidar User's in Respondent's Network", y = "Probability of Lidar Adoption") + theme_classic() + 
  scale_colour_discrete(name = "Location") +
  theme( axis.text=element_text(size=12), axis.title=element_text(size=14), plot.title = element_text(hjust = 0.5), strip.text = element_text(size=14))

model_plot


jtools::effect_plot(full.mod.priors, pred=prop_lidar_network)

```

Figure \@ref(fig:model-plot) displays the effect, when holding all other variables at their minimum, of the proportion of lidar user's in a respondent's network on lidar use by region. When every alter in a respondent's network used lidar, there was over 87% probability of the respondent adopting lidar. This drastically decreases as the proportion of lidar user in respondent's network decreases, with less than 15% of respondents using lidar.  

```{r, future-exp plot, echo=FALSE, include=FALSE}
### let's try simulating continuous variable

future.sim <- rep(c(1:5), each=180)

location.sim <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

preds.future <- add_fitted_draws(newdata=data.frame(future_1=future.sim,
                                              soep_1=5,
                                              incr_sev_flood=2, 
                                              prop_lidar_network=0,
                                              experience_1=0,
                                              science_trust=3,
                                              comm_total=0,
                                              expert_total=0, 
                                              location=location.sim),
                          draws = 1000,
                          full.mod.priors)

ggplot(preds.future, aes(as.factor(future_1), .value)) + 
  geom_boxplot() + 
  labs(x = "Probabiility of Future Flood Experirence", y = "Probability of Lidar Adoption") +
  theme_bw() 
  

```

```{r, ppc check, echo=FALSE, fig.cap="Displays a histogram, using the PPC function, that represents the number of individuals that do not use lidar (0) in the left column and use lidar (1) in the right column. The y histogram represents the actual data and the yrep represents the data generated from the posterior predictive distribution. Overall, the yrep is representative of the y, meaning our model has an accurate predictive ability."}
ppc <- pp_check(full.mod.priors, plotfun="ppc_hist", nreps=3) +
  labs(x = "Lidar Use", y = "Number of Lidar Users") +
  theme_bw() 

ppc

#*look up bayesplot 
```

Furthermore, we examined the out-of-sample predictive performance of our model. The Loo Information Criterion was 161.4 with standard error of 17.5. The predictive power of the model was assessed by using a Posterior Predictive Checking from the bayesplot package in R. 

```{r, LOOCV, include=FALSE, eval=FALSE}

Result_G <- data.frame(matrix(nrow=nrow(model.data.na.omit), ncol=6))

#set the column names
colnames(Result_G) <- c("MedianG", "LowerBound1_G", "LowerBound2_G", "UpperBound1_G", "UpperBound2_G", "R2_Bayes")

#do the leave one out cross validation
# create for loop function
for(i in 1:length(model.data.na.omit$lidaruse)){ # for all the data points
 sub_dat <- model.data.na.omit[-i,] #remove each one at a time
 m_sub <-  stan_glm(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + net_comm + net_expert + location,
                           prior = cauchy(0, 2.5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit"), control = list(adapt_delta = 0.99))
  post=posterior_predict(m_sub, model.data.na.omit[i,], draws=4000)
  r2=r2_bayes(m_sub)# predict that point
  Result_G[i,1]=quantile(post, 0.5) # fill a df with the credibility intervals
  Result_G[i,2]=quantile(post, 0.25) 
  Result_G[i,3]=quantile(post, 0.025)
  Result_G[i,4]=quantile(post, 0.75)
  Result_G[i,5]=quantile(post,0.975)
  Result_G[i,6]=r2
}

write.csv(Result_G, "C:/Users/tarapozzi/Documents/lidar_manuscript/data")
```

The Mean Absolute Error of our model was 1.45 and Root Mean Square Error was 1.75 therefore, our model had minimal variance in individual errors in our sample. Lastly, the Bayesian R-squared value for our model was 0.43, which represents a moderate effect size in social science data [@fergusonEffectSizePrimer2009]. 


# DISCUSSION 

Our findings offer partial support for the trends we expected to uncover from our exploratory analysis. We expected to find a positive relationship between the various collective predictors and lidar use. As expected, respondents with a higher proportion of lidar users in their social network were indeed more likely to adopt lidar. This result is supported by existing technology adoption literature [e.g. @loRoleSocialNorms2013; @poussinFactorsInfluenceFlood2014; @viglioneInsightsSociohydrologyModelling2014]. However, the amount of communication and expertise of alters in our respondent's social network did not have a statistically significant impact on lidar adoption. This result is surprising and contrary to our expectations. We expected that individuals who communicated more frequently with other lidar users, and communicated with other lidar users with higher levels of expertise, would be more likely to use lidar themselves. Perhaps, our analysis did not capture the nuanced effect of communication with specific types of alters though and therefore washed out the relationship between communication frequency, expertise of others, and lidar use.

We consider these findings supportive of the idea that flood risk managers learn from peers. This supports the themes that we encountered during our interviews prior to the survey. One interview we conducted, with a floodplain manager from Idaho at a regional conference,  mentioned "I feel like we should do a lot more networking in the state of Idaho, but oftentimes I have to reach out to people in like Washington for help or like at the national level for help. And so that's why coming to these conferences is really helpful for me because I meet peers outside of just our immediate, that have similar programs." This is an intriguing point that highlights Washington as the source of lidar information for a flood risk manager in Idaho. This point is supported by another interviewee who stated "we're all in the same kind of communities, which is helpful sometimes, but it also is a little bit of a silo thing... we are all stuck in the same point of view." Social networks provide a key component of information dissemination. It is important this need is capitalized on when creating flood risk management programs. 

Additionally, we found mixed support for individual predictors of lidar adoption. Direct experience, risk perception, and knowledge had moderate effects on lidar adoption. Direct experience has been studied extensively in the past as a significant predictor of individual risk perception and behavior [@lechowskaWhatDeterminesFlood2018]. While our study found a moderate effect of direct experience, previous research has found variable effects of experience on behavior and suggest that measuring the intensity of the event experience as a more informative measure (CITE). As mentioned previously, risk perception has had mixed results as a predictor of risk mitigation behavior [e.g. @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012]. We included risk perception in our study as a measure of future risk of damage from a flood event in the community. Our results supported that respondents with an increased future risk perception, were more likely to adopt lidar. We expected this result because respondents are more likely to want to accurately know their risk if they see it as potentially destructive. In addition, we included a knowledge predictor which we expected to have a postive effect on lidar adoption that our results supported.

Interestingly risk attitude had minimal impact on lidar adoption. We used the German Socio-Economic Panel Study risk question (SOEP) to measure general risk-taking attitude. The SOEP methodology is a direct question about how a respondent ranks their risk-taking attitude overall. This method of self-reported answering represents a valid, low-cost substitute for incentivized lottery schemes [@crosettoTheoreticalExperimentalAppraisal2016]. Due to our concern of incentivized risk elicitation methods causing mental fatigue and unneeded complication to our survey instrument, we chose to go with the SOEP method. Our results were inconclusive on the effect of risk attitude on lidar adoption. This is perhaps because of the duality of risk respondents face when adopting a new technology for flood risk management. There is an inherent risk in adopting a technology that they may not know how to use, but a pay off in managing in flood risk. Whereas, there may be others who are more willing to take the risk of potential flooding in order to minimize the risk of adopting a new technology. This inconclusive finding, suggests that we need to look into risk salience further to understand the layering of factors (e.g. technological risk, societal risk) in decision-making. For example, social influence might reduce the risk of adopting a new technology; if a trusted peer uses lidar, lidar could feel less risky. On the other hand, direct experience with flooding might enhance a person's perceived environmental risk in a way that makes them overcome the risk of adopting a new technology.

In addition, research has explored the impact of trust on risk mitigation behavior [@cannonClimateChangeDouble2020]. Our results suggested there was a minimal effect of trust in science on lidar adoption. This could be because lidar is well-established and trusted within the flood risk management community generally. This result is consistent with the finding that many interviewees reported trusting the efficacy of lidar.

## *Implications* 

Based on our findings, we suggest a more target focus on increasing collaboration across flood risk managers communities within states and between states. The need for more established networks was found in both our interviews and survey analysis.  Federal, state, and local level authorities capitalize on the importance of peer influence, not only for lidar adoption, but for general information dissemination of effective flood risk mitigation behavior and sustained best practices for flood risk management. For example, providing targeted networking events for the lidar community to gather and communicate lidar.  

Ultimately, Washington had 1.3 times more lidar users than Idaho. In addition to our model findings, this variation could be due to the lidar acquisition and coordination program in Washington. The Washington Geological Survey was granted funding from 2015-2021 for the collection and distribution of lidar data and lidar-derived products. Established in the Department of Natural Resources, the funding came from the Washington State General Fund and also provided funding for two permanent lidar positions, a lidar manager and a lidar specialist. In addition, Washington has focused on disseminating interactive (e.g. [Washington Story Map](https://wadnr.maps.arcgis.com/apps/Cascade/index.html?appid=b93c17aa1ef24669b656dbaea009b5ce_)) information on lidar to educate the public and advocate for sustained lidar investment at the state-level. Oregon and Idaho also have established lidar acquisition and coordination efforts, however they do not have a permanently funded position to manage lidar. Resource availability offers one explanation for the variation of lidar usage. Our analysis found that there could be more nuanced factors at play that contribute to this variation as well. In summary, the lidar model in Washington, which includes two full time positions and state funding, is likely one of the reasons we see more lidar use in Washington. Following the model of Washington might promote increased use of lidar in the other states. This would require both policy and funding-level changes in Oregon and Idaho. 

## *Limitations*

Our study could improve causual inference by conduction a longitudinal study to see how lidar adoption changes over time especially with target barrier reduction and increased channels for peer influence and resource sharing. A full network analysis could also help with identifying key stakeholders in the flood risk management community for target information dissemination and risk mitigation behavior in the flood risk management community. This could pose benefits  Additionally, our study does not account for the impact of lidar use, we operate under the assumption that lidar is useful to flood risk managers. The USGS has broken down the benefit-cost ratio for each State to help state-level decision makers plan and manage lidar acquisition in their (USGS, 2021). It would be helpful to conduct a study that demonstrates lidar is a cost-effective tool for flood risk management. 


# CONCLUSION


```{r, univariate-plots}
a <- hist(model.data.na.omit$experience_1)
b <- hist(model.data.na.omit$future_1)
c <- hist(model.data.na.omit$soep_1)
d <- hist(model.data.na.omit$incr_sev_flood)
e <- hist(model.data.na.omit$science_trust)
f <- hist(model.data.na.omit$prop_lidar_network)
g <- hist(model.data.na.omit$net_comm)
h <- hist(model.data.na.omit$net_expertise)

```


```{r, bivariate-plots, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Relationship between lidar use and each predictor by region." }
prop.lidar.plot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Proportion of Lidar Users in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

expertise.plot <- ggplot(model.data.na.omit, aes(x = net_expertise, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Perceived Expertise in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

comm.plot <- ggplot(model.data.na.omit, aes(x = net_comm, y = lidaruse, colour = location)) +
 # geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Communication with Alters in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

knowledge.plot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

future1.plot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
 # geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

soep1.plot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

experience.plot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

science.plot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial)) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
 scale_x_discrete(limits=c(1, 5), labels=c("Strongly distrust", "Strongly trust")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

ggpubr::ggarrange(prop.lidar.plot, experience.plot, future1.plot, knowledge.plot, soep1.plot, science.plot, comm.plot, expertise.plot, ncol = 4, nrow = 2)


# Literature Cited