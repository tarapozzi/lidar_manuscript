---
title: "Chapter 1 Manuscript"
author:
- name: Tara Pozzi
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::word_document2: default
  bookdown::pdf_document2: default
thanks: '.....'
affiliation: Boise State University
runhead: XXXX
geometry: margin=1in
fontsize: 12pt
csl: crm.csl
bibliography: references.bib
indent: yes
colorlinks: yes
---

```{r setup, include=FALSE}
library(citr) # doesn't work in this version of r...
library(tidyr)
library(ggplot2)
library(ggplotgui) # doesn't work in this version of r 
library(ggrepel)
library(plotly)
library(RColorBrewer)
library(here)
library(tinytex)
library(psych)
library(pastecs)
library(rstanarm)
library(loo) 
library(tidybayes)
library(tidyverse)  # ggplot, dplyr, %>%, and friends
library(ggdag)  # Make DAGs with ggplot
library(dagitty)  # Do basic DAG math
library(broom)  # For converting model output to data frames
library(clusterSim)
library(caret) # model comparison
library(plyr) # helps with model comparison computation
library(performance)
library(rworldmap)
library(hrbrthemes)
library(ggplotAssist)
library(dplyr)
library(purrr)
library(forcats)
library(modelr)
library(ggdist)
library(tidybayes)
library(cowplot)
library(rstan)
library(bookdown) # for cross-referencing
library(knitr) #for global options
library(formatR) 
```

```{r, global options, include=FALSE}
# Setup options for code decoration
opts_chunk$set(tidy = TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60),
highlight = TRUE)

# Setup options for code cache
opts_chunk$set(cache = 2, cache.path = "cache/")

# Setup options for plots
opts_chunk$set(fig.path = "Figures_MS/", dev = c("pdf", "png"),
dpi = 300)

## Locate figures as close as possible to requested position
## (=code)
opts_chunk$set(fig.pos = "H")

```

Abstract
========================
**"A concise and factual abstract is required. The abstract should state briefly the purpose of the research, the principal results and major conclusions. An abstract is often presented separately from the article, so it must be able to stand alone. For this reason, References should be avoided, but if essential, then cite the author(s) and year(s). Also, non-standard or uncommon abbreviations should be avoided, but if essential they must be defined at their first mention in the abstract itself."**

# HIGHLIGHTS
*Come back and edit this when results and discussion are finished*

1. There is between state variation of lidar use between Idaho and Washington.

2. Data suggests a more variable environment lends to flood risk manager's making more risk-prone decisions.

3. Social learning plays a critical role in shortfall minimization and lidar adoption.

# Introduction
*climate change and population rates are dangerous bc of increased flooding*
Floods are one of the most frequent and destructive natural disasters in the United States [@pralleDrawingLinesFEMA2019;@RiskMappingAssessment]. Flood events disrupt the ecological, cultural, and economic landscapes causing incalculable expenses to our society. Often, resulting with vulnerable groups even more at risk in the future [@howellDamagesDoneLongitudinal2019]. Since the National Centers for Environmental Information (NCEI) began tracking natural disaster events in 1980, there has been an increase in flood events in the U.S., some of those with unprecedented amounts of rainfall [@informationnceiBilliondollarWeatherClimate]. This is because as temperatures rise there is an increase in the amount of water vapor in the atmosphere, which exacerbates the potential for extreme rainfall events. In addition to growing flood risk from climate change, there is an rising rate of population growth and urbanization in coastal and inland floodplains [@pralleDrawingLinesFEMA2019; @schanzeFloodRiskManagement2006]. In 2015, 21.8 million (6.87%) of the U.S. population were identified as being exposed to a 100-year flood; meaning they lived in a location that would be inundated by a flood event that has a 1 in 100 chance of happening each year [@qiangDisparitiesPopulationExposed2019]. In light of these challenges, understanding how to manage flood risk is critical.

*introducing concept of risk risk*
Risk has a variety of definitions based on the disciplinary domain in which the concept is being examined. In an everyday sense, risk can be considered the chance of a negative outcome occurring [@mishraDecisionMakingRiskIntegrating2014]. We see risk as inherently transdisciplinary and needs to encapsulate the full context of the topic for which it is being applied. Therefore we define risk, in a flood context, as the quantifiable chance of a flood event given the known, contextual (e.g. social, environmental, political) factors. 

*floodplain maps as a function of risk*
Communities understand their flood risk typically by using Federal Emergency Management Agency (FEMA) floodplain maps that estimate the extent of flood hazards through hydrologic and hydraulic models. These analyses require topography, rainfall and run-off frequency distributions, and flood control structures (e.g. diversion dams, levees, bridges). In addition, these floodplain maps are essential for communicating flood risk to vulnerable populations, helping communities mitigate and adapt to floods, and the functioning of insurance programs, such as the the FEMA's National Flood Insurance Program [@pralleDrawingLinesFEMA2019]. However, recent reports estimate that approximately 25% of the flood damage claims occur outside of FEMA mapped floodplains each year because these maps can be outdated and inaccurate [@ludyFloodRiskPerception2012]. 100-year flood events are based on historical rainfall patterns, however this probability can change based on local land use, river impoundments, the amount of impervious surfaces, and long-term climate patterns [@TechniquesMethods2019].

*lidar as the solution*
-	Lidar is super accurate and awesome
-	 But it’s new and people don’t know how to use it
-	 it’s expensive to obtain

Previous research confirms that high-resolution topographic data is critical for an accurate floodplain map [@aliAssessingImpactDifferent2015; @cookEffectTopographicData2009]. In the past, flood risk managers typically used 10-meter or 30-meter resolution terrain models. However higher-resolution terrain models (e.g. 1-meter or smaller) are now available due to Light Detection and Ranging (lidar). Lidar is a laser-based remote sensing technology that uses the reflection of light to measure elevation and features on the ground such as vegetation and structures. Raw lidar data points are used to form a three-dimensional (3D) point cloud. These 3D point clouds can then be used in a wide-array of hazard applications such as wildfire fuel load calculation or wildlife habitat viewshed identification. In addition, lidar-derived products, such as high-resolution terrain models, are widely used in flood risk management to model different flooding scenarios [@muhadiUseLiDARDerivedFlood2020].
**START HERE**

Several government agencies initiated lidar acquisition projects in an effort to increase the availability of publically-accessible lidar. Foremost, the United States Geological Survey (USGS) established the 3D Elevation Program (3DEP) in 2010 as the first nationally-coordinated lidar acquisition program with a goal of flying the complete U.S. by 2023 with lidar data.  This would be the first ever national baseline of consistent, high-resolution topographic elevation data, including bare earth and 3D point clouds. In addition, FEMA established the Risk Mapping and Planning (RiskMAP) program as part of the Biggert-Waters Flood Insurance Reform Act of 2012. This act charged FEMA with reforming the flood insurance process, while also improving the accuracy and reliability of it's floodplain maps (USGS, 2017). As a result, both 3DEP and RiskMAP programs are used to fund lidar acquisition projects across the U.S. In addition, several other U.S. agencies including the National Oceanic and Atmospheric Administration (NOAA), the U.S. Department of Agriculture (USDA), the U.S. Army Corps of Engineers (USACE), and U.S. Forest Service (USFS) also participate in lidar acquisition. Figure XX displays the footprint of available topographic and bathymetric lidar across the contiguous, lower 48 states. From this image, there is a clear decrease in the availability of publically-accessible lidar in the western U.S. including Washington, Idaho, Montana, Oregon, Nevada, Utah, California, Arizona, and New Mexico. As lidar becomes more available and increasingly popular, it is important to understand the factors that influence a flood risk manager's decision to adopt this new technology into their practice of long-term risk mitigation.

!["The U.S. Interagency Elevation Inventory displays all publically-available lidar data and lidar-derived products for the continguous, lower 48 states. (source:https://coast.noaa.gov/inventory/)"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/lidar_map.PNG)

This study examines the landscape of factors that catalyze decision-makers to use lidar for flood risk management. Past research emphasizes risk perception as a key driver of risk mitigation behavior [@birkholzRethinkingRelationshipFlood2014;@mishraDecisionMakingRiskIntegrating2014;@slovicPerceptionRisk1987]. Risk perception is a fundamental way to characterize how individual’s intuitive risk judgment and allows for the identification, characterization, and quantification of risk. However, previous research on risk perception has been opposing. Past findings suggests individuals who perceive that natural hazards pose a greater risk also behave more cautiously [@vinhhungFloodRiskManagement2007]. Interestingly, studies also found the opposite in that individuals who perceive a greater risk engage in fewer mitigating behaviors [@bubeckReviewRiskPerceptions2012; @wachingerRiskPerceptionParadoxImplications2013]. This paradoxical behavior suggests that risk perception is more nuanced and moderated by individual and socio-cultural factors [@baerenklauUnderstandingTechnologyAdoption2005; @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012; @kahanCultureIdentityProtectiveCognition2007; @vanvalkengoedMetaanalysesFactorsMotivating2019].

While the field of hazards and disaster research is inherently multidisciplinary, there has been a recent, concerted effort for convergence research to integrate knowledge across disciplines and organizational boundaries to reduce disaster losses and promote collective well-being (Peek at al., 2020). In addition, previous flood risk research identified two main theoretical limitations of our existing intellectual understanding of risk-mitigating behavior. Firstly, there is limited predictive power of the theories applied so far (e.g. protection motivation theory) [@kellensPerceptionCommunicationFlood2013a]. Secondly, there is limited focus on the role of collective action (e.g. social beliefs) [@kuhlickeBehavioralTurnFlood2020].
In addition, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect the complex relationship between risk perception and risk mitigating behavior; there needs to be a critical look at the underlying assumptions of risk perception and a focus on coordination of theories, methods, and variables. [@rufatSwimmingAloneWhy2020]. We employ cultural evolutionary theory to study lidar adoption because it offers a convergent research lens to compare individual and collective action and has the potential for prediction.  

In the next section we review cultural evolutionary theory in detail and explain how this theory is beneficial for understanding the underlying mechanisms that shape flood risk management behavior. Next, we apply this theoretical framework to our case study of lidar adoption for flood risk management.  This is followed by the methods section that explains our survey instrument development process and statistical approach for analyzing the survey data. The results from our analysis and a discussion about significant trends will follow. Finally, we discuss the implications of these results and need for further research. 

# Individual and collective predictors of risk-mitigation behavior

Previous research identified the importance of serveral individual factors as a function of risk mitigation behavior, however there has been limited research in the role of collective action [@kuhlickeBehavioralTurnFlood2020]. It is important to look at the combined effects of both individual and collective predictors in predicting risk-mitigation behavior so that we can understand the relative contribution of each predictor [@vanvalkengoedMetaanalysesFactorsMotivating2019]. In this study, we examine the influence of a collection of individual and collective predictors on a flood risk manager's adoption of lidar for flood risk mitigation. The following section examines previous research into individual predictors of risk mitigation behavior and then explores how cultural evolutionary theory can help illuminate collective predictors of influence. 

## *Previous Risk Mitigation Research*

*individual factors*
Previous flood risk management research focused on flood risk perception as a critical factor of developing effective flood risk management strategies [@birkholzRethinkingRelationshipFlood2014]. However, recent research has re-examined risk perception's role in behavior and decision-making because of the difficulty connecting risk perception with management and the challenge of parsing out the connection of risk perception with underlying contextual factors [@rufatSwimmingAloneWhy2020]. For example, a study by Bubeck et al. (2012) found risk perception to be a weak predictor of precautionary behavior and suggests shifting focus towards flood-coping appraisal for explaining flood risk management behavior. In addition, Kellens et al. (2013) reviewed 57 empirically based peer-reviewed articles on flood risk perception and communication to assess overall trends in flood risk research. The authors found that the majority of studies were exploratory and did not apply a theoretical framework to examine risk perception [@kellensPerceptionCommunicationFlood2013a]. Of the studies that employed a theoretical framework, protection motivation theory (PMT) was the most common. PMT explains individual decisions about preparing for risk as a function of threat appraisal (e.g. likelihood of exposure to a flood, severity of exposure, and fear) and coping appraisal (e.g. self-efficacy, outcome efficacy, and outcome costs). The results of this review suggest future research should have more theoretical support and methodological openness; specifically, the use of a theoretical framework that emphasizes the effects of physical exposure and hazard experience [@kellensPerceptionCommunicationFlood2013a]. 

*collective factors*
Collective factors of risk mitigation behavior are limited in the existing flood risk management literature, however there has been some initial evidence found of the influence of social networks on risk mitigation behavior [@kuhlickeBehavioralTurnFlood2020]. This is of particular interest for our study because it helps elucidate the influential processes of peer-to-peer interactions have on risk mitigation behavior. Peer influence is the diffusion of ideas, practices, or technologies through network ties from social interactions [@muterSocialContagionRisk2013]. Peer influence is a helpful tool for behavior prediction based on an individual’s position in a social network [@daraganovaAutologisticActorAttribute2012; @levinProblemPatternScale1992]. Ego network analysis has been used to understand technology adoption due to its exposure of information of exchange and diffuse through network relations, which is a form of risk mitigation behavior [@pengResearchNoteDynamic2013]. The application of social networks to flood risk management decision-making is still in its infancy, however the findings from previous research with respect to social networks and technology adoption provide a compelling baseline for using it to understand peer influence in this application.

*our solution to risk mitigation behavior*
Additionally, recent research suggests the importance of context, local power relations, and constraints/opportunities that affect risk mitigating behavior calling for a coordination of theories, methods, and variables to understand the underlying assumptions of decision-making [@rufatSwimmingAloneWhy2020]. Given these findings about previous shortcomings of flood risk management research, we employ cultural evolutionary theory in an effort to employ a comprehensive theoretical baseline for flood risk mitigation behavior research that can be used across disciplines and scales. Furthermore, our study keeps the goals of convergence research in mind.

*our focus is different*
Secondly, current literature is pre-dominantly focused on the public flood risk behavior, rather than flood risk managers themselves. There has been some previous work focused on emergency manager decision-making, however this area is understudied [@robertsDecisionBiasesHeuristics2019; @brodyExaminingClimateChange2010]. Out study is solely focused on addressing individual and collective predictors of risk mitigation beahvior at the decision-maker level.

## *Culture and Risk*
*culture and how it is formed*
Culture is information acquired by individuals through social learning [@henrichEvolutionCulturalEvolution2003]. Social learning is the observing, modeling, and imitating of behaviors, attitudes, and emotional reactions of others (Bandura). Individual learning is a product of the environment an individual is in. This process creates a shared set of beliefs and norms among a group of individuals [@creanzaCulturalEvolutionaryTheory2017]. Several researchers believe social learning has improved human adaptability so much that we are able to inhabit such a wide range of habitats, unlike other animal species.  

*evolution of culture*
Behavioral adaptations display the variation of culture as a result of the evolutionary dynamics of cultural systems. Cultural evolutionary theory describes this process as the selection and transmission of culture over time. The selection process leads to variation of culture across temporal, spatial, and institutional scales and the transmission leads to adaptation (e.g. adoption of new technology). Reminiscent of genetic evolution, human culture evolves through the process of natural selection. This evolution results in between-group variation of adaptive behavior and cooperation and can lead to increased fitness or utility[@richersonCulturalGroupSelection2016; @henrichEvolutionCulturalEvolution2003]. Unlike genetic transmission, it is important to note cultural transmission can occur over a short time scale, within a generation through social learning [@richersonCulturalGroupSelection2016]. Cultural evolutionary theory and social learning are increasingly popular in explaining natural resource management because of the potential social learning has for increasing adaptive capacity [@reedWhatSocialLearning2010]. 

*CTR*
In a similar vein, the cultural theory of risk is the transmission of risk information among a network of individuals through social learning [@douglasRiskCultureEssay1983]. Previous flood risk management research has suggested the use of cultural theory of risk to contextualize the relationship of risk perception as a function of cultural adherence and social learning [@birkholzRethinkingRelationshipFlood2014]. This theory has been employed in a couple empirical flood risk management studies so far and provides an intriguing underpinning of risk perception research specifically [@shenFloodRiskPerception2009]. Cultural evolutionary theory is similar to cultural theory of risk, however it more broadly offers a way to understand the complex dynamics of cultural change through interactions between individuals and populations, such as is needed for flood risk management (Brooks et al., 2018).

*how the theory is implemented*
Several studies have implemented social network analysis to examine the the influence of social ties on communication in disaster management, however the effect of social networks on other topics in disaster management has been minimally explored [@bojovicUnderstandingDisseminationAdoption2020]. Bojovic et al. (2020) conducted a full network analysis on the diffusion of innovation and technologies for risk management, which was the first study of this topic in disaster management. The study focused on the identification of key actors to implement information dissemination through. In addition, there is another type of social network analysis called an ego network analysis, that is helpful for understanding the variation of behavior of individuals through identification of local social structures unique to the individual of interest (e.g. flood risk manager) [@hannemanChapterEgoNetworks2005].

*egonet limitations*



## *Predictor selection*
In order to select relevant individual and collective predictors of flood risk mitigating behavior *a priori*, we conducted a literature review of previous work that looked at the effect of constructs on flood risk mitigating behavior.  

!["Individual and collective constructs, with the motivation, effect, and supporting literature for each construct, that potentially influence risk mitigating behavior for flood risk management"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_lit_review.jpg)
*section about each predictor?*
Could say something like of these thirteen constructs, we selected eight to examine in further detail for flood risk management. These are the reasons why...instead of leaving the motivation in the chart?

## *Case study description*
Technology adoption in flood risk management provides an interesting case study to examine the effect of individual and collective predictors on risk mitigating behavior. This study examines the adoption of lidar in communities throughout Idaho, Oregon, and Washington which are expected to see an increase in precipitation and higher temperatures earlier in the year [@clarkChangesPatternsStreamflow2010; @ralphVisionFutureObservations2014; @idahoofficeofemergencymanagementStateIdahoHazard2018; @washingtonemergencymanagementdivisionWashingtonStateEnhanced2020; @slaterRecentTrendsFlood2016]. In addition, the Pacific Northwest is experiencing increasing urbanization rates and population growth, therefore changing flood risk in communities. 

While Idaho, Oregon, and Washington all reside in the same geographic region, each state has unique sets of challenges when it comes to flood risk management due to differential spatial landscape, population growth and urbanization, and resource availability (e.g. funding for flood risk management, educational opportunities for flood risk managers). 

### **Idaho**
In 2019, Idaho was home to 1.79 million people across 82,643 square miles; 21.7 people per square mile [@CensusBureauQuickFactsb]. It is a land-locked state and can be broken down into three main areas: the panhandle in the north is filled with coniferous forests and lakes, the central section is filled with vast mountain ranges and alpine lakes, and the southern section, known as the Snake River Plain, is filled with sagebrush steppe and high desert environment. There is influence from the Pacific Ocean in the north and west side of Idaho, resulting in cloudy, humid, and wet winters, whereas the east is the opposite with wet summers and dry winters. Average annual rainfall ranges from 10" in the arid southwest regions to 50" at higher elevations in certain river basins [@d.o.oIdahoUSAClimate]. In addition, Idaho sees abundant amounts of snowfall in the mountains. 

While most of the population is concentrated in the southern part of the state, there is flooding across the entire state that impacts people and structures. Idaho is prone to riverine flooding, ice/debris jam flooding, levee/dam/canal breaks, stormwater, sheet or areal flooding, and mudflows [@idahoofficeofemergencymanagementStateIdahoHazard2018]. In 2021, Idaho had 145 NFIP participating communities across 44 counties [@CommunityStatusBook]. 

Lidar acquisition is coordinated by Boise State University's Idaho Lidar Consortium in conjunction with Idaho State University's GIS Research and Training Center, which stores lidar data for public use. There is no state-approved funding for lidar acquisition and therefore, communities rely on using local funding in addition to applying for funding from 3DEP and/or FEMA. By the end of 2020, Idaho had 25% of the state covered with publically-available lidar. 

### **Oregon**
In 2019, Oregon had over 4.2 million residents across 95,988 square miles; 43.8 people per square mile [@CensusBureauQuickFactsa]. Oregon can be broken down into six main areas: the Coast Range, the Willamette Lowland, the Cascade Mountains, the Klamath Mountains, the Columbia Plateau, and the Basin and Range Region. There is a maritime influence across the entire state due to the Pacific Ocean. The Coast range is predominantly evergreen forests with many small coastal lakes. The mountain regions are typically several thousand feet about sea-level and have a range of dense forests and lakes. Eastern Oregon contains high desert environment with few steep mountains. 

Oregon's population is concentrated in the coastal region of the state. Oregon has an extensive history of multiple types of flooding including riverine flooding, flash floods, ice/debris jam flooding, coastal flooding, shallow area flooding, urban flooding, and playa flooding [@laytonStateInteragencyHazard2015]. In 2021, Oregon had 228 NFIP participating communities across 36 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the State of Oregon Department of Geology and Mineral Industries' Oregon Lidar Consortium. Bu the end of 2020, Oregon had 98% of Oregon's populated areas were covered with publically-available lidar [@DOGAMILidarOregon]... [change state for lidar coverage in state? would be a lot less, maybe email dogami-info@oregon.gov] 

### **Washington**
In 2019, Washington had over 7.6 million residents across 66,455 square miles; 114 people per square mile [@CensusBureauQuickFacts]. Washington can be broken down into six main areas: the Olympic Mountains, Coast Range, Puget Sound Lowlands, Cascade Mountains, Columbia Plateau, and Rocky Mountains. Most of the areas in the western and northern parts of Washington are predominately evergreen forests, where the eastern and southern parts of Washington are semiarid where grasses, sagebrush, and scattered shrubs can be found. Annual precipitation on the Pacific side of the Olympic Peninsula exceeds 150 inches, but places on the northwest of the peninsula receive less than 20 inches a year and on the eastern side receive less than 8 inches [@WashingtonStateCapital].

More than three-fourths of the population lives in Puget Sound Lowlands [@WashingtonStateCapital] Flooding in Washington typically occurs on a seasonal basis due to rainfall from atmospheric rivers, rainfall on snow, flash foods from storms, and winter storms causing storm surges and high tide [@washingtonemergencymanagementdivisionWashingtonStateEnhanced2020]. It is estimated that in 2021, Washington had 277 NFIP participating communities across 39 counties [@CommunityStatusBook].

Lidar acquisition is coordinated by the Washington State Department of Natural Resources and receives funding from the Washington State Legislature to acquire and upkeep lidar data for the state.  Over 50% of the state has been flown with lidar data [@GerWaLidar]. *get an updated number from Abby*

### **Physical flood risk**
Since flooding is becoming an increasingly damaging and costly issue, there has been a rise in interest from non-governmental groups to predict flood risk at the property level for households and property owners to be aware of their true flood risk. First Street Foundation, a non-profit organization of modelers, researchers, and data scientists to create the first publicly-available flood risk model for the lower 48 states. According to First Street, there is nearly 70% properties with substantial flood risk than previously predicted by FEMA floodplain maps [@firststreet.orgMissionFirstStreet]. This study is focused on Idaho, Oregon, and Washington so in an effort to understand the nature of physical flood risk in each of these states, we have compared the FEMA projections to the First Street projections as seen \@ref(tab:comparison-table). **should I turn this into a map?** . It is important to note that FEMA report's Idaho with the least amount of risk compared to Oregon and Washington, however First Street reports it as having the most. This difference could be because there are still many locations in Idaho that are not mapped by FEMA and therefore building in floodplain areas could be more likely.

```{r comparison-table, echo=FALSE, fig.cap="\\label{tab_statecomp}"}
id.floodrisk <- read.csv("data/idaho_floodrisk_firststreet.csv")
id.fema.2020.total <- round(sum(id.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
id.fema.2020.pct <- round(mean(id.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
id.fs.2020.total <- round(sum(id.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
id.fs.2020.pct <- round(mean(id.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

or.floodrisk <- read.csv("data/oregon_floodrisk_firststreet.csv")
or.fema.2020.total <- round(sum(or.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
or.fema.2020.pct <- round(mean(or.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
or.fs.2020.total <- round(sum(or.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
or.fs.2020.pct <- round(mean(or.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

wa.floodrisk <- read.csv("data/washington_floodrisk_firststreet.csv")
wa.fema.2020.total <- round(sum(wa.floodrisk$FEMA.Properties.at.Risk.2020..total.), digits=0)
wa.fema.2020.pct <- round(mean(wa.floodrisk$FEMA.Properties.at.Risk.2020..pct.), digits=1)
wa.fs.2020.total <- round(sum(wa.floodrisk$FS.Properties.at.Risk.2020..total.), digits=0)
wa.fs.2020.pct <- round(mean(wa.floodrisk$FS.Properties.at.Risk.2020..pct.), digits=1)

state.comp <- matrix(c(id.fema.2020.total, or.fema.2020.total, wa.fema.2020.total, id.fema.2020.pct, or.fema.2020.pct, wa.fema.2020.pct, id.fs.2020.total, or.fs.2020.total, wa.fs.2020.total, id.fs.2020.pct, or.fs.2020.pct, wa.fs.2020.pct), ncol=3, byrow=TRUE)
colnames(state.comp) <- c("Idaho", "Oregon", "Washington")
rownames(state.comp) <- c("Total FEMA Properties at Risk (2020) ", "Percent FEMA Properties at Risk (2020)", "Total FS Properties at Risk (2020) ", "Percent FS Properties at Risk (2020)")
knitr::kable(state.comp, caption="Summary information about environmental and social differences between Idaho and Washington.")
```


## *Relevant predictors of lidar adoption*
**this section might be better under the methods section?**
Given the previous literature, as well as the nature of our case study we narrowed down our study to focus on eight constructs, with a total of twelve predictors.   

!["Predictors of lidar adoption"](C:/Users/tarapozzi/Documents/lidar_manuscript/images/predictor_lidar.jpg)

# METHODS
```{r, survey data upload, include=FALSE}

idaho <- read.csv("data/id.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Idaho") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(idaho)

oregon <- read.csv("data/or.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses  
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Oregon") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(oregon)

washington <- read.csv("data/wa.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Washington") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(washington)

alaska <- read.csv("data/ak.csv", stringsAsFactors=FALSE)[-c(1),] %>% # subsetting: took out 40 & 134 because they had duplicate entries 
  filter(grepl("1", screen)) %>% # this tells it to only keep responses that selected "Yes" for the screening question\
  subset(.,experience_1!="") %>% # remove blank responses
  dplyr::select(matches("RecipientLastName|RecipientFirstName|RecipientEmail|LocationLatitude|LocationLongitude|screen|years|comm_name|NFIP|no_NFIP|experience_1|experience_2|experience_3|experience_4|experience_5|future_1|future_2|future_3|future_4|future_5|currentmap|flood_zone|prepared|incr_no_flood|incr_sev_flood|lidaruse|interestlid|barrier_1|barrier_2|barrier_3|barrier_4|barrier_5|barrier_6|usefulid|toolslid1|tooslid1_4_TEXT|accesslid|accesslid_7_TEXT|useslid|useslid_4_TEXT|toolslid|toolslid_4_TEXT|gender|age|education|degree|science_trust|gov_trust|gov_involve|soep_1|usefulid|no_alters|alter_names_1_TEXT|alter_names_2_TEXT|alter_names_3_TEXT|alter_names_4_TEXT|alter_names_5_TEXT|alter_names_6_TEXT|alter_names_7_TEXT|alter_names_9_TEXT|comm_1|lidar_1|expertise_1_1|comm_2|lidar_2|expertise_2_1|comm_3|lidar_3|expertise_3_1|comm_4|lidar_4|expertise_4_1|comm_5|lidar_5|expertise_5_1|comm_6|lidar_6|expertise_6_1|comm_7|lidar_7|expertise_7_1|comm_8|lidar_8|expertise_8_1")) %>% 
  dplyr::rename(LastName = RecipientLastName,
         FirstName = RecipientFirstName,
         Email=RecipientEmail) %>% #this is so that I can merge this data with the contact list
  #unite("ws_request",toolslid, toolslid1, toolslid_4_TEXT, toolslid1_4_TEXT,interestlid, sep="") %>% # this combines all columns that survey respondents selected into one column
  add_column(.,location="Alaska") %>%
  distinct(Email, .keep_all = TRUE) # there are some duplicate contacts, let's make sure to drop the duplicate 
str(alaska)

# Combine all four states into one dataset # 
survey.responses <- rbind(idaho, oregon, washington) %>% 
  subset(.,incr_no_flood!="") # remove additional blank responses
  

#write.csv(survey.responses, "C:/Users/tarapozzi/Documents/Manuscript/survey.responses.csv")

```

### *Survey design*
We conducted eight, semi-structured interviews with stakeholders (e.g. flood risk managers, state officials, academics) to identify common themes and the extent of relevant predictors to measure. These interviews were conducted prior to the finalization of the survey instrument and directly informed our approach to ensure our work could identify opportunities and barriers to lidar adoption. In addition, we pre-tested our survey instrument with three industry professionals (two women and one man) to provide feedback on question clarity and survey flow. The second pre-test was with six university students and staff to give additional feedback on the survey. We modified the survey based on this feedback. The final draft of the survey was then approved by the Institutional Review Board at Boise State University. 

The finalized survey consisted of four main parts (see Appendix A). The first part focused on gathering information about the respondent's experience and beliefs about their flood risk management community. The first section consisted of a screening question to assess if the respondent was qualified by asking if they were a flood risk manager. The second section gathered background information about the number of years in the profession and the third section asked the respondent about their local floodplain. This section ask the respondent if their community participated in the NFIP, as well as information about flooding damage history and perceived future flood damage in the next 30 years. The fourth section examined the current mapping data in the respondent's flood risk management community. This included questions about floodplain map accuracy, flooding outside the designated FEMA flood zone, and level of preparation for a flood event. The next section asked the respondent to report their personal beliefs about the change in the number and/or severity of flood events in their flood risk management community in the future. 

The next part of the survey centered around the respondent's relationship with lidar for flood risk management. The first section in this part focused on the respondent's relationship with lidar for flood risk management. Lidar use, in this context, referred to work with either raw lidar data (e.g. LAS or point clouds) or lidar-derived data (e.g. DEM, DSM). If the respondent replied yes, then they were asked questions about where they received it from and how they apply it to flood risk management, as well as continued educational opportunities. If they selected no, they were asked about barriers they faced to using lidar and if they would like to partake in education opportunities. 

The third part of the survey gathered information about the respondent's network. It focused on the respondent's flood risk management network connections. For each reported alter, the respondent answered if the alter used lidar, how much they communicated with them, and their expertise in flood risk management. 

The final part of the survey asked the respondent about their personal beliefs in risk-taking and trust. The first section in this part asked the respondent to report their general risk-taking attitude. The second section asked about the respondent's gender, age, and education level, as well as their personal beliefs regarding trust in the federal government, science, and involvement level of the federal government with flood risk management. 

### *Data collection*
The survey's target respondent was the floodplain manager or administrator from participating and non-participating NFIP communities in Idaho, Oregon, Washington, and Alaska. This also included individuals that may use lidar for flood risk management applications in conjunction with software applications such as Geographic Information System (GIS). The majority of sample respondents were municipal, state, and federal employees, as well as some private industry employees. In order to achieve our target population, the sample frame included several sources of contacts including NFIP coordinators, Association of State Floodplain Managers (ASFPM) recognized Certified Floodplain Mangers (CFM), county-level GIS administrators, the five largest cities and tribal GIS administrators if present, county and tribal emergency managers, the Federal Geospatial Data Coordination Contacts by State, and additional, relevant contacts for the 2019 Northwest Regional Floodplain Managers Association (NORFMA) Conference contact list. 

```{r, survey distribution, include=FALSE}
# Washington
washington.results <- read.csv("data/wa.csv")
wa.no <- washington.results %>%
  filter(grepl("2", screen))

#load distribution history
wa.dist <- read.csv("data/wa_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

a <- length(wa.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 398-13: 383 potential responses

# oregon
oregon.results <- read.csv("data/or.csv")
or.no <- oregon.results %>%
  filter(grepl("2", screen))

#load distribution history
or.dist <- read.csv("data/or_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in

b <- length(or.dist$Email) - 13
# Total number of relevant respondents: total distribution minus those who didn't pass screening: 357-13: 344 potential responses


# Idaho potential responses
id.pot.responses <- read.csv("data/id_contacts.csv")
c <- length(id.pot.responses$Email)
# 463 potential responses


#load distribution history
alaska.results <- read.csv("data/ak.csv")
ak.no <- alaska.results %>%
  filter(grepl("2", screen)) # 2 said no to screening

ak.dist <- read.csv("data/ak_dist.csv", stringsAsFactors=FALSE)[-c(1),] %>%
  filter(grepl("Email Sent|Finished Survey|Partially Completed Survey|Started Survey", Status)) # this only keeps working emails & those who have opted in
d <- length(ak.dist$Email) - 2

total.sample.frame <-  a + b + c + d


```

``` {r, response rate, include=FALSE}

id.rr <- round((96/a)*100, digits=1) # a is the total number of potential responses

or.rr <- round((58/b)*100, digits=1)

wa.rr <- round((54/c)*100, digits=1)

ak.rr <- round((6/d)*100, digits=1)


```

Survey data was collected through a structured, online survey distributed through Qualtrics to email addresses in our sample frame. The survey was sent to Idaho, Oregon, Washington and Alaska between May and July 2020. The survey took an average of 10 to 15 minutes to complete. There were four to six email correspondence messages with potential survey participants over the course of four weeks to help increased our response rate. Table \@ref(tab:data-collection) summarizes the potential respondents, number of usable survey responses, and response rate for each state. 

```{r, data-collection, echo=FALSE, fig.cap="\\label{survey data collection}"}
survey.data <- matrix(c(a, 96, id.rr, b, 58, or.rr, c, 54, wa.rr, d, 6, ak.rr), ncol=3, byrow=TRUE)
colnames(survey.data) <- c("Potential Respondents", "Number of Usable Responses", "Response Rate")
rownames(survey.data) <- c("Idaho", "Oregon", "Washington", "Alaska")
knitr::kable(survey.data, caption="Comparative survey distribution and collection.")
```


We did not include Alaska in our final statistical analysis because of insufficient number of responses. In addition, both Oregon and Washington had lower response rates than Idaho. It is common to see 10-25% response rate from detailed online surveys, which our response rates are within the typical bounds [@sauermannIncreasingWebSurvey2013]. We suspect the variation in response rates between states could be due to competing time availability during the COVID-19 pandemic which required extra time and energy from emergency managers, which was our target population for this survey. For this reason, we expected a lower survey response than typical. Given this, there may be some implications of the representativeness of sample. 

### *Estimation Analysis*

A Bayesian approach with a Generalized Logistic Regression (GLR) met the model criteria for understanding risk because it is able to characterize non-linear, unpredictable outcomes. Furthermore, a logistic regression was used because the response variable, lidar use, is binary. This analysis uses a Markov Chain Monte Carlo algorithm to predict posterior distributions of each parameter's effect on lidar use. 

The model followed a binomial distribution curve, where the distribution of lidar use, y_{ij}, was modeled as follows: 

\[b_i \approx N(0,\sigma_b)\]

\[\eta_i = \mu_\alpha + \beta x_{ij}+...+\beta_kx_{ij} + b_i\]

\[\pi_i = \frac{e^{\eta}_i}{1+e^{\eta}_i}\]

\[y_{ij} \approx Binom(1, \pi_i)\] (1)

where $x_{ij}$, predictors, are the ith rows of the known design matrices x, and $\beta$ is a vector of regression parameters. This Bayesian approach allowed for adjustment of uncertainty associated with each parameter on the final outcome, lidar use. In order to do this, each parameter had to be assigned a prior belief of that parameter value. The values for these parameters are fit by sampling from these distributions to maximize the likelihood under this model [@kwonClimateInformedFlood2008]. The regression parameters, $\beta$, are normally distributed, \[\beta_k\approx N(\eta_{\beta k}, \sigma_k)\]. Additionally, the parameters of this distribution, $\eta_{\beta k}$ and $\sigma_k$, also have prior distributions assigned to them that are constrained by 0 and a positive value.

#### **Priors**

We used a weakly informative prior distribution to provide modest regularization, reduce the chance of a Type I error, and improve the out-of-sample prediction for regression models (McElreath 2015). Gelman et al. (2008) suggests the use of a Cauchy distribution with center 0 and scale 2.5 for logistic regression models [@gelmanWeaklyInformativeDefault2008]. This study uses a Cauchy distribution as recommended for models with a low sample size [@lemoineMovingNoninformativePriors2019]. 

#### **Validation**

We assessed the overall model performance through Leave-One-Out Cross-Validation (LOOCV). This process provides an absolute metric for the model's predictive ability. Lastly, because this model had categorical predictors, we plotted the predicted probability against the observed proportion for some binning of the data using counterfactual plots (Levy, 2012).

#### **Error**

We specified our model to compute 2,000 lidar use predictions based on our predictors. We interpreted the mean of these results as the projected lidar use. We used Bayesian R-squared to measure our overall model accuracy. However, this can be unreliable for small sample sizes, so we also calculated the mean absolute error (MAE) of our model. 

#### **Exploratory Analysis**

Through the use of the programming language R and program Rstudio, we used Bayesian theory to analyze the effect of individual and collective predictors of lidar adoption. We used four Monte Carol Markov Chains (MCMC) with 2,000 iterations for warmup and an additional 2,000 iterations for the model. Additionally, the convergence was checked by visually inspecting the MCMC trace plots of the model parameters. We assessed effective sample size and checked model convergence, indicated by R-hat statistics close to 1 and stable, well-mixed chains [@gelmanBayesianWorkflow2020]

The results of this model allowed us to explore the effect of a multitude of predictors on lidar use in Idaho, Oregon, and Washington. We hypothesized that the model would be helpful for understanding the level of predictor influence, however we expected the predictive capacity of our model to be limited considering the large number of predictors and small sample size of our study. 

# RESULTS

Table \@ref(tab:survey-dem-table) below compares the demographic and experience background of survey respondents by region. We received the greatest number of responses from Idaho. This is potentially because of the survey's affiliation with Boise State University and therefore flood risk managers in Idaho took greater interest in this study. Washington had the highest percentage of female respondents, second highest percentage of respondents with a Bachelor's degree or higher, and longest average length of flood risk manager experience. The results show no significant distortions of representativeness found for age, gender, geographical area, or level of education. 


``` {r, survey demographics, include=FALSE}
id.sample.size <- count(idaho$location=="Idaho")
or.sample.size <- count(oregon$location=="Oregon")
wa.sample.size <- count(washington$location=="Washington")

count(idaho$age)
count(oregon$age)
count(washington$age)
# age breakdown
id.age.2 <- round((2/96)*100, digits=0) # the denominator is the sample size
id.age.3 <- round((17/96)*100, digits=0) # the numerator is from 
id.age.4 <- round((27/96)*100, digits=0)
id.age.5 <- round((48/96)*100, digits=0)

or.age.2 <- round((2/58)*100, digits=0)
or.age.3 <- round((9/58)*100, digits=0)
or.age.4 <- round((17/58)*100, digits=0)
or.age.5 <- round((25/58)*100, digits=0) # some people didn't respond

wa.age.2 <- round((3/54)*100, digits=0)
wa.age.3 <- round((10/54)*100, digits=0)
wa.age.4 <- round((17/54)*100, digits=0)
wa.age.5 <- round((23/54)*100, digits=0)

# gender breakdown
id.gender.male <- round((59/96)*100, digits=0)
id.gender.female <- round((37/96)*100, digits=0)

or.gender.male <- round((35/58)*100, digits=0)
or.gender.female <- round((20/58)*100, digits=0)

wa.gender.male <- round((30/54)*100, digits=0)
wa.gender.female <- round((24/54)*100, digits=0)

#education breakdown
id.edu.ba.ma <- round((37+29)/96*100, digits=0) # represents the number of respondents with either a bachelors or advanced degree
or.edu.ba.ma <- round((21+26)/58*100, digits=0) 
wa.edu.ba.ma <- round((17+24)/54*100, digits=0)

#years in the industry
idaho$years <- as.numeric(idaho$years)
id.years <- round(summarise(idaho, avg=mean(years)), digits=1)

oregon$years <- as.numeric(oregon$years)
or.years <- round(summarise(oregon, avg=mean(years, na.rm=TRUE)), digits=1) ### THIS ISNT WORKING

washington$years <- as.numeric(washington$years)
wa.years <- round(summarise(washington, avg=mean(years)), digits=1)
```

```{r, survey-dem-table, echo=FALSE, fig.cap="\\label{demographics}"}
survey.dem <- matrix(c(96, 58, 54, "39%", "34%", "44%", "69%","81%", "76%",id.years, or.years, wa.years), ncol=3, byrow=TRUE)
colnames(survey.dem) <- c("Idaho", "Oregon", "Washington")
rownames(survey.dem) <- c("Sample Size", "Female", "University Education", "Average Flood Risk Experience (years)")
knitr::kable(survey.dem, caption="Comparative descriptive statistics for survey demographics across Idaho, Oregon, and Washington.")
```

```{r, data sim, include=FALSE}
## STEP 1: Data Simulation ##
set.seed(124) 
N=180 # number of survey respondents # played around with n=720 and that made results significantly better
K=8 # number of predictors

## 1) Set the intercept ##

intercept=0 ## mean value of lidar use when all predictors are equal to 0

# for random effects, we could look at location

## 2) Set the predictor variables ##
## set simulate experience with binomial for each type of experience
experience_1 <- rbinom(N, 1, .5) # damage to property in community # yes (1) no (0)
future_1 <- sample(1:5, N, replace=TRUE) # damage to property in community in the future 1=0% to 5=100%
science_trust <- sample(1:5, N, replace=TRUE) # not at all (1) completely (5)
incr_sev_flood<- sample(1:3, N, replace=TRUE) # no (1) stay the same (2) yes (3)
soep <-  sample(0:10, N, replace=TRUE) # range of risk preference from 0 to 10, where 0 = I generally prefer to take risks to 10 = I generally prefer to avoid risks
alter_lidar_prop <- runif(N, 0, 1) # this simulates the a range of potential proportion of lidar users in alters
alter_exp_mean <- runif(N, 0, 10) # this will rep the mean expertise of alters 
alter_comm_mean <- runif(N, 1, 6) # reps the mean communication of alters with respondent

#location <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

# Make data frame with raw data
raw.data <- data.frame(experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)

raw.data <- tibble::rowid_to_column(raw.data, "ID")

## Now let's set the effect size ##
b_experience_1 <- .2
b_future_1 <- -.6
b_incr_sev_flood <- 0.6
b_science_trust <- 0.1
b_soep <-  0
b_alter_lidar_prop <- -1 
b_alter_comm_mean <- 0.1
b_alter_exp_mean <- 0.5


# 4) Set the response variable (aka deterministic part) ##
p <- intercept + experience_1*b_experience_1 + future_1*b_future_1 + incr_sev_flood*b_incr_sev_flood  + science_trust*b_science_trust + 
  soep*b_soep + alter_lidar_prop*b_alter_lidar_prop + alter_comm_mean*b_alter_comm_mean + alter_exp_mean*b_alter_exp_mean 

pr <- plogis(p) # convert from log odds to probability

lidaruse <- rbinom(N,1,pr)

## 5) Combine data into dataframe ##

sim.data <- data.frame(lidaruse, experience_1, future_1, science_trust, incr_sev_flood, soep, alter_lidar_prop, alter_comm_mean, alter_exp_mean)
write.csv(sim.data, "sim_data.csv")

#### Descriptive Stasim.data ####
# Check for correlations
cor(sim.data) # nothing > |.5| so we are good!

# plot the raw data & check for outliers
boxplot(sim.data[-24])

#### Check out the model & see if it returns out effects
model <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + future_1 + science_trust + soep + alter_lidar_prop + alter_comm_mean + alter_exp_mean, 
                  prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                  prior_intercept = cauchy(0,10), 
                  data=sim.data, family= binomial(link = "logit")) # this elimates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 


plot(model)
coef(model)

## experience is not being recovered very well... or science trust and alter lidar prop
```

```{r, recode, include=FALSE}
survey.responses$lidaruse <- revalue(survey.responses$lidaruse, c("1"="1", "2"="0", "3"="0"))
idaho$lidaruse <- revalue(idaho$lidaruse, c("1"="1", "2"="0", "3"="0"))
oregon$lidaruse <- revalue(oregon$lidaruse, c("1"="1", "2"="0", "3"="0"))
washington$lidaruse <- revalue(washington$lidaruse, c("1"="1", "2"="0", "3"="0"))

# yes=1, no=0

# Direct Experiences
survey.responses$experience_1 <- revalue(survey.responses$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
survey.responses$experience_2 <- revalue(survey.responses$experience_2, c("1"="1", "2"="0"))
survey.responses$experience_3 <- revalue(survey.responses$experience_3, c("1"="1", "2"="0"))
survey.responses$experience_4 <- revalue(survey.responses$experience_4, c("1"="1", "2"="0"))
survey.responses$experience_5 <- revalue(survey.responses$experience_5, c("1"="1", "2"="0"))

idaho$experience_1 <- revalue(idaho$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
idaho$experience_2 <- revalue(idaho$experience_2, c("1"="1", "2"="0"))
idaho$experience_3 <- revalue(idaho$experience_3, c("1"="1", "2"="0"))
idaho$experience_4 <- revalue(idaho$experience_4, c("1"="1", "2"="0"))
idaho$experience_5 <- revalue(idaho$experience_5, c("1"="1", "2"="0"))

washington$experience_1 <- revalue(washington$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
washington$experience_2 <- revalue(washington$experience_2, c("1"="1", "2"="0"))
washington$experience_3 <- revalue(washington$experience_3, c("1"="1", "2"="0"))
washington$experience_4 <- revalue(washington$experience_4, c("1"="1", "2"="0"))
washington$experience_5 <- revalue(washington$experience_5, c("1"="1", "2"="0"))

oregon$experience_1 <- revalue(oregon$experience_1, c("1"="1", "2"="0"))# yes=1, no=0
oregon$experience_2 <- revalue(oregon$experience_2, c("1"="1", "2"="0"))
oregon$experience_3 <- revalue(oregon$experience_3, c("1"="1", "2"="0"))
oregon$experience_4 <- revalue(oregon$experience_4, c("1"="1", "2"="0"))
oregon$experience_5 <- revalue(oregon$experience_5, c("1"="1", "2"="0"))


# closer the experience, higher the number for yes

# Trust-- reverse code
survey.responses$gov_trust <- revalue(survey.responses$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$gov_trust <- revalue(idaho$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$gov_trust <- revalue(oregon$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$gov_trust <- revalue(washington$gov_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

survey.responses$science_trust <- revalue(survey.responses$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
idaho$science_trust <- revalue(idaho$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
oregon$science_trust <- revalue(oregon$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
washington$science_trust <- revalue(washington$science_trust, c("1"="5", "2"="4", "3"="3", "4"="2", "5"="1"))
# 5=strongly trust
# 4=somewhat distrust
# 3=neither trust nor distrust
# 2=somewhat distrust
# 1=strongly distrust

# Gov Involvement
survey.responses$gov_involve <- revalue(survey.responses$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
idaho$gov_involve <- revalue(idaho$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
oregon$gov_involve <- revalue(oregon$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))
washington$gov_involve <- revalue(washington$gov_involve, c("1"="5", "6"="4", "7"="3", "2"="2", "3"="1"))

# 5=completely involved
# 4=mostly involved
# 3=moderately involved
# 2=somewhat involved
# 1=not at all involved

# Risk Perception
# future flood risk
survey.responses$future_1<- revalue(survey.responses$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_2<- revalue(survey.responses$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_3<- revalue(survey.responses$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_4<- revalue(survey.responses$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
survey.responses$future_5<- revalue(survey.responses$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

idaho$future_1<- revalue(idaho$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_2<- revalue(idaho$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_3<- revalue(idaho$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_4<- revalue(idaho$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
idaho$future_5<- revalue(idaho$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

oregon$future_1<- revalue(oregon$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_2<- revalue(oregon$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_3<- revalue(oregon$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_4<- revalue(oregon$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
oregon$future_5<- revalue(oregon$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))

washington$future_1<- revalue(washington$future_1, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_2<- revalue(washington$future_2, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_3<- revalue(washington$future_3, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_4<- revalue(washington$future_4, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
washington$future_5<- revalue(washington$future_5, c("20"="1", "19"=".75", "18"=".5", "17"=".25", "16"="0"))
# 100% chance of happening
# 75% chance of happening
# 50% chance of happening
# 25% chance of happening
# 0% chance of happening
# 1-5 to represent closeness

#increase number of floods
survey.responses$incr_no_flood <- revalue(survey.responses$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_no_flood <- revalue(idaho$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_no_flood <- revalue(oregon$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_no_flood <- revalue(washington$incr_no_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3# decrease=1# stay the same=2

#increase severity of floods
survey.responses$incr_sev_flood <- revalue(survey.responses$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
idaho$incr_sev_flood <- revalue(idaho$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
oregon$incr_sev_flood <- revalue(oregon$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
washington$incr_sev_flood <- revalue(washington$incr_sev_flood, c("1"="3", "2"="1", "3"="2"))
# increase=3
# decrease=1
# stay the same=2

# Demographics
# age is ordered correctly: 1= less than 20, 2=20-29 years, 3=30-39 years, 4=40-49 years, 5=50+ years
# education is ordered correctly: 1= some high school, 2= high school diploma, 3=college edu, no grad, 4= associates, 5=bachelors, 6= advanced
# gender is ordered fine: 1= male, 2=female

# Structural Barriers
# too expensive
survey.responses$barrier_1<- revalue(survey.responses$barrier_1, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

#lack of expertise
survey.responses$barrier_2<- revalue(survey.responses$barrier_2, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# sparse population
survey.responses$barrier_3<- revalue(survey.responses$barrier_3, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low rate of economic development
survey.responses$barrier_4<- revalue(survey.responses$barrier_4, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# low flooding risk
survey.responses$barrier_5<- revalue(survey.responses$barrier_5, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# lack of political support
survey.responses$barrier_6<- revalue(survey.responses$barrier_6, c("17"="1", "16"="2", "15"="3", "14"="4", "13"="5"))
# strongly disagree
# somewhat disagree
# neither agree nor disagree
# somewhat agree
# strongly agree

# add a column for barrier 7 which is if lidar is present or not

# SOEP
survey.responses$soep_1<- revalue(survey.responses$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
idaho$soep_1<- revalue(idaho$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
oregon$soep_1<- revalue(oregon$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))
washington$soep_1<- revalue(washington$soep_1, c("0"="10", "1"="9", "2"="8", "3"="7", "4"="6","5"="5", "6"="4", "7"="3", "8"="2", "9"="1","10"="0"))

# 10: risk loving
# 0: risk averse

#Gender all good, except "3" for other
survey.responses$gender <- revalue(survey.responses$gender, c("1"="1", "2"="2", "3"="NA"))
idaho$gender <- revalue(idaho$gender, c("1"="1", "2"="2", "3"="NA"))
oregon$gender <- revalue(oregon$gender, c("1"="1", "2"="2", "3"="NA"))
washington$gender <- revalue(washington$gender, c("1"="1", "2"="2", "3"="NA"))

#age doesn't need recoding

# Edcuation
survey.responses$education <- revalue(survey.responses$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
idaho$education <- revalue(idaho$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
oregon$education <- revalue(oregon$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
washington$education <- revalue(washington$education, c("1"="1", "2"="2", "3"="3", "4"="4", "5"="5", "7"="6"))
# 1: some high school
# 6: advanced degree

# prepared 
survey.responses$prepared <- revalue(survey.responses$prepared, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all prepared
#5: completely prepared

#usefulness
survey.responses$usefulid <- revalue(survey.responses$usefulid, c("5"="1", "4"="2", "3"="3", "2"="4", "1"="5"))
#1: not at all useful
#5: very useful

#flood zone
survey.responses$flood_zone <- revalue(survey.responses$flood_zone, c("1"="1", "2"="0"))
#1: flooding outside of flood zone
#2: no flooding outside


# NETWORK PROXIES
# amount of commmunication 
survey.responses$comm_1 <- revalue(survey.responses$comm_1, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_2 <- revalue(survey.responses$comm_2, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_3 <- revalue(survey.responses$comm_3, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_4 <- revalue(survey.responses$comm_4, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_5 <- revalue(survey.responses$comm_5, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_6 <- revalue(survey.responses$comm_6, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_7 <- revalue(survey.responses$comm_7, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
survey.responses$comm_8 <- revalue(survey.responses$comm_8, c("2"="1", "3"="2", "4"="3", "7"="4", "8"="5", "9"="6"))
#1: a few times a year
#2: once a month
#3: 2-3 times a month
#4: once a week
#5: several times a week
#6: several times a day

# alter lidar use
survey.responses$lidar_1 <- revalue(survey.responses$lidar_1, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_2 <- revalue(survey.responses$lidar_2, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_3 <- revalue(survey.responses$lidar_3, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_4 <- revalue(survey.responses$lidar_4, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_5 <- revalue(survey.responses$lidar_5, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_6 <- revalue(survey.responses$lidar_6, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_7 <- revalue(survey.responses$lidar_7, c("1"="1", "2"="2", "3"="3"))
survey.responses$lidar_8 <- revalue(survey.responses$lidar_8, c("1"="1", "2"="2", "3"="3"))

#1: yes
#2: no
#3: i don't know
 
#expertise
# no need to revalue
#0: no expertise at all
#10: lots of expertise 
```
```{r, numeric conversion, include=FALSE}
survey.responses <- survey.responses %>%
  mutate_at(vars('years','lidaruse', 'interestlid','LocationLatitude', 'LocationLongitude', 'years', 'experience_1', 'experience_2', 'experience_3', 
                 'experience_4', 'experience_5', 'future_1', 'future_2', 'future_3', 'future_4', 'future_5', 
                 'currentmap', 'flood_zone', 'prepared', 'incr_no_flood', 'incr_sev_flood', 'barrier_1', 'barrier_2', 
                 'barrier_3', 'barrier_4', 'barrier_5', 'barrier_6', 'soep_1', 'age', 'gender', 
                 'gov_trust', 'gov_involve', 'education', 'science_trust', 'toolslid', 'toolslid1', 'usefulid', 'no_alters',
                 'alter_names_1_TEXT', 'alter_names_2_TEXT', 'alter_names_3_TEXT', 'alter_names_4_TEXT', 'alter_names_5_TEXT', 'alter_names_6_TEXT',
                 'alter_names_7_TEXT', 'alter_names_9_TEXT', 'comm_1', 'lidar_1', 'expertise_1_1', 'comm_2', 'lidar_2', 'expertise_2_1', 'comm_3', 'lidar_3', 'expertise_3_1',
                'comm_4', 'lidar_4', 'expertise_4_1','comm_5', 'lidar_5', 'expertise_5_1', 'comm_6', 'lidar_6', 'expertise_6_1', 'comm_7', 'lidar_7','expertise_7_1',
                'comm_8', 'lidar_8', 'expertise_8_1'), as.numeric) 
```

```{r, network proxies, include=FALSE}
### Network Proxies

# proportion of alters that use lidar
network.variables <- c("Email", "no_alters","comm_1","lidar_1","expertise_1_1","comm_2","lidar_2","expertise_2_1","comm_3","lidar_3","expertise_3_1",
                "comm_4","lidar_4","expertise_4_1","comm_5","lidar_5","expertise_5_1","comm_6","lidar_6","expertise_6_1","comm_7","lidar_7","expertise_7_1",
                "comm_8","lidar_8","expertise_8_1")

network.subset <- survey.responses[network.variables] %>%
                  drop_na(no_alters) 

lidar.variables <- c("Email", "lidar_1","lidar_2","lidar_3",
                     "lidar_4","lidar_5","lidar_6","lidar_7",
                     "lidar_8")

lidar.subset <- survey.responses[lidar.variables] 

lidar.subset$no_lidar_alters <-  rowSums(lidar.subset == 1, na.rm=TRUE) # this adds allthe lidar users in the network (rep by 1)

# merge number of alters that use lidar with original dataset

survey.responses <- join(survey.responses, lidar.subset[, c("Email","no_lidar_alters")], by= "Email", type="left")


# now calculate the proportion of lidar users for each respondent 

survey.responses$prop_lidar_network <- survey.responses$no_lidar_alters/survey.responses$no_alters # calculates the proportion of lidar users in the network


# let's look at average amount of communication respondent has with network 

comm.variables <- c("Email", "comm_1","comm_2","comm_3",
                    "comm_4","comm_5","comm_6","comm_7",
                    "comm_8")

comm.subset <- survey.responses[comm.variables] 

comm.subset$comm_total <-  rowSums(comm.subset[,2:9], na.rm=TRUE)
comm.subset$comm_ave <- round(rowMeans(comm.subset[,2:9], na.rm=TRUE), digits=0)


survey.responses <- join(survey.responses, comm.subset[, c("Email", "comm_total", "comm_ave")], by= "Email", type="left")


# now let's calculate total expertise in a responsdent's network
expertise.variables <- c("Email", "expertise_1_1","expertise_2_1","expertise_3_1",
                         "expertise_7_1","expertise_6_1","expertise_5_1","expertise_4_1",
                         "expertise_8_1")

expertise.subset <- survey.responses[expertise.variables]


expertise.subset$expert_total <-  rowSums(expertise.subset[,2:9], na.rm=TRUE) 
expertise.subset$expert_ave <- round(rowMeans(expertise.subset[,2:9], na.rm=TRUE), digits=0)

survey.responses <- join(survey.responses, expertise.subset[, c("Email", "expert_total", "expert_ave")], by= "Email", type="left")

```

```{r, model specific data, include=FALSE}
# Dataset for the model
model.variables <- c("lidaruse", "experience_1", "future_1", "incr_sev_flood",  "soep_1", "science_trust","location","prop_lidar_network", "comm_total", "expert_total")

model.data <- survey.responses[model.variables]

str(model.data)

```

```{r, na omit, include=FALSE}
model.data.na.omit <- na.omit(model.data)
n = nrow(model.data.na.omit)
n # 145
```

```{r, lidar use stats, include=FALSE}
count(idaho$lidaruse==1)
id.lidaruse <- round((48/96)*100, digits=1)

count(oregon$lidaruse==1)
or.lidaruse <- round((36/58)*100, digits=1)

count(washington$lidaruse==1)
wa.lidaruse <- round((35/54)*100, digits=1)

```

## *Descriptive Results and Discussion*

Table \@ref(tab:survey-variable-table) summarizes survey respondent's responses to individual and collective predictors of interest. Flood risk manager's in Washington reported the highest direct experience with flood damage in their community, compared to Oregon which reported the least, although the majority of flood risk managers in all three states have had direct experience with flood damage in their communities. Interestingly, the percent of flood risk managers who perceive future flood damage in their communities significantly dropped with only 28% of flood risk managers in Idaho perceiving the future risk. And despite this small belief in future damage, a higher number reported a perceived increase in flood severity. This discrepancy could indicate that despite knowledge of increased flood risk, manager's feel as if their communities are adapting or have adapted to handle an increase in flood severity. We found that flood risk managers in Washington tend to be less risk-averse than managers in Oregon and Idaho. This is interesting and is hard to say exactly why... *any thoughts?* Overall, all three states reported a high trust in the accuracy of flood risk management scientific products (e.g. topographic data, floodplain mapping, floodplain modeling) with Washington reporting the highest percentage of trust. 

``` {r, survey-variable-table, echo=FALSE, fig.cap="\\label{survey variables}"}

#Direct experience with flood damage in community
id.experience <- round((table(idaho$experience_1 == 1)/length(idaho$experience_1))*100, digits=1)
id.experience <- 79.2
or.experience <- round((table(oregon$experience_1 == 1)/length(oregon$experience_1))*100, digits=1)
or.experience <- 72.4
wa.experience <- round((table(washington$experience_1 == 1)/length(washington$experience_1))*100, digits=1) 
wa.experience <- 85.2
# 1=yes

#Over 50% chance of direct experience with future flood damage in the community
id.future <- round((table(idaho$future_1 > .25 )/length(idaho$future_1))*100, digits=1)
id.future <- 28.1
or.future <- round((table(oregon$future_1 > .25)/length(oregon$future_1))*100, digits=1)
or.future <- 39.7
wa.future <- round((table(washington$future_1 > .25)/length(washington$future_1))*100, digits=1) 
wa.future <- 48.1
# includes those who selected 50, 75, and 100% chance of event happening

#Perceived Increase in Flood Severity
id.incr.sev.flood <- (table(idaho$incr_sev_flood == 3)/length(idaho$incr_sev_flood))*100
id.incr.sev.flood <- 38.5
or.incr.sev.flood <- (table(oregon$incr_sev_flood == 3)/length(oregon$incr_sev_flood))*100
or.incr.sev.flood <- 41.4
wa.incr.sev.flood <- (table(washington$incr_sev_flood == 3)/length(washington$incr_sev_flood))*100
wa.incr.sev.flood <- 57.4
# 3=increasing

#Risk-taking attitude
#idaho$soep_1 <- as.numeric(idaho$soep_1)
#id.soep <- round(summarise(idaho, avg=mean(soep_1, na.rm=TRUE)), digits=1)
id.soep <- 2.8
#oregon$soep_1 <- as.numeric(oregon$soep_1)
#or.soep <- round(summarise(oregon, avg=mean(soep_1, na.rm=TRUE)), digits=1)
or.soep <- 3.3
#washington$soep_1 <- as.numeric(washington$soep_1)
#wa.soep <- round(summarise(washington, avg=mean(soep_1, na.rm=TRUE)), digits=1)
wa.soep <- 3.7
#0-risk averse to 10-risk loving

#Trust in accuracy of flood risk management scientific products
id.science.trust <- (table(idaho$science_trust > 3)/length(idaho$science_trust))*100
id.science.trust <- 82.3
or.science.trust <- (table(oregon$science_trust > 3)/length(oregon$science_trust))*100
or.science.trust <- 81.0
wa.science.trust <- (table(washington$science_trust > 3)/length(washington$science_trust))*100
wa.science.trust <- 90.7
#4-somewhat trust, 5-completely trust

#Proportion of lidar users in flood risk management network
idaho <- join(idaho, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#id.prop.lidar <- (round(summarise(idaho, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
id.prop.lidar <- 35.0
oregon <- join(oregon, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#or.prop.lidar <- (round(summarise(oregon, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
or.prop.lidar <- 40.0
washington <- join(washington, survey.responses[, c("Email", "prop_lidar_network", "comm_ave", "expert_ave")], by= "Email", type="left")
#wa.prop.lidar <- (round(summarise(washington, avg=mean(prop_lidar_network, na.rm=TRUE)), digits=2))*100
wa.prop.lidar <- 42.0
#Total communication in flood risk management network
#id.comm.network <- (round(summarise(idaho, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
id.comm.network <- 3
#or.comm.network <- (round(summarise(oregon, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
or.comm.network <- 3
#wa.comm.network <- (round(summarise(washington, avg=mean(comm_ave, na.rm=TRUE)), digits=1))
wa.comm.network <- 3

#Perceived expertise in flood risk management network
#id.expert.network <- (round(summarise(idaho, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
id.expert.network <- 6.6
#or.expert.network <- (round(summarise(oregon, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
or.expert.network <- 7.3
#wa.expert.network <- (round(summarise(washington, avg=mean(expert_ave, na.rm=TRUE)), digits=1))
wa.expert.network <- 6.7

#Use of lidar for flood risk management
#id.lidaruse <- (table(idaho$lidaruse == 1)/length(idaho$lidaruse))*100
id.lidaruse <- 50.0
#or.lidaruse <- (table(oregon$lidaruse == 1)/length(oregon$lidaruse))*100
or.lidaruse <- 62.1
#wa.lidaruse <- (table(washington$lidaruse == 1)/length(washington$lidaruse))*100
wa.lidaruse <- 64.8

survey.variable.stats <- matrix(c(id.experience, or.experience, wa.experience, id.future, or.future, wa.future, id.incr.sev.flood, or.incr.sev.flood, wa.incr.sev.flood, id.soep, or.soep, wa.soep, id.science.trust, or.science.trust, wa.science.trust, id.prop.lidar, or.prop.lidar, wa.prop.lidar, id.comm.network, or.comm.network, wa.comm.network, id.expert.network, or.expert.network, wa.expert.network, id.lidaruse, or.lidaruse, wa.lidaruse), ncol=3, byrow=TRUE)
colnames(survey.variable.stats) <- c("Idaho", "Oregon", "Washington")
rownames(survey.variable.stats) <- c( "Direct experience with flood damage in community (%)", "Direct experience with future flood damage in the community (%)","Perceived Increase in Flood Severity (%)", "Average risk-taking attitude (0 to 10 with 10 being risk-loving)", "Trust in accuracy of flood risk management scientific products (%)", "Proportion of lidar users in flood risk management network (%)", "Average amount of communication with flood risk management network* ", "Perceived expertise in flood risk management network (0 to 10 with 10 being of highest expertise)", "Use lidar for flood risk management (%)" )
knitr::kable(survey.variable.stats, caption="Descriptive statistics of survey question responses by State.")
```

The collective predictors of lidar use varied slightly among the states, with the most significant difference between Washington, with an average of 42% proportion of lidar users in network, and Idaho with an average of 35% proportion of lidar users in network. We hypothesized this difference would lead to differential lidar adoption. Washington reported the highest amount of lidar use in flood risk management with almost 65% of respondent's using lidar. As expected, Idaho report a lower amount of 50% lidar users. This is significantly less than Washington and Oregon. One reason this could be the case is because of the lidar acquisition and coordination program in Washington. The Washington Geological Survey was granted funding from 2015-2021 for the collection and distribution of lidar data and lidar-derived products. Established in the Department of Natural Resources, the funding came from the Washington State General Fund and also provided funding for two permanent lidar positions, a lidar manager and a lidar specialist. In addition, Washington has focused on disseminating interactive (e.g. [Washington Story Map](https://wadnr.maps.arcgis.com/apps/Cascade/index.html?appid=b93c17aa1ef24669b656dbaea009b5ce_)) information on lidar to educate the public and advocate for sustained lidar investment at the state-level. Oregon and Idaho also have established lidar acquisition and coordination efforts, however they do not have a permanently funded position to manage lidar. 

Furthermore, we looked at general trends for each of the predictors and lidar use. Figure \@ref(fig:summary-plots) point out an interesting variation in relationships of certain predictors and lidar use. For each state, the proportion of lidar users and lidar use was highly correlated. While, the direct experience of flood damage in a flood risk manager's community was not highly correlated for lidar use in Washington, unlike Idaho and Oregon. This could be because lidar is already a widely-established technology in Washington. 
**build out this section more?**

```{r, summary-plots, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Relationship between lidar use and each predictor by region." }
prop.lidar.plot <- ggplot(model.data.na.omit, aes(x = prop_lidar_network, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Proportion of Lidar Users in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

expertise.plot <- ggplot(model.data.na.omit, aes(x = expert_total, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Perceived Expertise in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

comm.plot <- ggplot(model.data.na.omit, aes(x = comm_total, y = lidaruse, colour = location)) +
 # geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Total Communication with Alters in Respondents flood risk management Network', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  #scale_x_discrete(limits=c(0, 0.25, .50, .75, 1), labels=c("0%", "25", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

knowledge.plot <- ggplot(model.data.na.omit, aes(x = incr_sev_flood, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Change in Flood Severity', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(1,2,3), labels=c("Decrease", "Stay the Same", "Increase")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

future1.plot <- ggplot(model.data.na.omit, aes(x = future_1, y = lidaruse, colour = location)) +
 # geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, .25, .5, .75, 1), labels=c("0%", "25%", "50%", "75%", "100%")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

soep1.plot <- ggplot(model.data.na.omit, aes(x = soep_1, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Future Risk of Flood Damage', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 5, 8), labels=c("Risk Averse", "Risk Neutral", "Risk Prone")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

experience.plot <- ggplot(model.data.na.omit, aes(x = experience_1, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  geom_smooth(se = FALSE, method = 'lm', label=TRUE) +
  labs(x = 'Experienced flood damage in your community', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
  scale_x_discrete(limits=c(0, 1), labels=c("No", "Yes")) + 
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

science.plot <- ggplot(model.data.na.omit, aes(x = science_trust, y = lidaruse, colour = location)) +
  #geom_jitter(width=0.2, height=0.2)+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial)) +
  labs(x = 'Trust in Usefulness of Scientific Products', y = 'Lidar Use') +
  scale_y_discrete(limits=c(0,1), labels=c("No", "Yes")) +
 scale_x_discrete(limits=c(1, 5), labels=c("Strongly distrust", "Strongly trust")) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = 'none'
  )

ggpubr::ggarrange(prop.lidar.plot, experience.plot, future1.plot, knowledge.plot, soep1.plot, science.plot, comm.plot, expertise.plot, ncol = 4, nrow = 2)

```
**need to add legend**

## *Estimation Results* 

### **Missing data values**
Survey data commonly suffers from item nonresponse that can result in missing data. Typical methods for handling missing data are reweight, imputation, and dropping responses with missing data [@langkampTechniquesHandlingMissing2010]. The network section in our survey was more susceptible to nonresponse from respondents. Of the 208 usable responses we received, 50 of them did not fill out the network section. This could have been due to response fatigue since the network section came at the end of the survey. Another possible explanation is that this part was confusing or uncomfortable for respondents to fill out. In contrast, the rest of the survey, where the individual predictors of lidar use were collected, had less than 10% of responses with missing data. Since our model considers both individual and collective predictors and needs equal size data lengths for each predictor in order to run the model, we dropped almost 25% of our data responses. Since we used the drop technique for handling our missing data, it is likely that our results underestimate the effect size of our predictors and intercept [@langkampTechniquesHandlingMissing2010]. 

```{r, full model, include=FALSE, eval=FALSE}
#add number of alters?
full.mod <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location), data=model.data.na.omit, family= binomial(link = "logit")) # this eliminates if there are responses with a certain amount of Na's # also experience_4 had zero yes's so 
summary(full.mod)                                                           
plot(full.mod)
plot(full.mod, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod)) # best r2 value
pp_check(full.mod) # this checks the posterior predictive ability. looks good

# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors2.5 <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(0, 2.5),
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 

# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 

# informed priors model with cauchy distribution, this is based on the gelman et al. 2008 paper
full.mod.priors2 <- stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(location=c(0,0,0,0,0,0,0,0), 
                                          scale=c(3.7,6.4,1.1,8.4,3.3,7.0,.3,.1)),
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors2)
plot(full.mod.priors2)
plot(full.mod.priors2, "areas", prob = 0.95, prob_outer = 1)
median(bayes_R2(full.mod.priors2))
pp_check(full.mod.priors2) 

# no varying effects
full.mod.no.vary <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total,
                           prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.no.vary)
plot(full.mod.no.vary)
plot(full.mod.no.vary, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.no.vary))
pp_check(full.mod.no.vary)
# null model

null.mod <- stan_glm(lidaruse ~ 1, data=model.data.na.omit, family=binomial(link=logit))

## Model comparison

loo_compare(loo(full.mod), loo(full.mod.priors), loo(full.mod.priors2.5), loo(full.mod.priors2), loo(full.mod.no.vary))
# Based on the LOOIC comparison, full mod no vary with cauchy is the most accurate.
```

```{r, final model, include=FALSE}
# informed priors model with cauchy distribution, based on full mod prior summary
full.mod.priors <- stan_glm(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + location,
                           prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit")) 

summary(full.mod.priors)
plot(full.mod.priors)
plot(full.mod.priors, "areas", prob = 0.95, prob_outer = 1)
r2.final.model <- median(bayes_R2(full.mod.priors))
pp_check(full.mod.priors) 

```

Table \@ref(tab:model-summary-table) displays the results from our GLR model that considers the effect of individual and collective predictors on lidar use. In addition, this methodology allowed for propagation of uncertainty throughout the model. Our model has a Bayesian R-squared value of 'r r2.final.model` and LOOIC of 157.1 and standard error of 15.3. All Pareto k estimates are good (k < 0.5). 

```{r, LOOCV, include=FALSE, eval=FALSE}

Result_G <- data.frame(matrix(nrow=nrow(model.data.na.omit), ncol=6))

#set the column names
colnames(Result_G) <- c("MedianG", "LowerBound1_G", "LowerBound2_G", "UpperBound1_G", "UpperBound2_G", "R2_Bayes")

#do the leave one out cross validation
# create for loop function
for(i in 1:length(model.data.na.omit$lidaruse)){ # for all the data points
 sub_dat <- model.data.na.omit[-i,] #remove each one at a time
 m_sub <-  stan_glmer(lidaruse ~ incr_sev_flood + experience_1 + soep_1 + future_1 + science_trust + prop_lidar_network + comm_total + expert_total + (1|location),
                           prior = cauchy(0, .5), # I initially tried 2.5, but I got this message:  Rejecting initial value: Log probability evaluates to log(0), i.e. negative infinity.
                           prior_intercept = cauchy(0,10), 
                           data=model.data.na.omit, family= binomial(link = "logit"), control = list(adapt_delta = 0.99))
  post=posterior_predict(m_sub, model.data.na.omit[i,], draws=4000)
  r2=r2_bayes(m_sub)# predict that point
  Result_G[i,1]=quantile(post, 0.5) # fill a df with the credibility intervals
  Result_G[i,2]=quantile(post, 0.25) 
  Result_G[i,3]=quantile(post, 0.025)
  Result_G[i,4]=quantile(post, 0.75)
  Result_G[i,5]=quantile(post,0.975)
  Result_G[i,6]=r2
}

write.csv(Result_G, "C:/Users/tarapozzi/Documents/lidar_manuscript/data")
```

```{r, model-summary-table, echo=FALSE, fig.cap="\\label{summary_stats}"}
#these are based on summary(full.mod) which returned a slightly better result that fullmod.priors
summary.stats <- matrix(c(-2.6, 1.3, -4.9, -0.4,
                          0.3, 0.4, -0.3, 1.2,
                          0.4, 0.6, -0.4, 1.4,
                          0.2, 0.3, -0.2, 0.7, 
                          0.1, 0.1, -0.1, 0.2, 
                          -0.1, 0.2, -0.5, 0.3,
                          4.3, 0.8, 3.0, 5.7, 
                          0.0, 0.0, 0.0, 0.1, 
                          0.0, 0.0, 0.0, 0.0), ncol=4, byrow=TRUE) #this is the plogis values for mean and SD ### UPDATE THESE NUMBERS
colnames(summary.stats) <- c( "Mean (log odds)", "S.D.", "5%", "95%")
rownames(summary.stats) <- c("Intercept", "Direct Experience with Flood Damage in Community", "Future Experience with Flood Damage in Community", "Increase in Flood Severity", "Risk Attitude", "Trust in Accuracy of flood risk management Scientific Products", "Proportion of Lidar Users in Respondent's flood risk management Network", "Level Communication with Respondent and their flood risk management Network", "Total Expertise in Respondent's flood risk management Network")
knitr::kable(summary.stats, caption="Estimation results from the model")
```

```{r, posterior-distribution, echo=FALSE, fig.cap="Posterior Predictive Distribution for each predictor variable."}

library(bayesplot)

color_scheme_set("teal")

areas.plot <- mcmc_areas(full.mod.priors, pars = c("incr_sev_flood", "experience_1", "soep_1", "future_1", "science_trust", "prop_lidar_network", "comm_total", "expert_total"), prob = 0.95) +
  scale_y_discrete(expand = c(0, 0))

ci.plot <- plot(full.mod.priors, pars = c("incr_sev_flood", "experience_1", "soep_1", "future_1", "science_trust", "prop_lidar_network", "comm_total", "expert_total"))

ggpubr::ggarrange(areas.plot, ci.plot)

```

Figure \@ref(fig:posterior-distribution) displays the Posterior Predictive Distribution for each predictor and the intercept. This displays the range of predicted probability of lidar use given the predictor is present (?). 

```{r, model-plot, echo=FALSE, include=FALSE, fig.cap= "The effect of lidar proportion in respondent's network on lidar adoption, while holding all other variables at their minimum." }
### let's try simulating continuous variable

proplidar.sim <- seq(min(model.data.na.omit$prop_lidar_network), max(model.data.na.omit$prop_lidar_network), length.out=180)

location.sim <- as.factor(rep(c("Idaho", "Oregon", "Washington"), times=60))

preds <- add_fitted_draws(newdata=data.frame(prop_lidar_network=proplidar.sim,
                                              soep_1=5,
                                              incr_sev_flood=2, 
                                              future_1=0,
                                              experience_1=0,
                                              science_trust=3,
                                              comm_total=0,
                                              expert_total=0, 
                                              location=location.sim),
                          draws = 200,
                          full.mod.priors)

ggplot(preds, aes(x = prop_lidar_network, group=location, col=location)) +
  stat_lineribbon(aes(y = .value), .width = c(.95, .5)) + 
  scale_fill_brewer(palette = "Greys") +
  labs(x = "Proportion of Lidar User's in Respondent's Network", y = "Probability of LiDAR adoption") + theme_bw() + 
  scale_colour_discrete(name = "Location") +
  theme( axis.text=element_text(size=12), axis.title=element_text(size=14),plot.title = element_text(hjust = 0.5), strip.text = element_text(size=14))

```

Figure \@ref(fig:model-plot) displays the effect, when holding all other variables at their minimum, for respondents who have a higher percentage of lidar user's in their network there is an increased likelihood in lidar adoption when compared to respondents who did not have any network connections with lidar adoption. *create plot for each predictor?*

Direct experience, future risk perception, and knowledge have moderate effect on lidar use. We found this result to be surprising in that we expected experience to have a greater impact on lidar adoption. Risk attitude, trust in science, network communication and expertise have minimal effect on lidar use. 

# DISCUSSION 

The above findings offer partial support for the trends we expected to uncover from our exploratory analysis. Our first trend we expected was the significant impact of collective predictors on lidar use. As expected, those with higher portion of lidar users in their social network were more likely to adopt lidar. This result is supported by existing technology adoption literature [e.g. @loRoleSocialNorms2013; @poussinFactorsInfluenceFlood2014; @viglioneInsightsSociohydrologyModelling2014]. However, the amount of communication and expertise of alters in our respondent's social network did not have a statistically significant impact on lidar adoption. This result is surprising and contrary to what we thought would increase adoption. We predicted that, due to social learning through communication and perceived expertise of peers, individuals would imitate their peers. Perhaps, our analysis did not capture the nuanced effect of communication with specific types of alters though and therefore had a null impact. As a result, the measure of lidar proportion was more representative of the influence that we were interested in studying. 

We consider these findings illustrative of flood risk managers desire to learn from peers. This supports the themes that we encountered during our interviews prior to the survey. One interview we conducted, with a floodplain manager from Idaho at a regional conference,  mentioned "I feel like we should do a lot more networking in the state of Idaho, but oftentimes I have to reach out to people in like Washington for help or like at the national level for help. And so that's why coming to these conferences is really helpful for me because I meet peers outside of just our immediate, that have similar programs." This is an intriguing point that highlights a regional difference, where flood risk managers gain insightful information by networking outside of their state. It also highlights a potential gap in education in Idaho. This point was supported in our other interviews in Idaho; "we're all in the same kind of communities, which is helpful sometimes, but it also is a little bit of a silo thing... we are all stuck in the same point of view." 

Additionally, we found partial support for individual predictors of lidar adoption. Direct experience, risk perception, and knowledge had moderate effects on lidar adopt lidar adoption. Direct experience has been studied extensively in the past as a significant predictor of individual risk perception and behavior [@lechowskaWhatDeterminesFlood2018]. While our study found a moderate effect of direct experience, previous research has found variable effects of experience on behavior and suggest that measuring the intensity of the event experience as a more informative measure (CITE). As mentioned previously, risk perception has had mixed results in it's effect on risk mitigation behavior [e.g. @birkholzRethinkingRelationshipFlood2014; @bubeckReviewRiskPerceptions2012]. We included risk perception in our study as a measure of future risk of damage from a flood event in the community. Our results supported that respondents with an increased future risk perception, were more likely to adopt lidar. We expected this result because respondents are more likely to want to accurately know their risk if they see it as potentially destructive. In addition, we included a knowledge predictor which we expected to have a postive effect on lidar adoption that our results supported.

Interestingly risk attitude had minimal impact on lidar adoption. We used the German Socio-Economic Panel Study risk question (SOEP) to measure general risk-taking attitude. The SOEP methodology is a direct question about how a respondent ranks their risk-taking attitude overall. This method of self-reported answering represents a valid, low-cost substitute for incentivised lottery schemes [@crosettoTheoreticalExperimentalAppraisal2016]. Due to our concern of incentivised risk elicitation methods causing mental fatigue and unneeded complication to our survey instrument, we chose to go with the SOEP method. Our results were inconclusive on the effect of risk attitude on lidar adoption. This is perhaps because of the duality of risk respondents face when adopting a new technology for flood risk management. There is an inherent risk in adopting a technology that they may not know how to use, but a pay off in managing in flood risk. Whereas, there may be others who are more willing to take the risk of potential flooding in order to minimize the risk of adopting a new technology. This inconclusive finding, suggests that we need to look into risk salience further to understand the multi-layer (e.g. technological risk, societal risk) decision making... 

In addition, research has explored the impact of trust on risk mitigation behavior [@cannonClimateChangeDouble2020]. Our results which suggested there was a minimal effect of trust in science on lidar adoption. This could be because lidar is well-established and trusted within the flood risk management community generally. This result aligns with a general theme in of interviews that was most people trusted the efficacy of lidar now. However, there  was some research participants who offered a different perspective on lidar availability that was of particular interest in our study because the Western U.S. has significantly less privately-owned land than other parts of the country and therefore some concern of publicly-available lidar. For example, in response to why a community didst participate in the NFIP, “we were never mapped…a great deal of it really is our core private property rights and, probably some lower income population that couldn't afford flood insurance if, if they had to and really just being a self-sufficient community where if you’re going to build there and it’s along a river or creek, you probably inherently know that there’s some flood risk.” This point brings up a topic that is discussed in the disaster research community quite a bit; self-sufficiency. To a certain extent, self-sufficiency is necessary for individuals to prepare, respond, and recover to disasters. However, self-sufficiency without awareness of risk can result in a dangerous and potential deadly situation.
 
## *Implications* 

We suggest cross-institutional collaboration as an area of focus for resource allocation in the future.  We suggest that Federal, State, and Local level authorities capitalize on the importance of network connections, not only for lidar adoption, but for general information dissemination of effective flood risk mitigation behavior and sustained best practices for flood risk management. For example, providing targeted networking events for the lidar community to gather and communicate lidar.  

The regional differences in lidar adoption were correlated with the level of lidar acquisition and coordination in each State. The lidar model in Washington, which includes two full time position and state funding, is likely one of the reasons we see an increase in lidar usage in Washington. It is recommended that Oregon and Idaho follow a similar model to ensure an increase in uptake. This would require both policy and funding-level changes in Oregon and Idaho. The USGS has broken down the benefit-cost ratio for each State to help state-level decision makers plan and manage lidar acquisition in their (USGS, 2021). In addition, [add a part about the applied chapter here?]

# CONCLUSION

Future studies: need to do a longitudinal study to see how lidar adoption changes over time especially with target barrier reduction and increase network communication and expertise.

# Literature Cited